{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import random\n",
    "import imageio\n",
    "import argparse\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.utils.data as da"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "# 1. 读取数据，构建训练集和测试集"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "outputs": [],
   "source": [
    "def get_images(paths, labels, nb_samples = None, shuffle = True):\n",
    "    \"\"\"\n",
    "    获取一组字符文件夹和标签，并返回带有标签的图像文件的路径。\n",
    "    输入:\n",
    "        paths: 一个字符文件夹的列表\n",
    "        labels: 与路径长度相同的列表或numpy数组\n",
    "        nb_samples: 每个字符检索的图像数量\n",
    "    输出:\n",
    "        由元组(标签, 图像路径)构成的列表\n",
    "    \"\"\"\n",
    "    if nb_samples is not None:\n",
    "        sampler = lambda x: random.sample(x, nb_samples)\n",
    "    else:\n",
    "        sampler = lambda x: x\n",
    "    images_labels = [(i, os.path.join(path, image))\n",
    "                     for i, path in zip(labels, paths)\n",
    "                     for image in sampler([pathstr for pathstr in os.listdir(path) if pathstr[-4:] == '.png' ])]\n",
    "    if shuffle:\n",
    "        random.shuffle(images_labels)\n",
    "    return images_labels"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "outputs": [],
   "source": [
    "def image_file_to_array(filename, dim_input):\n",
    "    \"\"\"\n",
    "    读取图像路径并返回numpy数组\n",
    "    输入:\n",
    "        filename: 图像文件名称\n",
    "        dim_input: 图像的扁平形状\n",
    "    输出:\n",
    "        单通道图像\n",
    "    \"\"\"\n",
    "    image = imageio.v2.imread(filename)\n",
    "    image = image.reshape([dim_input])\n",
    "    image = image.astype(np.float32) / 255.0\n",
    "    image = 1.0 - image\n",
    "\n",
    "    return image"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "outputs": [],
   "source": [
    "def pair_shuffle(array_a, array_b):\n",
    "    \"\"\"\n",
    "    获取一个图像数组和一个标签数组\n",
    "    输出:\n",
    "        打乱的图像数组和标签数组\n",
    "    \"\"\"\n",
    "    temp_perm = np.random.permutation(array_a.shape[0])\n",
    "    array_a = array_a[temp_perm]\n",
    "    array_b = array_b[temp_perm]\n",
    "    return array_a, array_b"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "outputs": [],
   "source": [
    "def LoadData(num_classes = 50, num_samples_per_class_train = 15, num_samples_per_class_test = 5, seed = 1):\n",
    "    \"\"\"\n",
    "    加载数据并将其分割为训练和测试集\n",
    "    输入:\n",
    "        num_classes: 采用的类数，-1表示使用所有类\n",
    "        num_samples_per_class_train: 每个类用于训练的样本数量\n",
    "        num_samples_per_class_test: 每个类用于测试的样本数量\n",
    "        seed: 随机种子以确保结果一致\n",
    "    输出:\n",
    "        一个元组：(1)用于训练的图像(2)用于训练的标签(3)用于测试的图像，以及(4)用于测试的标签\n",
    "            (1) 形状[num_classes * num_samples_per_class_train, 784]，二进制像素的Numpy数组\n",
    "            (2) 形状[num_classes * num_samples_per_class_train]的Numpy数组，类标签的整数\n",
    "            (3) 形状[num_classes * num_samples_per_class_test, 784]，二进制像素的Numpy数组\n",
    "            (4) 形状[num_classes * num_samples_per_class_test]的Numpy数组，类标签的整数\n",
    "    \"\"\"\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    num_samples_per_class = num_samples_per_class_train + num_samples_per_class_test\n",
    "    assert num_classes <= 1623\n",
    "    assert num_samples_per_class <= 20\n",
    "    dim_input = 28 * 28   # 784\n",
    "\n",
    "    # 建立文件夹\n",
    "    data_folder = './omniglot_resized'\n",
    "    character_folders = [os.path.join(data_folder, family, character)\n",
    "                         for family in os.listdir(data_folder)\n",
    "                         if os.path.isdir(os.path.join(data_folder, family))\n",
    "                         for character in os.listdir(os.path.join(data_folder, family))\n",
    "                         if os.path.isdir(os.path.join(data_folder, family, character))]\n",
    "    random.shuffle(character_folders)\n",
    "    if num_classes == -1:\n",
    "        num_classes = len(character_folders)\n",
    "    else:\n",
    "        character_folders = character_folders[: num_classes]\n",
    "\n",
    "    # 读取图像\n",
    "    all_images = np.zeros(shape = (num_samples_per_class, num_classes, dim_input))\n",
    "    all_labels = np.zeros(shape = (num_samples_per_class, num_classes))\n",
    "    label_images = get_images(character_folders, list(range(num_classes)), nb_samples = num_samples_per_class, shuffle = True)\n",
    "    temp_count = np.zeros(num_classes, dtype=int)\n",
    "    for label,imagefile in label_images:\n",
    "        temp_num = temp_count[label]\n",
    "        all_images[temp_num, label, :] = image_file_to_array(imagefile, dim_input)\n",
    "        all_labels[temp_num, label] = label\n",
    "        temp_count[label] += 1\n",
    "\n",
    "    # 分裂和随机排列\n",
    "    train_image = all_images[:num_samples_per_class_train].reshape(-1,dim_input)\n",
    "    test_image  = all_images[num_samples_per_class_train:].reshape(-1,dim_input)\n",
    "    train_label = all_labels[:num_samples_per_class_train].reshape(-1)\n",
    "    test_label  = all_labels[num_samples_per_class_train:].reshape(-1)\n",
    "    train_image, train_label = pair_shuffle(train_image, train_label)\n",
    "    test_image, test_label = pair_shuffle(test_image, test_label)\n",
    "    return train_image, train_label, test_image, test_label"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "outputs": [],
   "source": [
    "train_image, train_label, test_image, test_label = LoadData()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "outputs": [
    {
     "data": {
      "text/plain": "(750, 784)"
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_image.shape"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# 2.实现全连接神经网络"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "outputs": [],
   "source": [
    "# 外部超参数设置\n",
    "parser = argparse.ArgumentParser()\n",
    "parser.add_argument('--num_classes', type=int, default=50,\n",
    "                    help='number of classes used')\n",
    "parser.add_argument('--num_samples_train', type=int, default=15,\n",
    "                    help='number of samples per class used for training')\n",
    "parser.add_argument('--num_samples_test', type=int, default=5,\n",
    "                    help='number of samples per class used for testing')\n",
    "parser.add_argument('--seed', type=int, default=1,\n",
    "                    help='random seed')\n",
    "parser.add_argument('--epochs', type=int, default=200,\n",
    "                    help='number of epochs')\n",
    "# args = parser.parse_args()\n",
    "args = parser.parse_known_args()[0]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "outputs": [],
   "source": [
    "# 加载数据\n",
    "train_image, train_label, test_image, test_label = LoadData(args.num_classes, args.num_samples_train, args.num_samples_test, args.seed)\n",
    "# 转换为pytorch可处理数据集\n",
    "train_dataset = da.TensorDataset(torch.from_numpy(train_image), torch.from_numpy(train_label))\n",
    "test_dataset = da.TensorDataset(torch.from_numpy(test_image), torch.from_numpy(test_label))\n",
    "# 数据分批\n",
    "size_batch = 100\n",
    "train_loader = da.DataLoader(dataset=train_dataset, batch_size=size_batch, shuffle=True)\n",
    "test_loader = da.DataLoader(dataset=test_dataset, batch_size=size_batch, shuffle=False)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "outputs": [],
   "source": [
    "# 加载数据的第二种思路：独热向量编码，注意后面部分代码需要修改\n",
    "train_image, train_label, test_image, test_label = LoadData(args.num_classes, args.num_samples_train, args.num_samples_test, args.seed)\n",
    "# 标签转换为独热向量\n",
    "train_labels = np.empty((0, args.num_classes))\n",
    "for i in range(0, len(train_label)):\n",
    "    temp_labels = np.array([np.zeros(args.num_classes)])\n",
    "    temp_labels[0, int(train_label[i])] = 1\n",
    "    train_labels = np.append(train_labels, temp_labels, axis=0)\n",
    "# 转换为pytorch可处理数据集\n",
    "train_dataset = da.TensorDataset(torch.from_numpy(train_image), torch.from_numpy(train_labels))\n",
    "test_dataset = da.TensorDataset(torch.from_numpy(test_image), torch.from_numpy(test_label))\n",
    "# 数据分批\n",
    "size_batch = 50\n",
    "train_loader = da.DataLoader(dataset=train_dataset, batch_size=size_batch, shuffle=True)\n",
    "test_loader = da.DataLoader(dataset=test_dataset, batch_size=size_batch, shuffle=False)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "outputs": [],
   "source": [
    "# 定义超参数, 模型, 损失函数 和 优化器\n",
    "# 定义内部超参数\n",
    "size_in = 784\n",
    "size_hidden = 500\n",
    "size_out = args.num_classes\n",
    "learning_rate = 1e-3\n",
    "# 定义模型\n",
    "class ANN(nn.Module):\n",
    "    def __init__(self, size_in, size_hidden, size_out):\n",
    "        super(ANN, self).__init__()\n",
    "        self.layer1 = nn.Linear(size_in, size_hidden)\n",
    "        self.layer2 = nn.ReLU()\n",
    "        self.layer3 = nn.Linear(size_hidden, size_out)\n",
    "    def forward(self, x):\n",
    "        x = x.to(torch.float32)\n",
    "        layer1_out = self.layer1(x)\n",
    "        layer2_out = self.layer2(layer1_out)\n",
    "        layer3_out = self.layer3(layer2_out)\n",
    "        return layer3_out\n",
    "# gpu\n",
    "# device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model = ANN(size_in, size_hidden, size_out)# .to(device)\n",
    "# 定义损失函数\n",
    "loss = nn.CrossEntropyLoss()\n",
    "# 定义优化器\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [1/200], Step: [1/16], Loss: 3.9054\n",
      "Epoch: [1/200], Step: [2/16], Loss: 3.8809\n",
      "Epoch: [1/200], Step: [3/16], Loss: 3.8884\n",
      "Epoch: [1/200], Step: [4/16], Loss: 3.8904\n",
      "Epoch: [1/200], Step: [5/16], Loss: 3.8835\n",
      "Epoch: [1/200], Step: [6/16], Loss: 3.8802\n",
      "Epoch: [1/200], Step: [7/16], Loss: 3.8147\n",
      "Epoch: [1/200], Step: [8/16], Loss: 3.8476\n",
      "Epoch: [1/200], Step: [9/16], Loss: 3.8158\n",
      "Epoch: [1/200], Step: [10/16], Loss: 3.8720\n",
      "Epoch: [1/200], Step: [11/16], Loss: 3.8220\n",
      "Epoch: [1/200], Step: [12/16], Loss: 3.8076\n",
      "Epoch: [1/200], Step: [13/16], Loss: 3.7825\n",
      "Epoch: [1/200], Step: [14/16], Loss: 3.8097\n",
      "Epoch: [1/200], Step: [15/16], Loss: 3.7676\n",
      "Epoch: [2/200], Step: [1/16], Loss: 3.5201\n",
      "Epoch: [2/200], Step: [2/16], Loss: 3.4793\n",
      "Epoch: [2/200], Step: [3/16], Loss: 3.4065\n",
      "Epoch: [2/200], Step: [4/16], Loss: 3.4104\n",
      "Epoch: [2/200], Step: [5/16], Loss: 3.2338\n",
      "Epoch: [2/200], Step: [6/16], Loss: 3.3448\n",
      "Epoch: [2/200], Step: [7/16], Loss: 3.3032\n",
      "Epoch: [2/200], Step: [8/16], Loss: 3.3165\n",
      "Epoch: [2/200], Step: [9/16], Loss: 3.3428\n",
      "Epoch: [2/200], Step: [10/16], Loss: 3.3652\n",
      "Epoch: [2/200], Step: [11/16], Loss: 3.2208\n",
      "Epoch: [2/200], Step: [12/16], Loss: 3.1871\n",
      "Epoch: [2/200], Step: [13/16], Loss: 3.1858\n",
      "Epoch: [2/200], Step: [14/16], Loss: 3.1561\n",
      "Epoch: [2/200], Step: [15/16], Loss: 3.0723\n",
      "Epoch: [3/200], Step: [1/16], Loss: 3.0082\n",
      "Epoch: [3/200], Step: [2/16], Loss: 2.8790\n",
      "Epoch: [3/200], Step: [3/16], Loss: 2.7530\n",
      "Epoch: [3/200], Step: [4/16], Loss: 2.6583\n",
      "Epoch: [3/200], Step: [5/16], Loss: 2.7437\n",
      "Epoch: [3/200], Step: [6/16], Loss: 2.7319\n",
      "Epoch: [3/200], Step: [7/16], Loss: 2.5643\n",
      "Epoch: [3/200], Step: [8/16], Loss: 2.6516\n",
      "Epoch: [3/200], Step: [9/16], Loss: 2.4320\n",
      "Epoch: [3/200], Step: [10/16], Loss: 2.5554\n",
      "Epoch: [3/200], Step: [11/16], Loss: 2.3945\n",
      "Epoch: [3/200], Step: [12/16], Loss: 2.7064\n",
      "Epoch: [3/200], Step: [13/16], Loss: 2.4486\n",
      "Epoch: [3/200], Step: [14/16], Loss: 2.4338\n",
      "Epoch: [3/200], Step: [15/16], Loss: 2.2458\n",
      "Epoch: [4/200], Step: [1/16], Loss: 2.3002\n",
      "Epoch: [4/200], Step: [2/16], Loss: 2.3155\n",
      "Epoch: [4/200], Step: [3/16], Loss: 2.0009\n",
      "Epoch: [4/200], Step: [4/16], Loss: 1.9692\n",
      "Epoch: [4/200], Step: [5/16], Loss: 2.0122\n",
      "Epoch: [4/200], Step: [6/16], Loss: 1.8521\n",
      "Epoch: [4/200], Step: [7/16], Loss: 1.8076\n",
      "Epoch: [4/200], Step: [8/16], Loss: 2.0737\n",
      "Epoch: [4/200], Step: [9/16], Loss: 1.8720\n",
      "Epoch: [4/200], Step: [10/16], Loss: 1.8323\n",
      "Epoch: [4/200], Step: [11/16], Loss: 1.9759\n",
      "Epoch: [4/200], Step: [12/16], Loss: 1.8909\n",
      "Epoch: [4/200], Step: [13/16], Loss: 1.7849\n",
      "Epoch: [4/200], Step: [14/16], Loss: 1.7288\n",
      "Epoch: [4/200], Step: [15/16], Loss: 1.6718\n",
      "Epoch: [5/200], Step: [1/16], Loss: 1.6756\n",
      "Epoch: [5/200], Step: [2/16], Loss: 1.4113\n",
      "Epoch: [5/200], Step: [3/16], Loss: 1.3218\n",
      "Epoch: [5/200], Step: [4/16], Loss: 1.6271\n",
      "Epoch: [5/200], Step: [5/16], Loss: 0.9303\n",
      "Epoch: [5/200], Step: [6/16], Loss: 1.7122\n",
      "Epoch: [5/200], Step: [7/16], Loss: 1.3256\n",
      "Epoch: [5/200], Step: [8/16], Loss: 1.4654\n",
      "Epoch: [5/200], Step: [9/16], Loss: 1.3191\n",
      "Epoch: [5/200], Step: [10/16], Loss: 1.4378\n",
      "Epoch: [5/200], Step: [11/16], Loss: 1.3838\n",
      "Epoch: [5/200], Step: [12/16], Loss: 1.4550\n",
      "Epoch: [5/200], Step: [13/16], Loss: 1.3330\n",
      "Epoch: [5/200], Step: [14/16], Loss: 1.0624\n",
      "Epoch: [5/200], Step: [15/16], Loss: 1.5595\n",
      "Epoch: [6/200], Step: [1/16], Loss: 1.2074\n",
      "Epoch: [6/200], Step: [2/16], Loss: 1.0951\n",
      "Epoch: [6/200], Step: [3/16], Loss: 0.9826\n",
      "Epoch: [6/200], Step: [4/16], Loss: 1.3851\n",
      "Epoch: [6/200], Step: [5/16], Loss: 1.0436\n",
      "Epoch: [6/200], Step: [6/16], Loss: 1.0063\n",
      "Epoch: [6/200], Step: [7/16], Loss: 0.8945\n",
      "Epoch: [6/200], Step: [8/16], Loss: 0.9987\n",
      "Epoch: [6/200], Step: [9/16], Loss: 0.9146\n",
      "Epoch: [6/200], Step: [10/16], Loss: 1.0855\n",
      "Epoch: [6/200], Step: [11/16], Loss: 0.8348\n",
      "Epoch: [6/200], Step: [12/16], Loss: 0.7997\n",
      "Epoch: [6/200], Step: [13/16], Loss: 1.1603\n",
      "Epoch: [6/200], Step: [14/16], Loss: 0.9300\n",
      "Epoch: [6/200], Step: [15/16], Loss: 1.0226\n",
      "Epoch: [7/200], Step: [1/16], Loss: 0.7447\n",
      "Epoch: [7/200], Step: [2/16], Loss: 0.6789\n",
      "Epoch: [7/200], Step: [3/16], Loss: 0.6827\n",
      "Epoch: [7/200], Step: [4/16], Loss: 0.7395\n",
      "Epoch: [7/200], Step: [5/16], Loss: 0.9044\n",
      "Epoch: [7/200], Step: [6/16], Loss: 0.8599\n",
      "Epoch: [7/200], Step: [7/16], Loss: 0.7269\n",
      "Epoch: [7/200], Step: [8/16], Loss: 0.8261\n",
      "Epoch: [7/200], Step: [9/16], Loss: 0.7869\n",
      "Epoch: [7/200], Step: [10/16], Loss: 0.6819\n",
      "Epoch: [7/200], Step: [11/16], Loss: 0.7857\n",
      "Epoch: [7/200], Step: [12/16], Loss: 0.7391\n",
      "Epoch: [7/200], Step: [13/16], Loss: 0.9765\n",
      "Epoch: [7/200], Step: [14/16], Loss: 0.6142\n",
      "Epoch: [7/200], Step: [15/16], Loss: 0.7473\n",
      "Epoch: [8/200], Step: [1/16], Loss: 0.6745\n",
      "Epoch: [8/200], Step: [2/16], Loss: 0.5840\n",
      "Epoch: [8/200], Step: [3/16], Loss: 0.4977\n",
      "Epoch: [8/200], Step: [4/16], Loss: 0.4704\n",
      "Epoch: [8/200], Step: [5/16], Loss: 0.5652\n",
      "Epoch: [8/200], Step: [6/16], Loss: 0.6113\n",
      "Epoch: [8/200], Step: [7/16], Loss: 0.5533\n",
      "Epoch: [8/200], Step: [8/16], Loss: 0.4533\n",
      "Epoch: [8/200], Step: [9/16], Loss: 0.5542\n",
      "Epoch: [8/200], Step: [10/16], Loss: 0.6393\n",
      "Epoch: [8/200], Step: [11/16], Loss: 0.6292\n",
      "Epoch: [8/200], Step: [12/16], Loss: 0.8679\n",
      "Epoch: [8/200], Step: [13/16], Loss: 0.5967\n",
      "Epoch: [8/200], Step: [14/16], Loss: 0.4372\n",
      "Epoch: [8/200], Step: [15/16], Loss: 0.6690\n",
      "Epoch: [9/200], Step: [1/16], Loss: 0.5127\n",
      "Epoch: [9/200], Step: [2/16], Loss: 0.4562\n",
      "Epoch: [9/200], Step: [3/16], Loss: 0.4077\n",
      "Epoch: [9/200], Step: [4/16], Loss: 0.4777\n",
      "Epoch: [9/200], Step: [5/16], Loss: 0.4581\n",
      "Epoch: [9/200], Step: [6/16], Loss: 0.4686\n",
      "Epoch: [9/200], Step: [7/16], Loss: 0.3544\n",
      "Epoch: [9/200], Step: [8/16], Loss: 0.5028\n",
      "Epoch: [9/200], Step: [9/16], Loss: 0.4000\n",
      "Epoch: [9/200], Step: [10/16], Loss: 0.5641\n",
      "Epoch: [9/200], Step: [11/16], Loss: 0.3640\n",
      "Epoch: [9/200], Step: [12/16], Loss: 0.3730\n",
      "Epoch: [9/200], Step: [13/16], Loss: 0.5103\n",
      "Epoch: [9/200], Step: [14/16], Loss: 0.5601\n",
      "Epoch: [9/200], Step: [15/16], Loss: 0.4420\n",
      "Epoch: [10/200], Step: [1/16], Loss: 0.4692\n",
      "Epoch: [10/200], Step: [2/16], Loss: 0.2697\n",
      "Epoch: [10/200], Step: [3/16], Loss: 0.4043\n",
      "Epoch: [10/200], Step: [4/16], Loss: 0.3028\n",
      "Epoch: [10/200], Step: [5/16], Loss: 0.2805\n",
      "Epoch: [10/200], Step: [6/16], Loss: 0.4280\n",
      "Epoch: [10/200], Step: [7/16], Loss: 0.2983\n",
      "Epoch: [10/200], Step: [8/16], Loss: 0.2984\n",
      "Epoch: [10/200], Step: [9/16], Loss: 0.4430\n",
      "Epoch: [10/200], Step: [10/16], Loss: 0.3982\n",
      "Epoch: [10/200], Step: [11/16], Loss: 0.3633\n",
      "Epoch: [10/200], Step: [12/16], Loss: 0.3410\n",
      "Epoch: [10/200], Step: [13/16], Loss: 0.3894\n",
      "Epoch: [10/200], Step: [14/16], Loss: 0.3842\n",
      "Epoch: [10/200], Step: [15/16], Loss: 0.3459\n",
      "Epoch: [11/200], Step: [1/16], Loss: 0.2961\n",
      "Epoch: [11/200], Step: [2/16], Loss: 0.2378\n",
      "Epoch: [11/200], Step: [3/16], Loss: 0.2449\n",
      "Epoch: [11/200], Step: [4/16], Loss: 0.2641\n",
      "Epoch: [11/200], Step: [5/16], Loss: 0.2736\n",
      "Epoch: [11/200], Step: [6/16], Loss: 0.2148\n",
      "Epoch: [11/200], Step: [7/16], Loss: 0.2853\n",
      "Epoch: [11/200], Step: [8/16], Loss: 0.3312\n",
      "Epoch: [11/200], Step: [9/16], Loss: 0.2606\n",
      "Epoch: [11/200], Step: [10/16], Loss: 0.3529\n",
      "Epoch: [11/200], Step: [11/16], Loss: 0.2761\n",
      "Epoch: [11/200], Step: [12/16], Loss: 0.2987\n",
      "Epoch: [11/200], Step: [13/16], Loss: 0.3416\n",
      "Epoch: [11/200], Step: [14/16], Loss: 0.2782\n",
      "Epoch: [11/200], Step: [15/16], Loss: 0.2455\n",
      "Epoch: [12/200], Step: [1/16], Loss: 0.1906\n",
      "Epoch: [12/200], Step: [2/16], Loss: 0.1620\n",
      "Epoch: [12/200], Step: [3/16], Loss: 0.2261\n",
      "Epoch: [12/200], Step: [4/16], Loss: 0.2424\n",
      "Epoch: [12/200], Step: [5/16], Loss: 0.2975\n",
      "Epoch: [12/200], Step: [6/16], Loss: 0.1453\n",
      "Epoch: [12/200], Step: [7/16], Loss: 0.2256\n",
      "Epoch: [12/200], Step: [8/16], Loss: 0.2559\n",
      "Epoch: [12/200], Step: [9/16], Loss: 0.2384\n",
      "Epoch: [12/200], Step: [10/16], Loss: 0.1836\n",
      "Epoch: [12/200], Step: [11/16], Loss: 0.1943\n",
      "Epoch: [12/200], Step: [12/16], Loss: 0.1939\n",
      "Epoch: [12/200], Step: [13/16], Loss: 0.2321\n",
      "Epoch: [12/200], Step: [14/16], Loss: 0.2767\n",
      "Epoch: [12/200], Step: [15/16], Loss: 0.2353\n",
      "Epoch: [13/200], Step: [1/16], Loss: 0.1790\n",
      "Epoch: [13/200], Step: [2/16], Loss: 0.2128\n",
      "Epoch: [13/200], Step: [3/16], Loss: 0.1757\n",
      "Epoch: [13/200], Step: [4/16], Loss: 0.2087\n",
      "Epoch: [13/200], Step: [5/16], Loss: 0.1950\n",
      "Epoch: [13/200], Step: [6/16], Loss: 0.1693\n",
      "Epoch: [13/200], Step: [7/16], Loss: 0.1538\n",
      "Epoch: [13/200], Step: [8/16], Loss: 0.1538\n",
      "Epoch: [13/200], Step: [9/16], Loss: 0.1689\n",
      "Epoch: [13/200], Step: [10/16], Loss: 0.1666\n",
      "Epoch: [13/200], Step: [11/16], Loss: 0.1535\n",
      "Epoch: [13/200], Step: [12/16], Loss: 0.1757\n",
      "Epoch: [13/200], Step: [13/16], Loss: 0.1655\n",
      "Epoch: [13/200], Step: [14/16], Loss: 0.1313\n",
      "Epoch: [13/200], Step: [15/16], Loss: 0.2340\n",
      "Epoch: [14/200], Step: [1/16], Loss: 0.1601\n",
      "Epoch: [14/200], Step: [2/16], Loss: 0.1322\n",
      "Epoch: [14/200], Step: [3/16], Loss: 0.1381\n",
      "Epoch: [14/200], Step: [4/16], Loss: 0.1505\n",
      "Epoch: [14/200], Step: [5/16], Loss: 0.1117\n",
      "Epoch: [14/200], Step: [6/16], Loss: 0.1620\n",
      "Epoch: [14/200], Step: [7/16], Loss: 0.1264\n",
      "Epoch: [14/200], Step: [8/16], Loss: 0.1523\n",
      "Epoch: [14/200], Step: [9/16], Loss: 0.1456\n",
      "Epoch: [14/200], Step: [10/16], Loss: 0.1220\n",
      "Epoch: [14/200], Step: [11/16], Loss: 0.1260\n",
      "Epoch: [14/200], Step: [12/16], Loss: 0.1444\n",
      "Epoch: [14/200], Step: [13/16], Loss: 0.1822\n",
      "Epoch: [14/200], Step: [14/16], Loss: 0.1532\n",
      "Epoch: [14/200], Step: [15/16], Loss: 0.1611\n",
      "Epoch: [15/200], Step: [1/16], Loss: 0.1122\n",
      "Epoch: [15/200], Step: [2/16], Loss: 0.0881\n",
      "Epoch: [15/200], Step: [3/16], Loss: 0.1801\n",
      "Epoch: [15/200], Step: [4/16], Loss: 0.1302\n",
      "Epoch: [15/200], Step: [5/16], Loss: 0.1369\n",
      "Epoch: [15/200], Step: [6/16], Loss: 0.1277\n",
      "Epoch: [15/200], Step: [7/16], Loss: 0.0983\n",
      "Epoch: [15/200], Step: [8/16], Loss: 0.0924\n",
      "Epoch: [15/200], Step: [9/16], Loss: 0.1152\n",
      "Epoch: [15/200], Step: [10/16], Loss: 0.0943\n",
      "Epoch: [15/200], Step: [11/16], Loss: 0.0917\n",
      "Epoch: [15/200], Step: [12/16], Loss: 0.1269\n",
      "Epoch: [15/200], Step: [13/16], Loss: 0.1327\n",
      "Epoch: [15/200], Step: [14/16], Loss: 0.1222\n",
      "Epoch: [15/200], Step: [15/16], Loss: 0.1194\n",
      "Epoch: [16/200], Step: [1/16], Loss: 0.0846\n",
      "Epoch: [16/200], Step: [2/16], Loss: 0.0719\n",
      "Epoch: [16/200], Step: [3/16], Loss: 0.1008\n",
      "Epoch: [16/200], Step: [4/16], Loss: 0.0945\n",
      "Epoch: [16/200], Step: [5/16], Loss: 0.1387\n",
      "Epoch: [16/200], Step: [6/16], Loss: 0.1019\n",
      "Epoch: [16/200], Step: [7/16], Loss: 0.0998\n",
      "Epoch: [16/200], Step: [8/16], Loss: 0.0970\n",
      "Epoch: [16/200], Step: [9/16], Loss: 0.1017\n",
      "Epoch: [16/200], Step: [10/16], Loss: 0.1079\n",
      "Epoch: [16/200], Step: [11/16], Loss: 0.1340\n",
      "Epoch: [16/200], Step: [12/16], Loss: 0.0716\n",
      "Epoch: [16/200], Step: [13/16], Loss: 0.0860\n",
      "Epoch: [16/200], Step: [14/16], Loss: 0.0868\n",
      "Epoch: [16/200], Step: [15/16], Loss: 0.0904\n",
      "Epoch: [17/200], Step: [1/16], Loss: 0.0744\n",
      "Epoch: [17/200], Step: [2/16], Loss: 0.0762\n",
      "Epoch: [17/200], Step: [3/16], Loss: 0.0756\n",
      "Epoch: [17/200], Step: [4/16], Loss: 0.0778\n",
      "Epoch: [17/200], Step: [5/16], Loss: 0.0680\n",
      "Epoch: [17/200], Step: [6/16], Loss: 0.0783\n",
      "Epoch: [17/200], Step: [7/16], Loss: 0.1149\n",
      "Epoch: [17/200], Step: [8/16], Loss: 0.0828\n",
      "Epoch: [17/200], Step: [9/16], Loss: 0.0995\n",
      "Epoch: [17/200], Step: [10/16], Loss: 0.1135\n",
      "Epoch: [17/200], Step: [11/16], Loss: 0.0879\n",
      "Epoch: [17/200], Step: [12/16], Loss: 0.0807\n",
      "Epoch: [17/200], Step: [13/16], Loss: 0.0690\n",
      "Epoch: [17/200], Step: [14/16], Loss: 0.0721\n",
      "Epoch: [17/200], Step: [15/16], Loss: 0.0725\n",
      "Epoch: [18/200], Step: [1/16], Loss: 0.0857\n",
      "Epoch: [18/200], Step: [2/16], Loss: 0.0706\n",
      "Epoch: [18/200], Step: [3/16], Loss: 0.0770\n",
      "Epoch: [18/200], Step: [4/16], Loss: 0.0579\n",
      "Epoch: [18/200], Step: [5/16], Loss: 0.0766\n",
      "Epoch: [18/200], Step: [6/16], Loss: 0.0772\n",
      "Epoch: [18/200], Step: [7/16], Loss: 0.0759\n",
      "Epoch: [18/200], Step: [8/16], Loss: 0.0626\n",
      "Epoch: [18/200], Step: [9/16], Loss: 0.0524\n",
      "Epoch: [18/200], Step: [10/16], Loss: 0.0705\n",
      "Epoch: [18/200], Step: [11/16], Loss: 0.0731\n",
      "Epoch: [18/200], Step: [12/16], Loss: 0.0795\n",
      "Epoch: [18/200], Step: [13/16], Loss: 0.0727\n",
      "Epoch: [18/200], Step: [14/16], Loss: 0.0695\n",
      "Epoch: [18/200], Step: [15/16], Loss: 0.0649\n",
      "Epoch: [19/200], Step: [1/16], Loss: 0.0664\n",
      "Epoch: [19/200], Step: [2/16], Loss: 0.0533\n",
      "Epoch: [19/200], Step: [3/16], Loss: 0.0572\n",
      "Epoch: [19/200], Step: [4/16], Loss: 0.0619\n",
      "Epoch: [19/200], Step: [5/16], Loss: 0.0611\n",
      "Epoch: [19/200], Step: [6/16], Loss: 0.0593\n",
      "Epoch: [19/200], Step: [7/16], Loss: 0.0650\n",
      "Epoch: [19/200], Step: [8/16], Loss: 0.0516\n",
      "Epoch: [19/200], Step: [9/16], Loss: 0.0849\n",
      "Epoch: [19/200], Step: [10/16], Loss: 0.0593\n",
      "Epoch: [19/200], Step: [11/16], Loss: 0.0807\n",
      "Epoch: [19/200], Step: [12/16], Loss: 0.0537\n",
      "Epoch: [19/200], Step: [13/16], Loss: 0.0594\n",
      "Epoch: [19/200], Step: [14/16], Loss: 0.0619\n",
      "Epoch: [19/200], Step: [15/16], Loss: 0.0551\n",
      "Epoch: [20/200], Step: [1/16], Loss: 0.0643\n",
      "Epoch: [20/200], Step: [2/16], Loss: 0.0503\n",
      "Epoch: [20/200], Step: [3/16], Loss: 0.0544\n",
      "Epoch: [20/200], Step: [4/16], Loss: 0.0623\n",
      "Epoch: [20/200], Step: [5/16], Loss: 0.0525\n",
      "Epoch: [20/200], Step: [6/16], Loss: 0.0504\n",
      "Epoch: [20/200], Step: [7/16], Loss: 0.0464\n",
      "Epoch: [20/200], Step: [8/16], Loss: 0.0410\n",
      "Epoch: [20/200], Step: [9/16], Loss: 0.0448\n",
      "Epoch: [20/200], Step: [10/16], Loss: 0.0428\n",
      "Epoch: [20/200], Step: [11/16], Loss: 0.0668\n",
      "Epoch: [20/200], Step: [12/16], Loss: 0.0556\n",
      "Epoch: [20/200], Step: [13/16], Loss: 0.0458\n",
      "Epoch: [20/200], Step: [14/16], Loss: 0.0645\n",
      "Epoch: [20/200], Step: [15/16], Loss: 0.0535\n",
      "Epoch: [21/200], Step: [1/16], Loss: 0.0673\n",
      "Epoch: [21/200], Step: [2/16], Loss: 0.0446\n",
      "Epoch: [21/200], Step: [3/16], Loss: 0.0418\n",
      "Epoch: [21/200], Step: [4/16], Loss: 0.0437\n",
      "Epoch: [21/200], Step: [5/16], Loss: 0.0535\n",
      "Epoch: [21/200], Step: [6/16], Loss: 0.0489\n",
      "Epoch: [21/200], Step: [7/16], Loss: 0.0311\n",
      "Epoch: [21/200], Step: [8/16], Loss: 0.0426\n",
      "Epoch: [21/200], Step: [9/16], Loss: 0.0474\n",
      "Epoch: [21/200], Step: [10/16], Loss: 0.0376\n",
      "Epoch: [21/200], Step: [11/16], Loss: 0.0537\n",
      "Epoch: [21/200], Step: [12/16], Loss: 0.0390\n",
      "Epoch: [21/200], Step: [13/16], Loss: 0.0505\n",
      "Epoch: [21/200], Step: [14/16], Loss: 0.0539\n",
      "Epoch: [21/200], Step: [15/16], Loss: 0.0527\n",
      "Epoch: [22/200], Step: [1/16], Loss: 0.0411\n",
      "Epoch: [22/200], Step: [2/16], Loss: 0.0403\n",
      "Epoch: [22/200], Step: [3/16], Loss: 0.0330\n",
      "Epoch: [22/200], Step: [4/16], Loss: 0.0380\n",
      "Epoch: [22/200], Step: [5/16], Loss: 0.0416\n",
      "Epoch: [22/200], Step: [6/16], Loss: 0.0526\n",
      "Epoch: [22/200], Step: [7/16], Loss: 0.0463\n",
      "Epoch: [22/200], Step: [8/16], Loss: 0.0398\n",
      "Epoch: [22/200], Step: [9/16], Loss: 0.0468\n",
      "Epoch: [22/200], Step: [10/16], Loss: 0.0436\n",
      "Epoch: [22/200], Step: [11/16], Loss: 0.0312\n",
      "Epoch: [22/200], Step: [12/16], Loss: 0.0501\n",
      "Epoch: [22/200], Step: [13/16], Loss: 0.0374\n",
      "Epoch: [22/200], Step: [14/16], Loss: 0.0332\n",
      "Epoch: [22/200], Step: [15/16], Loss: 0.0560\n",
      "Epoch: [23/200], Step: [1/16], Loss: 0.0460\n",
      "Epoch: [23/200], Step: [2/16], Loss: 0.0337\n",
      "Epoch: [23/200], Step: [3/16], Loss: 0.0358\n",
      "Epoch: [23/200], Step: [4/16], Loss: 0.0359\n",
      "Epoch: [23/200], Step: [5/16], Loss: 0.0431\n",
      "Epoch: [23/200], Step: [6/16], Loss: 0.0361\n",
      "Epoch: [23/200], Step: [7/16], Loss: 0.0376\n",
      "Epoch: [23/200], Step: [8/16], Loss: 0.0383\n",
      "Epoch: [23/200], Step: [9/16], Loss: 0.0476\n",
      "Epoch: [23/200], Step: [10/16], Loss: 0.0366\n",
      "Epoch: [23/200], Step: [11/16], Loss: 0.0373\n",
      "Epoch: [23/200], Step: [12/16], Loss: 0.0373\n",
      "Epoch: [23/200], Step: [13/16], Loss: 0.0359\n",
      "Epoch: [23/200], Step: [14/16], Loss: 0.0353\n",
      "Epoch: [23/200], Step: [15/16], Loss: 0.0300\n",
      "Epoch: [24/200], Step: [1/16], Loss: 0.0265\n",
      "Epoch: [24/200], Step: [2/16], Loss: 0.0329\n",
      "Epoch: [24/200], Step: [3/16], Loss: 0.0335\n",
      "Epoch: [24/200], Step: [4/16], Loss: 0.0358\n",
      "Epoch: [24/200], Step: [5/16], Loss: 0.0304\n",
      "Epoch: [24/200], Step: [6/16], Loss: 0.0362\n",
      "Epoch: [24/200], Step: [7/16], Loss: 0.0390\n",
      "Epoch: [24/200], Step: [8/16], Loss: 0.0316\n",
      "Epoch: [24/200], Step: [9/16], Loss: 0.0339\n",
      "Epoch: [24/200], Step: [10/16], Loss: 0.0341\n",
      "Epoch: [24/200], Step: [11/16], Loss: 0.0325\n",
      "Epoch: [24/200], Step: [12/16], Loss: 0.0313\n",
      "Epoch: [24/200], Step: [13/16], Loss: 0.0423\n",
      "Epoch: [24/200], Step: [14/16], Loss: 0.0336\n",
      "Epoch: [24/200], Step: [15/16], Loss: 0.0365\n",
      "Epoch: [25/200], Step: [1/16], Loss: 0.0280\n",
      "Epoch: [25/200], Step: [2/16], Loss: 0.0276\n",
      "Epoch: [25/200], Step: [3/16], Loss: 0.0334\n",
      "Epoch: [25/200], Step: [4/16], Loss: 0.0256\n",
      "Epoch: [25/200], Step: [5/16], Loss: 0.0316\n",
      "Epoch: [25/200], Step: [6/16], Loss: 0.0331\n",
      "Epoch: [25/200], Step: [7/16], Loss: 0.0307\n",
      "Epoch: [25/200], Step: [8/16], Loss: 0.0308\n",
      "Epoch: [25/200], Step: [9/16], Loss: 0.0273\n",
      "Epoch: [25/200], Step: [10/16], Loss: 0.0353\n",
      "Epoch: [25/200], Step: [11/16], Loss: 0.0317\n",
      "Epoch: [25/200], Step: [12/16], Loss: 0.0290\n",
      "Epoch: [25/200], Step: [13/16], Loss: 0.0352\n",
      "Epoch: [25/200], Step: [14/16], Loss: 0.0286\n",
      "Epoch: [25/200], Step: [15/16], Loss: 0.0315\n",
      "Epoch: [26/200], Step: [1/16], Loss: 0.0236\n",
      "Epoch: [26/200], Step: [2/16], Loss: 0.0247\n",
      "Epoch: [26/200], Step: [3/16], Loss: 0.0290\n",
      "Epoch: [26/200], Step: [4/16], Loss: 0.0230\n",
      "Epoch: [26/200], Step: [5/16], Loss: 0.0280\n",
      "Epoch: [26/200], Step: [6/16], Loss: 0.0263\n",
      "Epoch: [26/200], Step: [7/16], Loss: 0.0288\n",
      "Epoch: [26/200], Step: [8/16], Loss: 0.0284\n",
      "Epoch: [26/200], Step: [9/16], Loss: 0.0305\n",
      "Epoch: [26/200], Step: [10/16], Loss: 0.0330\n",
      "Epoch: [26/200], Step: [11/16], Loss: 0.0238\n",
      "Epoch: [26/200], Step: [12/16], Loss: 0.0296\n",
      "Epoch: [26/200], Step: [13/16], Loss: 0.0287\n",
      "Epoch: [26/200], Step: [14/16], Loss: 0.0278\n",
      "Epoch: [26/200], Step: [15/16], Loss: 0.0374\n",
      "Epoch: [27/200], Step: [1/16], Loss: 0.0328\n",
      "Epoch: [27/200], Step: [2/16], Loss: 0.0247\n",
      "Epoch: [27/200], Step: [3/16], Loss: 0.0181\n",
      "Epoch: [27/200], Step: [4/16], Loss: 0.0245\n",
      "Epoch: [27/200], Step: [5/16], Loss: 0.0243\n",
      "Epoch: [27/200], Step: [6/16], Loss: 0.0226\n",
      "Epoch: [27/200], Step: [7/16], Loss: 0.0324\n",
      "Epoch: [27/200], Step: [8/16], Loss: 0.0268\n",
      "Epoch: [27/200], Step: [9/16], Loss: 0.0247\n",
      "Epoch: [27/200], Step: [10/16], Loss: 0.0263\n",
      "Epoch: [27/200], Step: [11/16], Loss: 0.0264\n",
      "Epoch: [27/200], Step: [12/16], Loss: 0.0260\n",
      "Epoch: [27/200], Step: [13/16], Loss: 0.0237\n",
      "Epoch: [27/200], Step: [14/16], Loss: 0.0319\n",
      "Epoch: [27/200], Step: [15/16], Loss: 0.0238\n",
      "Epoch: [28/200], Step: [1/16], Loss: 0.0305\n",
      "Epoch: [28/200], Step: [2/16], Loss: 0.0308\n",
      "Epoch: [28/200], Step: [3/16], Loss: 0.0231\n",
      "Epoch: [28/200], Step: [4/16], Loss: 0.0296\n",
      "Epoch: [28/200], Step: [5/16], Loss: 0.0220\n",
      "Epoch: [28/200], Step: [6/16], Loss: 0.0193\n",
      "Epoch: [28/200], Step: [7/16], Loss: 0.0227\n",
      "Epoch: [28/200], Step: [8/16], Loss: 0.0236\n",
      "Epoch: [28/200], Step: [9/16], Loss: 0.0211\n",
      "Epoch: [28/200], Step: [10/16], Loss: 0.0219\n",
      "Epoch: [28/200], Step: [11/16], Loss: 0.0251\n",
      "Epoch: [28/200], Step: [12/16], Loss: 0.0198\n",
      "Epoch: [28/200], Step: [13/16], Loss: 0.0201\n",
      "Epoch: [28/200], Step: [14/16], Loss: 0.0259\n",
      "Epoch: [28/200], Step: [15/16], Loss: 0.0200\n",
      "Epoch: [29/200], Step: [1/16], Loss: 0.0205\n",
      "Epoch: [29/200], Step: [2/16], Loss: 0.0236\n",
      "Epoch: [29/200], Step: [3/16], Loss: 0.0269\n",
      "Epoch: [29/200], Step: [4/16], Loss: 0.0222\n",
      "Epoch: [29/200], Step: [5/16], Loss: 0.0186\n",
      "Epoch: [29/200], Step: [6/16], Loss: 0.0216\n",
      "Epoch: [29/200], Step: [7/16], Loss: 0.0222\n",
      "Epoch: [29/200], Step: [8/16], Loss: 0.0199\n",
      "Epoch: [29/200], Step: [9/16], Loss: 0.0241\n",
      "Epoch: [29/200], Step: [10/16], Loss: 0.0199\n",
      "Epoch: [29/200], Step: [11/16], Loss: 0.0208\n",
      "Epoch: [29/200], Step: [12/16], Loss: 0.0233\n",
      "Epoch: [29/200], Step: [13/16], Loss: 0.0195\n",
      "Epoch: [29/200], Step: [14/16], Loss: 0.0200\n",
      "Epoch: [29/200], Step: [15/16], Loss: 0.0238\n",
      "Epoch: [30/200], Step: [1/16], Loss: 0.0205\n",
      "Epoch: [30/200], Step: [2/16], Loss: 0.0216\n",
      "Epoch: [30/200], Step: [3/16], Loss: 0.0183\n",
      "Epoch: [30/200], Step: [4/16], Loss: 0.0208\n",
      "Epoch: [30/200], Step: [5/16], Loss: 0.0211\n",
      "Epoch: [30/200], Step: [6/16], Loss: 0.0178\n",
      "Epoch: [30/200], Step: [7/16], Loss: 0.0224\n",
      "Epoch: [30/200], Step: [8/16], Loss: 0.0210\n",
      "Epoch: [30/200], Step: [9/16], Loss: 0.0209\n",
      "Epoch: [30/200], Step: [10/16], Loss: 0.0208\n",
      "Epoch: [30/200], Step: [11/16], Loss: 0.0178\n",
      "Epoch: [30/200], Step: [12/16], Loss: 0.0185\n",
      "Epoch: [30/200], Step: [13/16], Loss: 0.0196\n",
      "Epoch: [30/200], Step: [14/16], Loss: 0.0191\n",
      "Epoch: [30/200], Step: [15/16], Loss: 0.0221\n",
      "Epoch: [31/200], Step: [1/16], Loss: 0.0225\n",
      "Epoch: [31/200], Step: [2/16], Loss: 0.0180\n",
      "Epoch: [31/200], Step: [3/16], Loss: 0.0163\n",
      "Epoch: [31/200], Step: [4/16], Loss: 0.0168\n",
      "Epoch: [31/200], Step: [5/16], Loss: 0.0210\n",
      "Epoch: [31/200], Step: [6/16], Loss: 0.0209\n",
      "Epoch: [31/200], Step: [7/16], Loss: 0.0193\n",
      "Epoch: [31/200], Step: [8/16], Loss: 0.0201\n",
      "Epoch: [31/200], Step: [9/16], Loss: 0.0142\n",
      "Epoch: [31/200], Step: [10/16], Loss: 0.0201\n",
      "Epoch: [31/200], Step: [11/16], Loss: 0.0169\n",
      "Epoch: [31/200], Step: [12/16], Loss: 0.0152\n",
      "Epoch: [31/200], Step: [13/16], Loss: 0.0193\n",
      "Epoch: [31/200], Step: [14/16], Loss: 0.0220\n",
      "Epoch: [31/200], Step: [15/16], Loss: 0.0191\n",
      "Epoch: [32/200], Step: [1/16], Loss: 0.0216\n",
      "Epoch: [32/200], Step: [2/16], Loss: 0.0180\n",
      "Epoch: [32/200], Step: [3/16], Loss: 0.0169\n",
      "Epoch: [32/200], Step: [4/16], Loss: 0.0185\n",
      "Epoch: [32/200], Step: [5/16], Loss: 0.0158\n",
      "Epoch: [32/200], Step: [6/16], Loss: 0.0241\n",
      "Epoch: [32/200], Step: [7/16], Loss: 0.0172\n",
      "Epoch: [32/200], Step: [8/16], Loss: 0.0146\n",
      "Epoch: [32/200], Step: [9/16], Loss: 0.0132\n",
      "Epoch: [32/200], Step: [10/16], Loss: 0.0176\n",
      "Epoch: [32/200], Step: [11/16], Loss: 0.0175\n",
      "Epoch: [32/200], Step: [12/16], Loss: 0.0147\n",
      "Epoch: [32/200], Step: [13/16], Loss: 0.0175\n",
      "Epoch: [32/200], Step: [14/16], Loss: 0.0198\n",
      "Epoch: [32/200], Step: [15/16], Loss: 0.0156\n",
      "Epoch: [33/200], Step: [1/16], Loss: 0.0141\n",
      "Epoch: [33/200], Step: [2/16], Loss: 0.0209\n",
      "Epoch: [33/200], Step: [3/16], Loss: 0.0143\n",
      "Epoch: [33/200], Step: [4/16], Loss: 0.0181\n",
      "Epoch: [33/200], Step: [5/16], Loss: 0.0145\n",
      "Epoch: [33/200], Step: [6/16], Loss: 0.0156\n",
      "Epoch: [33/200], Step: [7/16], Loss: 0.0188\n",
      "Epoch: [33/200], Step: [8/16], Loss: 0.0141\n",
      "Epoch: [33/200], Step: [9/16], Loss: 0.0172\n",
      "Epoch: [33/200], Step: [10/16], Loss: 0.0138\n",
      "Epoch: [33/200], Step: [11/16], Loss: 0.0197\n",
      "Epoch: [33/200], Step: [12/16], Loss: 0.0174\n",
      "Epoch: [33/200], Step: [13/16], Loss: 0.0147\n",
      "Epoch: [33/200], Step: [14/16], Loss: 0.0135\n",
      "Epoch: [33/200], Step: [15/16], Loss: 0.0197\n",
      "Epoch: [34/200], Step: [1/16], Loss: 0.0151\n",
      "Epoch: [34/200], Step: [2/16], Loss: 0.0177\n",
      "Epoch: [34/200], Step: [3/16], Loss: 0.0182\n",
      "Epoch: [34/200], Step: [4/16], Loss: 0.0147\n",
      "Epoch: [34/200], Step: [5/16], Loss: 0.0147\n",
      "Epoch: [34/200], Step: [6/16], Loss: 0.0148\n",
      "Epoch: [34/200], Step: [7/16], Loss: 0.0151\n",
      "Epoch: [34/200], Step: [8/16], Loss: 0.0133\n",
      "Epoch: [34/200], Step: [9/16], Loss: 0.0150\n",
      "Epoch: [34/200], Step: [10/16], Loss: 0.0171\n",
      "Epoch: [34/200], Step: [11/16], Loss: 0.0174\n",
      "Epoch: [34/200], Step: [12/16], Loss: 0.0144\n",
      "Epoch: [34/200], Step: [13/16], Loss: 0.0145\n",
      "Epoch: [34/200], Step: [14/16], Loss: 0.0146\n",
      "Epoch: [34/200], Step: [15/16], Loss: 0.0145\n",
      "Epoch: [35/200], Step: [1/16], Loss: 0.0172\n",
      "Epoch: [35/200], Step: [2/16], Loss: 0.0164\n",
      "Epoch: [35/200], Step: [3/16], Loss: 0.0133\n",
      "Epoch: [35/200], Step: [4/16], Loss: 0.0174\n",
      "Epoch: [35/200], Step: [5/16], Loss: 0.0146\n",
      "Epoch: [35/200], Step: [6/16], Loss: 0.0158\n",
      "Epoch: [35/200], Step: [7/16], Loss: 0.0117\n",
      "Epoch: [35/200], Step: [8/16], Loss: 0.0162\n",
      "Epoch: [35/200], Step: [9/16], Loss: 0.0148\n",
      "Epoch: [35/200], Step: [10/16], Loss: 0.0124\n",
      "Epoch: [35/200], Step: [11/16], Loss: 0.0130\n",
      "Epoch: [35/200], Step: [12/16], Loss: 0.0146\n",
      "Epoch: [35/200], Step: [13/16], Loss: 0.0134\n",
      "Epoch: [35/200], Step: [14/16], Loss: 0.0116\n",
      "Epoch: [35/200], Step: [15/16], Loss: 0.0156\n",
      "Epoch: [36/200], Step: [1/16], Loss: 0.0107\n",
      "Epoch: [36/200], Step: [2/16], Loss: 0.0106\n",
      "Epoch: [36/200], Step: [3/16], Loss: 0.0141\n",
      "Epoch: [36/200], Step: [4/16], Loss: 0.0155\n",
      "Epoch: [36/200], Step: [5/16], Loss: 0.0153\n",
      "Epoch: [36/200], Step: [6/16], Loss: 0.0118\n",
      "Epoch: [36/200], Step: [7/16], Loss: 0.0145\n",
      "Epoch: [36/200], Step: [8/16], Loss: 0.0154\n",
      "Epoch: [36/200], Step: [9/16], Loss: 0.0111\n",
      "Epoch: [36/200], Step: [10/16], Loss: 0.0138\n",
      "Epoch: [36/200], Step: [11/16], Loss: 0.0148\n",
      "Epoch: [36/200], Step: [12/16], Loss: 0.0153\n",
      "Epoch: [36/200], Step: [13/16], Loss: 0.0144\n",
      "Epoch: [36/200], Step: [14/16], Loss: 0.0131\n",
      "Epoch: [36/200], Step: [15/16], Loss: 0.0139\n",
      "Epoch: [37/200], Step: [1/16], Loss: 0.0131\n",
      "Epoch: [37/200], Step: [2/16], Loss: 0.0117\n",
      "Epoch: [37/200], Step: [3/16], Loss: 0.0119\n",
      "Epoch: [37/200], Step: [4/16], Loss: 0.0116\n",
      "Epoch: [37/200], Step: [5/16], Loss: 0.0142\n",
      "Epoch: [37/200], Step: [6/16], Loss: 0.0139\n",
      "Epoch: [37/200], Step: [7/16], Loss: 0.0128\n",
      "Epoch: [37/200], Step: [8/16], Loss: 0.0130\n",
      "Epoch: [37/200], Step: [9/16], Loss: 0.0129\n",
      "Epoch: [37/200], Step: [10/16], Loss: 0.0157\n",
      "Epoch: [37/200], Step: [11/16], Loss: 0.0138\n",
      "Epoch: [37/200], Step: [12/16], Loss: 0.0128\n",
      "Epoch: [37/200], Step: [13/16], Loss: 0.0139\n",
      "Epoch: [37/200], Step: [14/16], Loss: 0.0099\n",
      "Epoch: [37/200], Step: [15/16], Loss: 0.0126\n",
      "Epoch: [38/200], Step: [1/16], Loss: 0.0120\n",
      "Epoch: [38/200], Step: [2/16], Loss: 0.0107\n",
      "Epoch: [38/200], Step: [3/16], Loss: 0.0106\n",
      "Epoch: [38/200], Step: [4/16], Loss: 0.0136\n",
      "Epoch: [38/200], Step: [5/16], Loss: 0.0131\n",
      "Epoch: [38/200], Step: [6/16], Loss: 0.0132\n",
      "Epoch: [38/200], Step: [7/16], Loss: 0.0110\n",
      "Epoch: [38/200], Step: [8/16], Loss: 0.0117\n",
      "Epoch: [38/200], Step: [9/16], Loss: 0.0148\n",
      "Epoch: [38/200], Step: [10/16], Loss: 0.0097\n",
      "Epoch: [38/200], Step: [11/16], Loss: 0.0094\n",
      "Epoch: [38/200], Step: [12/16], Loss: 0.0142\n",
      "Epoch: [38/200], Step: [13/16], Loss: 0.0103\n",
      "Epoch: [38/200], Step: [14/16], Loss: 0.0136\n",
      "Epoch: [38/200], Step: [15/16], Loss: 0.0139\n",
      "Epoch: [39/200], Step: [1/16], Loss: 0.0105\n",
      "Epoch: [39/200], Step: [2/16], Loss: 0.0114\n",
      "Epoch: [39/200], Step: [3/16], Loss: 0.0117\n",
      "Epoch: [39/200], Step: [4/16], Loss: 0.0103\n",
      "Epoch: [39/200], Step: [5/16], Loss: 0.0144\n",
      "Epoch: [39/200], Step: [6/16], Loss: 0.0119\n",
      "Epoch: [39/200], Step: [7/16], Loss: 0.0119\n",
      "Epoch: [39/200], Step: [8/16], Loss: 0.0135\n",
      "Epoch: [39/200], Step: [9/16], Loss: 0.0107\n",
      "Epoch: [39/200], Step: [10/16], Loss: 0.0118\n",
      "Epoch: [39/200], Step: [11/16], Loss: 0.0082\n",
      "Epoch: [39/200], Step: [12/16], Loss: 0.0132\n",
      "Epoch: [39/200], Step: [13/16], Loss: 0.0104\n",
      "Epoch: [39/200], Step: [14/16], Loss: 0.0085\n",
      "Epoch: [39/200], Step: [15/16], Loss: 0.0138\n",
      "Epoch: [40/200], Step: [1/16], Loss: 0.0121\n",
      "Epoch: [40/200], Step: [2/16], Loss: 0.0116\n",
      "Epoch: [40/200], Step: [3/16], Loss: 0.0092\n",
      "Epoch: [40/200], Step: [4/16], Loss: 0.0100\n",
      "Epoch: [40/200], Step: [5/16], Loss: 0.0116\n",
      "Epoch: [40/200], Step: [6/16], Loss: 0.0134\n",
      "Epoch: [40/200], Step: [7/16], Loss: 0.0112\n",
      "Epoch: [40/200], Step: [8/16], Loss: 0.0096\n",
      "Epoch: [40/200], Step: [9/16], Loss: 0.0118\n",
      "Epoch: [40/200], Step: [10/16], Loss: 0.0104\n",
      "Epoch: [40/200], Step: [11/16], Loss: 0.0086\n",
      "Epoch: [40/200], Step: [12/16], Loss: 0.0126\n",
      "Epoch: [40/200], Step: [13/16], Loss: 0.0107\n",
      "Epoch: [40/200], Step: [14/16], Loss: 0.0081\n",
      "Epoch: [40/200], Step: [15/16], Loss: 0.0126\n",
      "Epoch: [41/200], Step: [1/16], Loss: 0.0098\n",
      "Epoch: [41/200], Step: [2/16], Loss: 0.0091\n",
      "Epoch: [41/200], Step: [3/16], Loss: 0.0107\n",
      "Epoch: [41/200], Step: [4/16], Loss: 0.0116\n",
      "Epoch: [41/200], Step: [5/16], Loss: 0.0090\n",
      "Epoch: [41/200], Step: [6/16], Loss: 0.0121\n",
      "Epoch: [41/200], Step: [7/16], Loss: 0.0097\n",
      "Epoch: [41/200], Step: [8/16], Loss: 0.0105\n",
      "Epoch: [41/200], Step: [9/16], Loss: 0.0111\n",
      "Epoch: [41/200], Step: [10/16], Loss: 0.0088\n",
      "Epoch: [41/200], Step: [11/16], Loss: 0.0096\n",
      "Epoch: [41/200], Step: [12/16], Loss: 0.0104\n",
      "Epoch: [41/200], Step: [13/16], Loss: 0.0119\n",
      "Epoch: [41/200], Step: [14/16], Loss: 0.0111\n",
      "Epoch: [41/200], Step: [15/16], Loss: 0.0097\n",
      "Epoch: [42/200], Step: [1/16], Loss: 0.0102\n",
      "Epoch: [42/200], Step: [2/16], Loss: 0.0100\n",
      "Epoch: [42/200], Step: [3/16], Loss: 0.0105\n",
      "Epoch: [42/200], Step: [4/16], Loss: 0.0098\n",
      "Epoch: [42/200], Step: [5/16], Loss: 0.0092\n",
      "Epoch: [42/200], Step: [6/16], Loss: 0.0109\n",
      "Epoch: [42/200], Step: [7/16], Loss: 0.0109\n",
      "Epoch: [42/200], Step: [8/16], Loss: 0.0086\n",
      "Epoch: [42/200], Step: [9/16], Loss: 0.0124\n",
      "Epoch: [42/200], Step: [10/16], Loss: 0.0096\n",
      "Epoch: [42/200], Step: [11/16], Loss: 0.0082\n",
      "Epoch: [42/200], Step: [12/16], Loss: 0.0083\n",
      "Epoch: [42/200], Step: [13/16], Loss: 0.0075\n",
      "Epoch: [42/200], Step: [14/16], Loss: 0.0100\n",
      "Epoch: [42/200], Step: [15/16], Loss: 0.0115\n",
      "Epoch: [43/200], Step: [1/16], Loss: 0.0077\n",
      "Epoch: [43/200], Step: [2/16], Loss: 0.0097\n",
      "Epoch: [43/200], Step: [3/16], Loss: 0.0108\n",
      "Epoch: [43/200], Step: [4/16], Loss: 0.0107\n",
      "Epoch: [43/200], Step: [5/16], Loss: 0.0108\n",
      "Epoch: [43/200], Step: [6/16], Loss: 0.0078\n",
      "Epoch: [43/200], Step: [7/16], Loss: 0.0083\n",
      "Epoch: [43/200], Step: [8/16], Loss: 0.0102\n",
      "Epoch: [43/200], Step: [9/16], Loss: 0.0086\n",
      "Epoch: [43/200], Step: [10/16], Loss: 0.0091\n",
      "Epoch: [43/200], Step: [11/16], Loss: 0.0078\n",
      "Epoch: [43/200], Step: [12/16], Loss: 0.0092\n",
      "Epoch: [43/200], Step: [13/16], Loss: 0.0101\n",
      "Epoch: [43/200], Step: [14/16], Loss: 0.0099\n",
      "Epoch: [43/200], Step: [15/16], Loss: 0.0094\n",
      "Epoch: [44/200], Step: [1/16], Loss: 0.0084\n",
      "Epoch: [44/200], Step: [2/16], Loss: 0.0080\n",
      "Epoch: [44/200], Step: [3/16], Loss: 0.0081\n",
      "Epoch: [44/200], Step: [4/16], Loss: 0.0087\n",
      "Epoch: [44/200], Step: [5/16], Loss: 0.0094\n",
      "Epoch: [44/200], Step: [6/16], Loss: 0.0088\n",
      "Epoch: [44/200], Step: [7/16], Loss: 0.0086\n",
      "Epoch: [44/200], Step: [8/16], Loss: 0.0092\n",
      "Epoch: [44/200], Step: [9/16], Loss: 0.0093\n",
      "Epoch: [44/200], Step: [10/16], Loss: 0.0103\n",
      "Epoch: [44/200], Step: [11/16], Loss: 0.0091\n",
      "Epoch: [44/200], Step: [12/16], Loss: 0.0113\n",
      "Epoch: [44/200], Step: [13/16], Loss: 0.0080\n",
      "Epoch: [44/200], Step: [14/16], Loss: 0.0084\n",
      "Epoch: [44/200], Step: [15/16], Loss: 0.0085\n",
      "Epoch: [45/200], Step: [1/16], Loss: 0.0072\n",
      "Epoch: [45/200], Step: [2/16], Loss: 0.0101\n",
      "Epoch: [45/200], Step: [3/16], Loss: 0.0076\n",
      "Epoch: [45/200], Step: [4/16], Loss: 0.0081\n",
      "Epoch: [45/200], Step: [5/16], Loss: 0.0111\n",
      "Epoch: [45/200], Step: [6/16], Loss: 0.0058\n",
      "Epoch: [45/200], Step: [7/16], Loss: 0.0092\n",
      "Epoch: [45/200], Step: [8/16], Loss: 0.0087\n",
      "Epoch: [45/200], Step: [9/16], Loss: 0.0084\n",
      "Epoch: [45/200], Step: [10/16], Loss: 0.0075\n",
      "Epoch: [45/200], Step: [11/16], Loss: 0.0072\n",
      "Epoch: [45/200], Step: [12/16], Loss: 0.0095\n",
      "Epoch: [45/200], Step: [13/16], Loss: 0.0097\n",
      "Epoch: [45/200], Step: [14/16], Loss: 0.0096\n",
      "Epoch: [45/200], Step: [15/16], Loss: 0.0084\n",
      "Epoch: [46/200], Step: [1/16], Loss: 0.0086\n",
      "Epoch: [46/200], Step: [2/16], Loss: 0.0082\n",
      "Epoch: [46/200], Step: [3/16], Loss: 0.0084\n",
      "Epoch: [46/200], Step: [4/16], Loss: 0.0088\n",
      "Epoch: [46/200], Step: [5/16], Loss: 0.0087\n",
      "Epoch: [46/200], Step: [6/16], Loss: 0.0075\n",
      "Epoch: [46/200], Step: [7/16], Loss: 0.0071\n",
      "Epoch: [46/200], Step: [8/16], Loss: 0.0078\n",
      "Epoch: [46/200], Step: [9/16], Loss: 0.0072\n",
      "Epoch: [46/200], Step: [10/16], Loss: 0.0074\n",
      "Epoch: [46/200], Step: [11/16], Loss: 0.0071\n",
      "Epoch: [46/200], Step: [12/16], Loss: 0.0086\n",
      "Epoch: [46/200], Step: [13/16], Loss: 0.0073\n",
      "Epoch: [46/200], Step: [14/16], Loss: 0.0101\n",
      "Epoch: [46/200], Step: [15/16], Loss: 0.0097\n",
      "Epoch: [47/200], Step: [1/16], Loss: 0.0083\n",
      "Epoch: [47/200], Step: [2/16], Loss: 0.0090\n",
      "Epoch: [47/200], Step: [3/16], Loss: 0.0072\n",
      "Epoch: [47/200], Step: [4/16], Loss: 0.0078\n",
      "Epoch: [47/200], Step: [5/16], Loss: 0.0074\n",
      "Epoch: [47/200], Step: [6/16], Loss: 0.0082\n",
      "Epoch: [47/200], Step: [7/16], Loss: 0.0086\n",
      "Epoch: [47/200], Step: [8/16], Loss: 0.0077\n",
      "Epoch: [47/200], Step: [9/16], Loss: 0.0062\n",
      "Epoch: [47/200], Step: [10/16], Loss: 0.0078\n",
      "Epoch: [47/200], Step: [11/16], Loss: 0.0073\n",
      "Epoch: [47/200], Step: [12/16], Loss: 0.0076\n",
      "Epoch: [47/200], Step: [13/16], Loss: 0.0082\n",
      "Epoch: [47/200], Step: [14/16], Loss: 0.0080\n",
      "Epoch: [47/200], Step: [15/16], Loss: 0.0080\n",
      "Epoch: [48/200], Step: [1/16], Loss: 0.0070\n",
      "Epoch: [48/200], Step: [2/16], Loss: 0.0070\n",
      "Epoch: [48/200], Step: [3/16], Loss: 0.0066\n",
      "Epoch: [48/200], Step: [4/16], Loss: 0.0076\n",
      "Epoch: [48/200], Step: [5/16], Loss: 0.0063\n",
      "Epoch: [48/200], Step: [6/16], Loss: 0.0074\n",
      "Epoch: [48/200], Step: [7/16], Loss: 0.0072\n",
      "Epoch: [48/200], Step: [8/16], Loss: 0.0069\n",
      "Epoch: [48/200], Step: [9/16], Loss: 0.0090\n",
      "Epoch: [48/200], Step: [10/16], Loss: 0.0072\n",
      "Epoch: [48/200], Step: [11/16], Loss: 0.0067\n",
      "Epoch: [48/200], Step: [12/16], Loss: 0.0095\n",
      "Epoch: [48/200], Step: [13/16], Loss: 0.0067\n",
      "Epoch: [48/200], Step: [14/16], Loss: 0.0078\n",
      "Epoch: [48/200], Step: [15/16], Loss: 0.0093\n",
      "Epoch: [49/200], Step: [1/16], Loss: 0.0068\n",
      "Epoch: [49/200], Step: [2/16], Loss: 0.0082\n",
      "Epoch: [49/200], Step: [3/16], Loss: 0.0068\n",
      "Epoch: [49/200], Step: [4/16], Loss: 0.0079\n",
      "Epoch: [49/200], Step: [5/16], Loss: 0.0060\n",
      "Epoch: [49/200], Step: [6/16], Loss: 0.0057\n",
      "Epoch: [49/200], Step: [7/16], Loss: 0.0072\n",
      "Epoch: [49/200], Step: [8/16], Loss: 0.0070\n",
      "Epoch: [49/200], Step: [9/16], Loss: 0.0068\n",
      "Epoch: [49/200], Step: [10/16], Loss: 0.0078\n",
      "Epoch: [49/200], Step: [11/16], Loss: 0.0080\n",
      "Epoch: [49/200], Step: [12/16], Loss: 0.0079\n",
      "Epoch: [49/200], Step: [13/16], Loss: 0.0075\n",
      "Epoch: [49/200], Step: [14/16], Loss: 0.0067\n",
      "Epoch: [49/200], Step: [15/16], Loss: 0.0070\n",
      "Epoch: [50/200], Step: [1/16], Loss: 0.0066\n",
      "Epoch: [50/200], Step: [2/16], Loss: 0.0069\n",
      "Epoch: [50/200], Step: [3/16], Loss: 0.0055\n",
      "Epoch: [50/200], Step: [4/16], Loss: 0.0073\n",
      "Epoch: [50/200], Step: [5/16], Loss: 0.0072\n",
      "Epoch: [50/200], Step: [6/16], Loss: 0.0060\n",
      "Epoch: [50/200], Step: [7/16], Loss: 0.0070\n",
      "Epoch: [50/200], Step: [8/16], Loss: 0.0061\n",
      "Epoch: [50/200], Step: [9/16], Loss: 0.0070\n",
      "Epoch: [50/200], Step: [10/16], Loss: 0.0073\n",
      "Epoch: [50/200], Step: [11/16], Loss: 0.0067\n",
      "Epoch: [50/200], Step: [12/16], Loss: 0.0067\n",
      "Epoch: [50/200], Step: [13/16], Loss: 0.0063\n",
      "Epoch: [50/200], Step: [14/16], Loss: 0.0103\n",
      "Epoch: [50/200], Step: [15/16], Loss: 0.0065\n",
      "Epoch: [51/200], Step: [1/16], Loss: 0.0064\n",
      "Epoch: [51/200], Step: [2/16], Loss: 0.0070\n",
      "Epoch: [51/200], Step: [3/16], Loss: 0.0065\n",
      "Epoch: [51/200], Step: [4/16], Loss: 0.0064\n",
      "Epoch: [51/200], Step: [5/16], Loss: 0.0062\n",
      "Epoch: [51/200], Step: [6/16], Loss: 0.0050\n",
      "Epoch: [51/200], Step: [7/16], Loss: 0.0053\n",
      "Epoch: [51/200], Step: [8/16], Loss: 0.0070\n",
      "Epoch: [51/200], Step: [9/16], Loss: 0.0074\n",
      "Epoch: [51/200], Step: [10/16], Loss: 0.0066\n",
      "Epoch: [51/200], Step: [11/16], Loss: 0.0053\n",
      "Epoch: [51/200], Step: [12/16], Loss: 0.0077\n",
      "Epoch: [51/200], Step: [13/16], Loss: 0.0077\n",
      "Epoch: [51/200], Step: [14/16], Loss: 0.0072\n",
      "Epoch: [51/200], Step: [15/16], Loss: 0.0080\n",
      "Epoch: [52/200], Step: [1/16], Loss: 0.0066\n",
      "Epoch: [52/200], Step: [2/16], Loss: 0.0049\n",
      "Epoch: [52/200], Step: [3/16], Loss: 0.0062\n",
      "Epoch: [52/200], Step: [4/16], Loss: 0.0053\n",
      "Epoch: [52/200], Step: [5/16], Loss: 0.0075\n",
      "Epoch: [52/200], Step: [6/16], Loss: 0.0055\n",
      "Epoch: [52/200], Step: [7/16], Loss: 0.0062\n",
      "Epoch: [52/200], Step: [8/16], Loss: 0.0069\n",
      "Epoch: [52/200], Step: [9/16], Loss: 0.0065\n",
      "Epoch: [52/200], Step: [10/16], Loss: 0.0071\n",
      "Epoch: [52/200], Step: [11/16], Loss: 0.0066\n",
      "Epoch: [52/200], Step: [12/16], Loss: 0.0060\n",
      "Epoch: [52/200], Step: [13/16], Loss: 0.0067\n",
      "Epoch: [52/200], Step: [14/16], Loss: 0.0062\n",
      "Epoch: [52/200], Step: [15/16], Loss: 0.0068\n",
      "Epoch: [53/200], Step: [1/16], Loss: 0.0064\n",
      "Epoch: [53/200], Step: [2/16], Loss: 0.0067\n",
      "Epoch: [53/200], Step: [3/16], Loss: 0.0055\n",
      "Epoch: [53/200], Step: [4/16], Loss: 0.0064\n",
      "Epoch: [53/200], Step: [5/16], Loss: 0.0056\n",
      "Epoch: [53/200], Step: [6/16], Loss: 0.0060\n",
      "Epoch: [53/200], Step: [7/16], Loss: 0.0082\n",
      "Epoch: [53/200], Step: [8/16], Loss: 0.0053\n",
      "Epoch: [53/200], Step: [9/16], Loss: 0.0057\n",
      "Epoch: [53/200], Step: [10/16], Loss: 0.0061\n",
      "Epoch: [53/200], Step: [11/16], Loss: 0.0054\n",
      "Epoch: [53/200], Step: [12/16], Loss: 0.0069\n",
      "Epoch: [53/200], Step: [13/16], Loss: 0.0060\n",
      "Epoch: [53/200], Step: [14/16], Loss: 0.0066\n",
      "Epoch: [53/200], Step: [15/16], Loss: 0.0051\n",
      "Epoch: [54/200], Step: [1/16], Loss: 0.0053\n",
      "Epoch: [54/200], Step: [2/16], Loss: 0.0055\n",
      "Epoch: [54/200], Step: [3/16], Loss: 0.0056\n",
      "Epoch: [54/200], Step: [4/16], Loss: 0.0058\n",
      "Epoch: [54/200], Step: [5/16], Loss: 0.0064\n",
      "Epoch: [54/200], Step: [6/16], Loss: 0.0065\n",
      "Epoch: [54/200], Step: [7/16], Loss: 0.0060\n",
      "Epoch: [54/200], Step: [8/16], Loss: 0.0060\n",
      "Epoch: [54/200], Step: [9/16], Loss: 0.0053\n",
      "Epoch: [54/200], Step: [10/16], Loss: 0.0061\n",
      "Epoch: [54/200], Step: [11/16], Loss: 0.0066\n",
      "Epoch: [54/200], Step: [12/16], Loss: 0.0058\n",
      "Epoch: [54/200], Step: [13/16], Loss: 0.0056\n",
      "Epoch: [54/200], Step: [14/16], Loss: 0.0062\n",
      "Epoch: [54/200], Step: [15/16], Loss: 0.0058\n",
      "Epoch: [55/200], Step: [1/16], Loss: 0.0048\n",
      "Epoch: [55/200], Step: [2/16], Loss: 0.0063\n",
      "Epoch: [55/200], Step: [3/16], Loss: 0.0051\n",
      "Epoch: [55/200], Step: [4/16], Loss: 0.0059\n",
      "Epoch: [55/200], Step: [5/16], Loss: 0.0062\n",
      "Epoch: [55/200], Step: [6/16], Loss: 0.0045\n",
      "Epoch: [55/200], Step: [7/16], Loss: 0.0062\n",
      "Epoch: [55/200], Step: [8/16], Loss: 0.0061\n",
      "Epoch: [55/200], Step: [9/16], Loss: 0.0043\n",
      "Epoch: [55/200], Step: [10/16], Loss: 0.0067\n",
      "Epoch: [55/200], Step: [11/16], Loss: 0.0046\n",
      "Epoch: [55/200], Step: [12/16], Loss: 0.0056\n",
      "Epoch: [55/200], Step: [13/16], Loss: 0.0058\n",
      "Epoch: [55/200], Step: [14/16], Loss: 0.0059\n",
      "Epoch: [55/200], Step: [15/16], Loss: 0.0068\n",
      "Epoch: [56/200], Step: [1/16], Loss: 0.0047\n",
      "Epoch: [56/200], Step: [2/16], Loss: 0.0058\n",
      "Epoch: [56/200], Step: [3/16], Loss: 0.0053\n",
      "Epoch: [56/200], Step: [4/16], Loss: 0.0051\n",
      "Epoch: [56/200], Step: [5/16], Loss: 0.0061\n",
      "Epoch: [56/200], Step: [6/16], Loss: 0.0057\n",
      "Epoch: [56/200], Step: [7/16], Loss: 0.0043\n",
      "Epoch: [56/200], Step: [8/16], Loss: 0.0063\n",
      "Epoch: [56/200], Step: [9/16], Loss: 0.0059\n",
      "Epoch: [56/200], Step: [10/16], Loss: 0.0053\n",
      "Epoch: [56/200], Step: [11/16], Loss: 0.0061\n",
      "Epoch: [56/200], Step: [12/16], Loss: 0.0063\n",
      "Epoch: [56/200], Step: [13/16], Loss: 0.0052\n",
      "Epoch: [56/200], Step: [14/16], Loss: 0.0051\n",
      "Epoch: [56/200], Step: [15/16], Loss: 0.0049\n",
      "Epoch: [57/200], Step: [1/16], Loss: 0.0053\n",
      "Epoch: [57/200], Step: [2/16], Loss: 0.0053\n",
      "Epoch: [57/200], Step: [3/16], Loss: 0.0056\n",
      "Epoch: [57/200], Step: [4/16], Loss: 0.0053\n",
      "Epoch: [57/200], Step: [5/16], Loss: 0.0065\n",
      "Epoch: [57/200], Step: [6/16], Loss: 0.0058\n",
      "Epoch: [57/200], Step: [7/16], Loss: 0.0047\n",
      "Epoch: [57/200], Step: [8/16], Loss: 0.0057\n",
      "Epoch: [57/200], Step: [9/16], Loss: 0.0060\n",
      "Epoch: [57/200], Step: [10/16], Loss: 0.0046\n",
      "Epoch: [57/200], Step: [11/16], Loss: 0.0049\n",
      "Epoch: [57/200], Step: [12/16], Loss: 0.0045\n",
      "Epoch: [57/200], Step: [13/16], Loss: 0.0044\n",
      "Epoch: [57/200], Step: [14/16], Loss: 0.0051\n",
      "Epoch: [57/200], Step: [15/16], Loss: 0.0054\n",
      "Epoch: [58/200], Step: [1/16], Loss: 0.0057\n",
      "Epoch: [58/200], Step: [2/16], Loss: 0.0050\n",
      "Epoch: [58/200], Step: [3/16], Loss: 0.0037\n",
      "Epoch: [58/200], Step: [4/16], Loss: 0.0060\n",
      "Epoch: [58/200], Step: [5/16], Loss: 0.0054\n",
      "Epoch: [58/200], Step: [6/16], Loss: 0.0049\n",
      "Epoch: [58/200], Step: [7/16], Loss: 0.0050\n",
      "Epoch: [58/200], Step: [8/16], Loss: 0.0052\n",
      "Epoch: [58/200], Step: [9/16], Loss: 0.0042\n",
      "Epoch: [58/200], Step: [10/16], Loss: 0.0045\n",
      "Epoch: [58/200], Step: [11/16], Loss: 0.0058\n",
      "Epoch: [58/200], Step: [12/16], Loss: 0.0054\n",
      "Epoch: [58/200], Step: [13/16], Loss: 0.0059\n",
      "Epoch: [58/200], Step: [14/16], Loss: 0.0053\n",
      "Epoch: [58/200], Step: [15/16], Loss: 0.0044\n",
      "Epoch: [59/200], Step: [1/16], Loss: 0.0061\n",
      "Epoch: [59/200], Step: [2/16], Loss: 0.0049\n",
      "Epoch: [59/200], Step: [3/16], Loss: 0.0045\n",
      "Epoch: [59/200], Step: [4/16], Loss: 0.0041\n",
      "Epoch: [59/200], Step: [5/16], Loss: 0.0049\n",
      "Epoch: [59/200], Step: [6/16], Loss: 0.0057\n",
      "Epoch: [59/200], Step: [7/16], Loss: 0.0041\n",
      "Epoch: [59/200], Step: [8/16], Loss: 0.0046\n",
      "Epoch: [59/200], Step: [9/16], Loss: 0.0054\n",
      "Epoch: [59/200], Step: [10/16], Loss: 0.0044\n",
      "Epoch: [59/200], Step: [11/16], Loss: 0.0041\n",
      "Epoch: [59/200], Step: [12/16], Loss: 0.0055\n",
      "Epoch: [59/200], Step: [13/16], Loss: 0.0043\n",
      "Epoch: [59/200], Step: [14/16], Loss: 0.0050\n",
      "Epoch: [59/200], Step: [15/16], Loss: 0.0063\n",
      "Epoch: [60/200], Step: [1/16], Loss: 0.0040\n",
      "Epoch: [60/200], Step: [2/16], Loss: 0.0049\n",
      "Epoch: [60/200], Step: [3/16], Loss: 0.0047\n",
      "Epoch: [60/200], Step: [4/16], Loss: 0.0051\n",
      "Epoch: [60/200], Step: [5/16], Loss: 0.0044\n",
      "Epoch: [60/200], Step: [6/16], Loss: 0.0050\n",
      "Epoch: [60/200], Step: [7/16], Loss: 0.0057\n",
      "Epoch: [60/200], Step: [8/16], Loss: 0.0044\n",
      "Epoch: [60/200], Step: [9/16], Loss: 0.0052\n",
      "Epoch: [60/200], Step: [10/16], Loss: 0.0038\n",
      "Epoch: [60/200], Step: [11/16], Loss: 0.0054\n",
      "Epoch: [60/200], Step: [12/16], Loss: 0.0045\n",
      "Epoch: [60/200], Step: [13/16], Loss: 0.0040\n",
      "Epoch: [60/200], Step: [14/16], Loss: 0.0049\n",
      "Epoch: [60/200], Step: [15/16], Loss: 0.0051\n",
      "Epoch: [61/200], Step: [1/16], Loss: 0.0051\n",
      "Epoch: [61/200], Step: [2/16], Loss: 0.0051\n",
      "Epoch: [61/200], Step: [3/16], Loss: 0.0047\n",
      "Epoch: [61/200], Step: [4/16], Loss: 0.0043\n",
      "Epoch: [61/200], Step: [5/16], Loss: 0.0053\n",
      "Epoch: [61/200], Step: [6/16], Loss: 0.0042\n",
      "Epoch: [61/200], Step: [7/16], Loss: 0.0040\n",
      "Epoch: [61/200], Step: [8/16], Loss: 0.0047\n",
      "Epoch: [61/200], Step: [9/16], Loss: 0.0057\n",
      "Epoch: [61/200], Step: [10/16], Loss: 0.0050\n",
      "Epoch: [61/200], Step: [11/16], Loss: 0.0040\n",
      "Epoch: [61/200], Step: [12/16], Loss: 0.0045\n",
      "Epoch: [61/200], Step: [13/16], Loss: 0.0040\n",
      "Epoch: [61/200], Step: [14/16], Loss: 0.0044\n",
      "Epoch: [61/200], Step: [15/16], Loss: 0.0037\n",
      "Epoch: [62/200], Step: [1/16], Loss: 0.0041\n",
      "Epoch: [62/200], Step: [2/16], Loss: 0.0045\n",
      "Epoch: [62/200], Step: [3/16], Loss: 0.0048\n",
      "Epoch: [62/200], Step: [4/16], Loss: 0.0052\n",
      "Epoch: [62/200], Step: [5/16], Loss: 0.0048\n",
      "Epoch: [62/200], Step: [6/16], Loss: 0.0043\n",
      "Epoch: [62/200], Step: [7/16], Loss: 0.0044\n",
      "Epoch: [62/200], Step: [8/16], Loss: 0.0038\n",
      "Epoch: [62/200], Step: [9/16], Loss: 0.0044\n",
      "Epoch: [62/200], Step: [10/16], Loss: 0.0043\n",
      "Epoch: [62/200], Step: [11/16], Loss: 0.0045\n",
      "Epoch: [62/200], Step: [12/16], Loss: 0.0040\n",
      "Epoch: [62/200], Step: [13/16], Loss: 0.0043\n",
      "Epoch: [62/200], Step: [14/16], Loss: 0.0041\n",
      "Epoch: [62/200], Step: [15/16], Loss: 0.0049\n",
      "Epoch: [63/200], Step: [1/16], Loss: 0.0043\n",
      "Epoch: [63/200], Step: [2/16], Loss: 0.0048\n",
      "Epoch: [63/200], Step: [3/16], Loss: 0.0043\n",
      "Epoch: [63/200], Step: [4/16], Loss: 0.0046\n",
      "Epoch: [63/200], Step: [5/16], Loss: 0.0050\n",
      "Epoch: [63/200], Step: [6/16], Loss: 0.0045\n",
      "Epoch: [63/200], Step: [7/16], Loss: 0.0032\n",
      "Epoch: [63/200], Step: [8/16], Loss: 0.0038\n",
      "Epoch: [63/200], Step: [9/16], Loss: 0.0043\n",
      "Epoch: [63/200], Step: [10/16], Loss: 0.0041\n",
      "Epoch: [63/200], Step: [11/16], Loss: 0.0039\n",
      "Epoch: [63/200], Step: [12/16], Loss: 0.0045\n",
      "Epoch: [63/200], Step: [13/16], Loss: 0.0046\n",
      "Epoch: [63/200], Step: [14/16], Loss: 0.0031\n",
      "Epoch: [63/200], Step: [15/16], Loss: 0.0053\n",
      "Epoch: [64/200], Step: [1/16], Loss: 0.0051\n",
      "Epoch: [64/200], Step: [2/16], Loss: 0.0036\n",
      "Epoch: [64/200], Step: [3/16], Loss: 0.0043\n",
      "Epoch: [64/200], Step: [4/16], Loss: 0.0041\n",
      "Epoch: [64/200], Step: [5/16], Loss: 0.0031\n",
      "Epoch: [64/200], Step: [6/16], Loss: 0.0046\n",
      "Epoch: [64/200], Step: [7/16], Loss: 0.0040\n",
      "Epoch: [64/200], Step: [8/16], Loss: 0.0041\n",
      "Epoch: [64/200], Step: [9/16], Loss: 0.0041\n",
      "Epoch: [64/200], Step: [10/16], Loss: 0.0037\n",
      "Epoch: [64/200], Step: [11/16], Loss: 0.0041\n",
      "Epoch: [64/200], Step: [12/16], Loss: 0.0046\n",
      "Epoch: [64/200], Step: [13/16], Loss: 0.0041\n",
      "Epoch: [64/200], Step: [14/16], Loss: 0.0041\n",
      "Epoch: [64/200], Step: [15/16], Loss: 0.0049\n",
      "Epoch: [65/200], Step: [1/16], Loss: 0.0035\n",
      "Epoch: [65/200], Step: [2/16], Loss: 0.0038\n",
      "Epoch: [65/200], Step: [3/16], Loss: 0.0036\n",
      "Epoch: [65/200], Step: [4/16], Loss: 0.0047\n",
      "Epoch: [65/200], Step: [5/16], Loss: 0.0042\n",
      "Epoch: [65/200], Step: [6/16], Loss: 0.0046\n",
      "Epoch: [65/200], Step: [7/16], Loss: 0.0037\n",
      "Epoch: [65/200], Step: [8/16], Loss: 0.0039\n",
      "Epoch: [65/200], Step: [9/16], Loss: 0.0043\n",
      "Epoch: [65/200], Step: [10/16], Loss: 0.0032\n",
      "Epoch: [65/200], Step: [11/16], Loss: 0.0045\n",
      "Epoch: [65/200], Step: [12/16], Loss: 0.0046\n",
      "Epoch: [65/200], Step: [13/16], Loss: 0.0040\n",
      "Epoch: [65/200], Step: [14/16], Loss: 0.0037\n",
      "Epoch: [65/200], Step: [15/16], Loss: 0.0043\n",
      "Epoch: [66/200], Step: [1/16], Loss: 0.0034\n",
      "Epoch: [66/200], Step: [2/16], Loss: 0.0030\n",
      "Epoch: [66/200], Step: [3/16], Loss: 0.0038\n",
      "Epoch: [66/200], Step: [4/16], Loss: 0.0038\n",
      "Epoch: [66/200], Step: [5/16], Loss: 0.0036\n",
      "Epoch: [66/200], Step: [6/16], Loss: 0.0045\n",
      "Epoch: [66/200], Step: [7/16], Loss: 0.0038\n",
      "Epoch: [66/200], Step: [8/16], Loss: 0.0037\n",
      "Epoch: [66/200], Step: [9/16], Loss: 0.0035\n",
      "Epoch: [66/200], Step: [10/16], Loss: 0.0043\n",
      "Epoch: [66/200], Step: [11/16], Loss: 0.0043\n",
      "Epoch: [66/200], Step: [12/16], Loss: 0.0039\n",
      "Epoch: [66/200], Step: [13/16], Loss: 0.0054\n",
      "Epoch: [66/200], Step: [14/16], Loss: 0.0035\n",
      "Epoch: [66/200], Step: [15/16], Loss: 0.0041\n",
      "Epoch: [67/200], Step: [1/16], Loss: 0.0044\n",
      "Epoch: [67/200], Step: [2/16], Loss: 0.0039\n",
      "Epoch: [67/200], Step: [3/16], Loss: 0.0040\n",
      "Epoch: [67/200], Step: [4/16], Loss: 0.0032\n",
      "Epoch: [67/200], Step: [5/16], Loss: 0.0043\n",
      "Epoch: [67/200], Step: [6/16], Loss: 0.0035\n",
      "Epoch: [67/200], Step: [7/16], Loss: 0.0033\n",
      "Epoch: [67/200], Step: [8/16], Loss: 0.0036\n",
      "Epoch: [67/200], Step: [9/16], Loss: 0.0039\n",
      "Epoch: [67/200], Step: [10/16], Loss: 0.0035\n",
      "Epoch: [67/200], Step: [11/16], Loss: 0.0033\n",
      "Epoch: [67/200], Step: [12/16], Loss: 0.0047\n",
      "Epoch: [67/200], Step: [13/16], Loss: 0.0037\n",
      "Epoch: [67/200], Step: [14/16], Loss: 0.0037\n",
      "Epoch: [67/200], Step: [15/16], Loss: 0.0041\n",
      "Epoch: [68/200], Step: [1/16], Loss: 0.0039\n",
      "Epoch: [68/200], Step: [2/16], Loss: 0.0035\n",
      "Epoch: [68/200], Step: [3/16], Loss: 0.0043\n",
      "Epoch: [68/200], Step: [4/16], Loss: 0.0029\n",
      "Epoch: [68/200], Step: [5/16], Loss: 0.0039\n",
      "Epoch: [68/200], Step: [6/16], Loss: 0.0035\n",
      "Epoch: [68/200], Step: [7/16], Loss: 0.0033\n",
      "Epoch: [68/200], Step: [8/16], Loss: 0.0040\n",
      "Epoch: [68/200], Step: [9/16], Loss: 0.0037\n",
      "Epoch: [68/200], Step: [10/16], Loss: 0.0029\n",
      "Epoch: [68/200], Step: [11/16], Loss: 0.0037\n",
      "Epoch: [68/200], Step: [12/16], Loss: 0.0037\n",
      "Epoch: [68/200], Step: [13/16], Loss: 0.0039\n",
      "Epoch: [68/200], Step: [14/16], Loss: 0.0037\n",
      "Epoch: [68/200], Step: [15/16], Loss: 0.0044\n",
      "Epoch: [69/200], Step: [1/16], Loss: 0.0048\n",
      "Epoch: [69/200], Step: [2/16], Loss: 0.0037\n",
      "Epoch: [69/200], Step: [3/16], Loss: 0.0038\n",
      "Epoch: [69/200], Step: [4/16], Loss: 0.0044\n",
      "Epoch: [69/200], Step: [5/16], Loss: 0.0030\n",
      "Epoch: [69/200], Step: [6/16], Loss: 0.0032\n",
      "Epoch: [69/200], Step: [7/16], Loss: 0.0029\n",
      "Epoch: [69/200], Step: [8/16], Loss: 0.0030\n",
      "Epoch: [69/200], Step: [9/16], Loss: 0.0036\n",
      "Epoch: [69/200], Step: [10/16], Loss: 0.0035\n",
      "Epoch: [69/200], Step: [11/16], Loss: 0.0034\n",
      "Epoch: [69/200], Step: [12/16], Loss: 0.0032\n",
      "Epoch: [69/200], Step: [13/16], Loss: 0.0040\n",
      "Epoch: [69/200], Step: [14/16], Loss: 0.0034\n",
      "Epoch: [69/200], Step: [15/16], Loss: 0.0038\n",
      "Epoch: [70/200], Step: [1/16], Loss: 0.0031\n",
      "Epoch: [70/200], Step: [2/16], Loss: 0.0038\n",
      "Epoch: [70/200], Step: [3/16], Loss: 0.0039\n",
      "Epoch: [70/200], Step: [4/16], Loss: 0.0035\n",
      "Epoch: [70/200], Step: [5/16], Loss: 0.0031\n",
      "Epoch: [70/200], Step: [6/16], Loss: 0.0036\n",
      "Epoch: [70/200], Step: [7/16], Loss: 0.0038\n",
      "Epoch: [70/200], Step: [8/16], Loss: 0.0034\n",
      "Epoch: [70/200], Step: [9/16], Loss: 0.0039\n",
      "Epoch: [70/200], Step: [10/16], Loss: 0.0037\n",
      "Epoch: [70/200], Step: [11/16], Loss: 0.0028\n",
      "Epoch: [70/200], Step: [12/16], Loss: 0.0037\n",
      "Epoch: [70/200], Step: [13/16], Loss: 0.0034\n",
      "Epoch: [70/200], Step: [14/16], Loss: 0.0029\n",
      "Epoch: [70/200], Step: [15/16], Loss: 0.0035\n",
      "Epoch: [71/200], Step: [1/16], Loss: 0.0034\n",
      "Epoch: [71/200], Step: [2/16], Loss: 0.0041\n",
      "Epoch: [71/200], Step: [3/16], Loss: 0.0034\n",
      "Epoch: [71/200], Step: [4/16], Loss: 0.0033\n",
      "Epoch: [71/200], Step: [5/16], Loss: 0.0026\n",
      "Epoch: [71/200], Step: [6/16], Loss: 0.0038\n",
      "Epoch: [71/200], Step: [7/16], Loss: 0.0029\n",
      "Epoch: [71/200], Step: [8/16], Loss: 0.0032\n",
      "Epoch: [71/200], Step: [9/16], Loss: 0.0029\n",
      "Epoch: [71/200], Step: [10/16], Loss: 0.0036\n",
      "Epoch: [71/200], Step: [11/16], Loss: 0.0034\n",
      "Epoch: [71/200], Step: [12/16], Loss: 0.0029\n",
      "Epoch: [71/200], Step: [13/16], Loss: 0.0028\n",
      "Epoch: [71/200], Step: [14/16], Loss: 0.0038\n",
      "Epoch: [71/200], Step: [15/16], Loss: 0.0043\n",
      "Epoch: [72/200], Step: [1/16], Loss: 0.0031\n",
      "Epoch: [72/200], Step: [2/16], Loss: 0.0026\n",
      "Epoch: [72/200], Step: [3/16], Loss: 0.0036\n",
      "Epoch: [72/200], Step: [4/16], Loss: 0.0031\n",
      "Epoch: [72/200], Step: [5/16], Loss: 0.0034\n",
      "Epoch: [72/200], Step: [6/16], Loss: 0.0030\n",
      "Epoch: [72/200], Step: [7/16], Loss: 0.0028\n",
      "Epoch: [72/200], Step: [8/16], Loss: 0.0032\n",
      "Epoch: [72/200], Step: [9/16], Loss: 0.0036\n",
      "Epoch: [72/200], Step: [10/16], Loss: 0.0033\n",
      "Epoch: [72/200], Step: [11/16], Loss: 0.0031\n",
      "Epoch: [72/200], Step: [12/16], Loss: 0.0031\n",
      "Epoch: [72/200], Step: [13/16], Loss: 0.0032\n",
      "Epoch: [72/200], Step: [14/16], Loss: 0.0042\n",
      "Epoch: [72/200], Step: [15/16], Loss: 0.0039\n",
      "Epoch: [73/200], Step: [1/16], Loss: 0.0036\n",
      "Epoch: [73/200], Step: [2/16], Loss: 0.0037\n",
      "Epoch: [73/200], Step: [3/16], Loss: 0.0030\n",
      "Epoch: [73/200], Step: [4/16], Loss: 0.0031\n",
      "Epoch: [73/200], Step: [5/16], Loss: 0.0036\n",
      "Epoch: [73/200], Step: [6/16], Loss: 0.0033\n",
      "Epoch: [73/200], Step: [7/16], Loss: 0.0028\n",
      "Epoch: [73/200], Step: [8/16], Loss: 0.0030\n",
      "Epoch: [73/200], Step: [9/16], Loss: 0.0035\n",
      "Epoch: [73/200], Step: [10/16], Loss: 0.0032\n",
      "Epoch: [73/200], Step: [11/16], Loss: 0.0032\n",
      "Epoch: [73/200], Step: [12/16], Loss: 0.0027\n",
      "Epoch: [73/200], Step: [13/16], Loss: 0.0027\n",
      "Epoch: [73/200], Step: [14/16], Loss: 0.0031\n",
      "Epoch: [73/200], Step: [15/16], Loss: 0.0034\n",
      "Epoch: [74/200], Step: [1/16], Loss: 0.0029\n",
      "Epoch: [74/200], Step: [2/16], Loss: 0.0038\n",
      "Epoch: [74/200], Step: [3/16], Loss: 0.0029\n",
      "Epoch: [74/200], Step: [4/16], Loss: 0.0031\n",
      "Epoch: [74/200], Step: [5/16], Loss: 0.0031\n",
      "Epoch: [74/200], Step: [6/16], Loss: 0.0034\n",
      "Epoch: [74/200], Step: [7/16], Loss: 0.0027\n",
      "Epoch: [74/200], Step: [8/16], Loss: 0.0028\n",
      "Epoch: [74/200], Step: [9/16], Loss: 0.0031\n",
      "Epoch: [74/200], Step: [10/16], Loss: 0.0028\n",
      "Epoch: [74/200], Step: [11/16], Loss: 0.0034\n",
      "Epoch: [74/200], Step: [12/16], Loss: 0.0036\n",
      "Epoch: [74/200], Step: [13/16], Loss: 0.0031\n",
      "Epoch: [74/200], Step: [14/16], Loss: 0.0027\n",
      "Epoch: [74/200], Step: [15/16], Loss: 0.0029\n",
      "Epoch: [75/200], Step: [1/16], Loss: 0.0031\n",
      "Epoch: [75/200], Step: [2/16], Loss: 0.0034\n",
      "Epoch: [75/200], Step: [3/16], Loss: 0.0034\n",
      "Epoch: [75/200], Step: [4/16], Loss: 0.0032\n",
      "Epoch: [75/200], Step: [5/16], Loss: 0.0028\n",
      "Epoch: [75/200], Step: [6/16], Loss: 0.0029\n",
      "Epoch: [75/200], Step: [7/16], Loss: 0.0026\n",
      "Epoch: [75/200], Step: [8/16], Loss: 0.0028\n",
      "Epoch: [75/200], Step: [9/16], Loss: 0.0029\n",
      "Epoch: [75/200], Step: [10/16], Loss: 0.0035\n",
      "Epoch: [75/200], Step: [11/16], Loss: 0.0030\n",
      "Epoch: [75/200], Step: [12/16], Loss: 0.0030\n",
      "Epoch: [75/200], Step: [13/16], Loss: 0.0026\n",
      "Epoch: [75/200], Step: [14/16], Loss: 0.0021\n",
      "Epoch: [75/200], Step: [15/16], Loss: 0.0038\n",
      "Epoch: [76/200], Step: [1/16], Loss: 0.0030\n",
      "Epoch: [76/200], Step: [2/16], Loss: 0.0032\n",
      "Epoch: [76/200], Step: [3/16], Loss: 0.0028\n",
      "Epoch: [76/200], Step: [4/16], Loss: 0.0024\n",
      "Epoch: [76/200], Step: [5/16], Loss: 0.0033\n",
      "Epoch: [76/200], Step: [6/16], Loss: 0.0031\n",
      "Epoch: [76/200], Step: [7/16], Loss: 0.0023\n",
      "Epoch: [76/200], Step: [8/16], Loss: 0.0027\n",
      "Epoch: [76/200], Step: [9/16], Loss: 0.0034\n",
      "Epoch: [76/200], Step: [10/16], Loss: 0.0027\n",
      "Epoch: [76/200], Step: [11/16], Loss: 0.0030\n",
      "Epoch: [76/200], Step: [12/16], Loss: 0.0032\n",
      "Epoch: [76/200], Step: [13/16], Loss: 0.0030\n",
      "Epoch: [76/200], Step: [14/16], Loss: 0.0030\n",
      "Epoch: [76/200], Step: [15/16], Loss: 0.0029\n",
      "Epoch: [77/200], Step: [1/16], Loss: 0.0024\n",
      "Epoch: [77/200], Step: [2/16], Loss: 0.0022\n",
      "Epoch: [77/200], Step: [3/16], Loss: 0.0030\n",
      "Epoch: [77/200], Step: [4/16], Loss: 0.0037\n",
      "Epoch: [77/200], Step: [5/16], Loss: 0.0027\n",
      "Epoch: [77/200], Step: [6/16], Loss: 0.0028\n",
      "Epoch: [77/200], Step: [7/16], Loss: 0.0032\n",
      "Epoch: [77/200], Step: [8/16], Loss: 0.0027\n",
      "Epoch: [77/200], Step: [9/16], Loss: 0.0025\n",
      "Epoch: [77/200], Step: [10/16], Loss: 0.0028\n",
      "Epoch: [77/200], Step: [11/16], Loss: 0.0031\n",
      "Epoch: [77/200], Step: [12/16], Loss: 0.0030\n",
      "Epoch: [77/200], Step: [13/16], Loss: 0.0029\n",
      "Epoch: [77/200], Step: [14/16], Loss: 0.0029\n",
      "Epoch: [77/200], Step: [15/16], Loss: 0.0028\n",
      "Epoch: [78/200], Step: [1/16], Loss: 0.0025\n",
      "Epoch: [78/200], Step: [2/16], Loss: 0.0030\n",
      "Epoch: [78/200], Step: [3/16], Loss: 0.0034\n",
      "Epoch: [78/200], Step: [4/16], Loss: 0.0029\n",
      "Epoch: [78/200], Step: [5/16], Loss: 0.0025\n",
      "Epoch: [78/200], Step: [6/16], Loss: 0.0032\n",
      "Epoch: [78/200], Step: [7/16], Loss: 0.0029\n",
      "Epoch: [78/200], Step: [8/16], Loss: 0.0026\n",
      "Epoch: [78/200], Step: [9/16], Loss: 0.0030\n",
      "Epoch: [78/200], Step: [10/16], Loss: 0.0028\n",
      "Epoch: [78/200], Step: [11/16], Loss: 0.0023\n",
      "Epoch: [78/200], Step: [12/16], Loss: 0.0024\n",
      "Epoch: [78/200], Step: [13/16], Loss: 0.0029\n",
      "Epoch: [78/200], Step: [14/16], Loss: 0.0029\n",
      "Epoch: [78/200], Step: [15/16], Loss: 0.0025\n",
      "Epoch: [79/200], Step: [1/16], Loss: 0.0025\n",
      "Epoch: [79/200], Step: [2/16], Loss: 0.0028\n",
      "Epoch: [79/200], Step: [3/16], Loss: 0.0032\n",
      "Epoch: [79/200], Step: [4/16], Loss: 0.0034\n",
      "Epoch: [79/200], Step: [5/16], Loss: 0.0028\n",
      "Epoch: [79/200], Step: [6/16], Loss: 0.0027\n",
      "Epoch: [79/200], Step: [7/16], Loss: 0.0031\n",
      "Epoch: [79/200], Step: [8/16], Loss: 0.0028\n",
      "Epoch: [79/200], Step: [9/16], Loss: 0.0023\n",
      "Epoch: [79/200], Step: [10/16], Loss: 0.0022\n",
      "Epoch: [79/200], Step: [11/16], Loss: 0.0024\n",
      "Epoch: [79/200], Step: [12/16], Loss: 0.0028\n",
      "Epoch: [79/200], Step: [13/16], Loss: 0.0027\n",
      "Epoch: [79/200], Step: [14/16], Loss: 0.0025\n",
      "Epoch: [79/200], Step: [15/16], Loss: 0.0023\n",
      "Epoch: [80/200], Step: [1/16], Loss: 0.0021\n",
      "Epoch: [80/200], Step: [2/16], Loss: 0.0024\n",
      "Epoch: [80/200], Step: [3/16], Loss: 0.0023\n",
      "Epoch: [80/200], Step: [4/16], Loss: 0.0028\n",
      "Epoch: [80/200], Step: [5/16], Loss: 0.0025\n",
      "Epoch: [80/200], Step: [6/16], Loss: 0.0026\n",
      "Epoch: [80/200], Step: [7/16], Loss: 0.0028\n",
      "Epoch: [80/200], Step: [8/16], Loss: 0.0029\n",
      "Epoch: [80/200], Step: [9/16], Loss: 0.0028\n",
      "Epoch: [80/200], Step: [10/16], Loss: 0.0029\n",
      "Epoch: [80/200], Step: [11/16], Loss: 0.0023\n",
      "Epoch: [80/200], Step: [12/16], Loss: 0.0032\n",
      "Epoch: [80/200], Step: [13/16], Loss: 0.0024\n",
      "Epoch: [80/200], Step: [14/16], Loss: 0.0026\n",
      "Epoch: [80/200], Step: [15/16], Loss: 0.0028\n",
      "Epoch: [81/200], Step: [1/16], Loss: 0.0027\n",
      "Epoch: [81/200], Step: [2/16], Loss: 0.0024\n",
      "Epoch: [81/200], Step: [3/16], Loss: 0.0025\n",
      "Epoch: [81/200], Step: [4/16], Loss: 0.0026\n",
      "Epoch: [81/200], Step: [5/16], Loss: 0.0029\n",
      "Epoch: [81/200], Step: [6/16], Loss: 0.0028\n",
      "Epoch: [81/200], Step: [7/16], Loss: 0.0023\n",
      "Epoch: [81/200], Step: [8/16], Loss: 0.0022\n",
      "Epoch: [81/200], Step: [9/16], Loss: 0.0024\n",
      "Epoch: [81/200], Step: [10/16], Loss: 0.0026\n",
      "Epoch: [81/200], Step: [11/16], Loss: 0.0026\n",
      "Epoch: [81/200], Step: [12/16], Loss: 0.0024\n",
      "Epoch: [81/200], Step: [13/16], Loss: 0.0028\n",
      "Epoch: [81/200], Step: [14/16], Loss: 0.0026\n",
      "Epoch: [81/200], Step: [15/16], Loss: 0.0029\n",
      "Epoch: [82/200], Step: [1/16], Loss: 0.0023\n",
      "Epoch: [82/200], Step: [2/16], Loss: 0.0030\n",
      "Epoch: [82/200], Step: [3/16], Loss: 0.0023\n",
      "Epoch: [82/200], Step: [4/16], Loss: 0.0027\n",
      "Epoch: [82/200], Step: [5/16], Loss: 0.0025\n",
      "Epoch: [82/200], Step: [6/16], Loss: 0.0021\n",
      "Epoch: [82/200], Step: [7/16], Loss: 0.0023\n",
      "Epoch: [82/200], Step: [8/16], Loss: 0.0027\n",
      "Epoch: [82/200], Step: [9/16], Loss: 0.0025\n",
      "Epoch: [82/200], Step: [10/16], Loss: 0.0026\n",
      "Epoch: [82/200], Step: [11/16], Loss: 0.0022\n",
      "Epoch: [82/200], Step: [12/16], Loss: 0.0030\n",
      "Epoch: [82/200], Step: [13/16], Loss: 0.0029\n",
      "Epoch: [82/200], Step: [14/16], Loss: 0.0025\n",
      "Epoch: [82/200], Step: [15/16], Loss: 0.0021\n",
      "Epoch: [83/200], Step: [1/16], Loss: 0.0026\n",
      "Epoch: [83/200], Step: [2/16], Loss: 0.0024\n",
      "Epoch: [83/200], Step: [3/16], Loss: 0.0028\n",
      "Epoch: [83/200], Step: [4/16], Loss: 0.0022\n",
      "Epoch: [83/200], Step: [5/16], Loss: 0.0025\n",
      "Epoch: [83/200], Step: [6/16], Loss: 0.0026\n",
      "Epoch: [83/200], Step: [7/16], Loss: 0.0023\n",
      "Epoch: [83/200], Step: [8/16], Loss: 0.0025\n",
      "Epoch: [83/200], Step: [9/16], Loss: 0.0025\n",
      "Epoch: [83/200], Step: [10/16], Loss: 0.0024\n",
      "Epoch: [83/200], Step: [11/16], Loss: 0.0025\n",
      "Epoch: [83/200], Step: [12/16], Loss: 0.0025\n",
      "Epoch: [83/200], Step: [13/16], Loss: 0.0023\n",
      "Epoch: [83/200], Step: [14/16], Loss: 0.0021\n",
      "Epoch: [83/200], Step: [15/16], Loss: 0.0026\n",
      "Epoch: [84/200], Step: [1/16], Loss: 0.0024\n",
      "Epoch: [84/200], Step: [2/16], Loss: 0.0021\n",
      "Epoch: [84/200], Step: [3/16], Loss: 0.0020\n",
      "Epoch: [84/200], Step: [4/16], Loss: 0.0027\n",
      "Epoch: [84/200], Step: [5/16], Loss: 0.0022\n",
      "Epoch: [84/200], Step: [6/16], Loss: 0.0024\n",
      "Epoch: [84/200], Step: [7/16], Loss: 0.0028\n",
      "Epoch: [84/200], Step: [8/16], Loss: 0.0023\n",
      "Epoch: [84/200], Step: [9/16], Loss: 0.0025\n",
      "Epoch: [84/200], Step: [10/16], Loss: 0.0028\n",
      "Epoch: [84/200], Step: [11/16], Loss: 0.0023\n",
      "Epoch: [84/200], Step: [12/16], Loss: 0.0025\n",
      "Epoch: [84/200], Step: [13/16], Loss: 0.0029\n",
      "Epoch: [84/200], Step: [14/16], Loss: 0.0020\n",
      "Epoch: [84/200], Step: [15/16], Loss: 0.0021\n",
      "Epoch: [85/200], Step: [1/16], Loss: 0.0021\n",
      "Epoch: [85/200], Step: [2/16], Loss: 0.0023\n",
      "Epoch: [85/200], Step: [3/16], Loss: 0.0028\n",
      "Epoch: [85/200], Step: [4/16], Loss: 0.0023\n",
      "Epoch: [85/200], Step: [5/16], Loss: 0.0022\n",
      "Epoch: [85/200], Step: [6/16], Loss: 0.0020\n",
      "Epoch: [85/200], Step: [7/16], Loss: 0.0025\n",
      "Epoch: [85/200], Step: [8/16], Loss: 0.0026\n",
      "Epoch: [85/200], Step: [9/16], Loss: 0.0022\n",
      "Epoch: [85/200], Step: [10/16], Loss: 0.0024\n",
      "Epoch: [85/200], Step: [11/16], Loss: 0.0025\n",
      "Epoch: [85/200], Step: [12/16], Loss: 0.0027\n",
      "Epoch: [85/200], Step: [13/16], Loss: 0.0025\n",
      "Epoch: [85/200], Step: [14/16], Loss: 0.0020\n",
      "Epoch: [85/200], Step: [15/16], Loss: 0.0018\n",
      "Epoch: [86/200], Step: [1/16], Loss: 0.0027\n",
      "Epoch: [86/200], Step: [2/16], Loss: 0.0019\n",
      "Epoch: [86/200], Step: [3/16], Loss: 0.0025\n",
      "Epoch: [86/200], Step: [4/16], Loss: 0.0027\n",
      "Epoch: [86/200], Step: [5/16], Loss: 0.0015\n",
      "Epoch: [86/200], Step: [6/16], Loss: 0.0025\n",
      "Epoch: [86/200], Step: [7/16], Loss: 0.0021\n",
      "Epoch: [86/200], Step: [8/16], Loss: 0.0025\n",
      "Epoch: [86/200], Step: [9/16], Loss: 0.0022\n",
      "Epoch: [86/200], Step: [10/16], Loss: 0.0023\n",
      "Epoch: [86/200], Step: [11/16], Loss: 0.0022\n",
      "Epoch: [86/200], Step: [12/16], Loss: 0.0027\n",
      "Epoch: [86/200], Step: [13/16], Loss: 0.0022\n",
      "Epoch: [86/200], Step: [14/16], Loss: 0.0018\n",
      "Epoch: [86/200], Step: [15/16], Loss: 0.0021\n",
      "Epoch: [87/200], Step: [1/16], Loss: 0.0026\n",
      "Epoch: [87/200], Step: [2/16], Loss: 0.0022\n",
      "Epoch: [87/200], Step: [3/16], Loss: 0.0023\n",
      "Epoch: [87/200], Step: [4/16], Loss: 0.0020\n",
      "Epoch: [87/200], Step: [5/16], Loss: 0.0024\n",
      "Epoch: [87/200], Step: [6/16], Loss: 0.0023\n",
      "Epoch: [87/200], Step: [7/16], Loss: 0.0021\n",
      "Epoch: [87/200], Step: [8/16], Loss: 0.0021\n",
      "Epoch: [87/200], Step: [9/16], Loss: 0.0019\n",
      "Epoch: [87/200], Step: [10/16], Loss: 0.0025\n",
      "Epoch: [87/200], Step: [11/16], Loss: 0.0021\n",
      "Epoch: [87/200], Step: [12/16], Loss: 0.0026\n",
      "Epoch: [87/200], Step: [13/16], Loss: 0.0020\n",
      "Epoch: [87/200], Step: [14/16], Loss: 0.0018\n",
      "Epoch: [87/200], Step: [15/16], Loss: 0.0025\n",
      "Epoch: [88/200], Step: [1/16], Loss: 0.0023\n",
      "Epoch: [88/200], Step: [2/16], Loss: 0.0019\n",
      "Epoch: [88/200], Step: [3/16], Loss: 0.0019\n",
      "Epoch: [88/200], Step: [4/16], Loss: 0.0016\n",
      "Epoch: [88/200], Step: [5/16], Loss: 0.0025\n",
      "Epoch: [88/200], Step: [6/16], Loss: 0.0019\n",
      "Epoch: [88/200], Step: [7/16], Loss: 0.0024\n",
      "Epoch: [88/200], Step: [8/16], Loss: 0.0021\n",
      "Epoch: [88/200], Step: [9/16], Loss: 0.0024\n",
      "Epoch: [88/200], Step: [10/16], Loss: 0.0021\n",
      "Epoch: [88/200], Step: [11/16], Loss: 0.0025\n",
      "Epoch: [88/200], Step: [12/16], Loss: 0.0021\n",
      "Epoch: [88/200], Step: [13/16], Loss: 0.0027\n",
      "Epoch: [88/200], Step: [14/16], Loss: 0.0021\n",
      "Epoch: [88/200], Step: [15/16], Loss: 0.0023\n",
      "Epoch: [89/200], Step: [1/16], Loss: 0.0019\n",
      "Epoch: [89/200], Step: [2/16], Loss: 0.0020\n",
      "Epoch: [89/200], Step: [3/16], Loss: 0.0020\n",
      "Epoch: [89/200], Step: [4/16], Loss: 0.0022\n",
      "Epoch: [89/200], Step: [5/16], Loss: 0.0022\n",
      "Epoch: [89/200], Step: [6/16], Loss: 0.0024\n",
      "Epoch: [89/200], Step: [7/16], Loss: 0.0020\n",
      "Epoch: [89/200], Step: [8/16], Loss: 0.0023\n",
      "Epoch: [89/200], Step: [9/16], Loss: 0.0020\n",
      "Epoch: [89/200], Step: [10/16], Loss: 0.0020\n",
      "Epoch: [89/200], Step: [11/16], Loss: 0.0020\n",
      "Epoch: [89/200], Step: [12/16], Loss: 0.0020\n",
      "Epoch: [89/200], Step: [13/16], Loss: 0.0027\n",
      "Epoch: [89/200], Step: [14/16], Loss: 0.0023\n",
      "Epoch: [89/200], Step: [15/16], Loss: 0.0018\n",
      "Epoch: [90/200], Step: [1/16], Loss: 0.0022\n",
      "Epoch: [90/200], Step: [2/16], Loss: 0.0017\n",
      "Epoch: [90/200], Step: [3/16], Loss: 0.0021\n",
      "Epoch: [90/200], Step: [4/16], Loss: 0.0021\n",
      "Epoch: [90/200], Step: [5/16], Loss: 0.0017\n",
      "Epoch: [90/200], Step: [6/16], Loss: 0.0024\n",
      "Epoch: [90/200], Step: [7/16], Loss: 0.0020\n",
      "Epoch: [90/200], Step: [8/16], Loss: 0.0024\n",
      "Epoch: [90/200], Step: [9/16], Loss: 0.0023\n",
      "Epoch: [90/200], Step: [10/16], Loss: 0.0019\n",
      "Epoch: [90/200], Step: [11/16], Loss: 0.0018\n",
      "Epoch: [90/200], Step: [12/16], Loss: 0.0019\n",
      "Epoch: [90/200], Step: [13/16], Loss: 0.0023\n",
      "Epoch: [90/200], Step: [14/16], Loss: 0.0022\n",
      "Epoch: [90/200], Step: [15/16], Loss: 0.0022\n",
      "Epoch: [91/200], Step: [1/16], Loss: 0.0020\n",
      "Epoch: [91/200], Step: [2/16], Loss: 0.0022\n",
      "Epoch: [91/200], Step: [3/16], Loss: 0.0023\n",
      "Epoch: [91/200], Step: [4/16], Loss: 0.0020\n",
      "Epoch: [91/200], Step: [5/16], Loss: 0.0019\n",
      "Epoch: [91/200], Step: [6/16], Loss: 0.0024\n",
      "Epoch: [91/200], Step: [7/16], Loss: 0.0017\n",
      "Epoch: [91/200], Step: [8/16], Loss: 0.0021\n",
      "Epoch: [91/200], Step: [9/16], Loss: 0.0019\n",
      "Epoch: [91/200], Step: [10/16], Loss: 0.0022\n",
      "Epoch: [91/200], Step: [11/16], Loss: 0.0020\n",
      "Epoch: [91/200], Step: [12/16], Loss: 0.0020\n",
      "Epoch: [91/200], Step: [13/16], Loss: 0.0017\n",
      "Epoch: [91/200], Step: [14/16], Loss: 0.0018\n",
      "Epoch: [91/200], Step: [15/16], Loss: 0.0020\n",
      "Epoch: [92/200], Step: [1/16], Loss: 0.0017\n",
      "Epoch: [92/200], Step: [2/16], Loss: 0.0022\n",
      "Epoch: [92/200], Step: [3/16], Loss: 0.0019\n",
      "Epoch: [92/200], Step: [4/16], Loss: 0.0020\n",
      "Epoch: [92/200], Step: [5/16], Loss: 0.0017\n",
      "Epoch: [92/200], Step: [6/16], Loss: 0.0020\n",
      "Epoch: [92/200], Step: [7/16], Loss: 0.0021\n",
      "Epoch: [92/200], Step: [8/16], Loss: 0.0022\n",
      "Epoch: [92/200], Step: [9/16], Loss: 0.0021\n",
      "Epoch: [92/200], Step: [10/16], Loss: 0.0019\n",
      "Epoch: [92/200], Step: [11/16], Loss: 0.0020\n",
      "Epoch: [92/200], Step: [12/16], Loss: 0.0019\n",
      "Epoch: [92/200], Step: [13/16], Loss: 0.0018\n",
      "Epoch: [92/200], Step: [14/16], Loss: 0.0023\n",
      "Epoch: [92/200], Step: [15/16], Loss: 0.0018\n",
      "Epoch: [93/200], Step: [1/16], Loss: 0.0019\n",
      "Epoch: [93/200], Step: [2/16], Loss: 0.0015\n",
      "Epoch: [93/200], Step: [3/16], Loss: 0.0020\n",
      "Epoch: [93/200], Step: [4/16], Loss: 0.0019\n",
      "Epoch: [93/200], Step: [5/16], Loss: 0.0018\n",
      "Epoch: [93/200], Step: [6/16], Loss: 0.0017\n",
      "Epoch: [93/200], Step: [7/16], Loss: 0.0022\n",
      "Epoch: [93/200], Step: [8/16], Loss: 0.0020\n",
      "Epoch: [93/200], Step: [9/16], Loss: 0.0021\n",
      "Epoch: [93/200], Step: [10/16], Loss: 0.0022\n",
      "Epoch: [93/200], Step: [11/16], Loss: 0.0018\n",
      "Epoch: [93/200], Step: [12/16], Loss: 0.0020\n",
      "Epoch: [93/200], Step: [13/16], Loss: 0.0023\n",
      "Epoch: [93/200], Step: [14/16], Loss: 0.0018\n",
      "Epoch: [93/200], Step: [15/16], Loss: 0.0020\n",
      "Epoch: [94/200], Step: [1/16], Loss: 0.0019\n",
      "Epoch: [94/200], Step: [2/16], Loss: 0.0015\n",
      "Epoch: [94/200], Step: [3/16], Loss: 0.0018\n",
      "Epoch: [94/200], Step: [4/16], Loss: 0.0023\n",
      "Epoch: [94/200], Step: [5/16], Loss: 0.0022\n",
      "Epoch: [94/200], Step: [6/16], Loss: 0.0019\n",
      "Epoch: [94/200], Step: [7/16], Loss: 0.0020\n",
      "Epoch: [94/200], Step: [8/16], Loss: 0.0022\n",
      "Epoch: [94/200], Step: [9/16], Loss: 0.0023\n",
      "Epoch: [94/200], Step: [10/16], Loss: 0.0018\n",
      "Epoch: [94/200], Step: [11/16], Loss: 0.0016\n",
      "Epoch: [94/200], Step: [12/16], Loss: 0.0015\n",
      "Epoch: [94/200], Step: [13/16], Loss: 0.0019\n",
      "Epoch: [94/200], Step: [14/16], Loss: 0.0018\n",
      "Epoch: [94/200], Step: [15/16], Loss: 0.0018\n",
      "Epoch: [95/200], Step: [1/16], Loss: 0.0020\n",
      "Epoch: [95/200], Step: [2/16], Loss: 0.0019\n",
      "Epoch: [95/200], Step: [3/16], Loss: 0.0021\n",
      "Epoch: [95/200], Step: [4/16], Loss: 0.0018\n",
      "Epoch: [95/200], Step: [5/16], Loss: 0.0018\n",
      "Epoch: [95/200], Step: [6/16], Loss: 0.0017\n",
      "Epoch: [95/200], Step: [7/16], Loss: 0.0018\n",
      "Epoch: [95/200], Step: [8/16], Loss: 0.0017\n",
      "Epoch: [95/200], Step: [9/16], Loss: 0.0021\n",
      "Epoch: [95/200], Step: [10/16], Loss: 0.0020\n",
      "Epoch: [95/200], Step: [11/16], Loss: 0.0020\n",
      "Epoch: [95/200], Step: [12/16], Loss: 0.0020\n",
      "Epoch: [95/200], Step: [13/16], Loss: 0.0017\n",
      "Epoch: [95/200], Step: [14/16], Loss: 0.0015\n",
      "Epoch: [95/200], Step: [15/16], Loss: 0.0019\n",
      "Epoch: [96/200], Step: [1/16], Loss: 0.0016\n",
      "Epoch: [96/200], Step: [2/16], Loss: 0.0021\n",
      "Epoch: [96/200], Step: [3/16], Loss: 0.0016\n",
      "Epoch: [96/200], Step: [4/16], Loss: 0.0021\n",
      "Epoch: [96/200], Step: [5/16], Loss: 0.0019\n",
      "Epoch: [96/200], Step: [6/16], Loss: 0.0016\n",
      "Epoch: [96/200], Step: [7/16], Loss: 0.0018\n",
      "Epoch: [96/200], Step: [8/16], Loss: 0.0020\n",
      "Epoch: [96/200], Step: [9/16], Loss: 0.0016\n",
      "Epoch: [96/200], Step: [10/16], Loss: 0.0015\n",
      "Epoch: [96/200], Step: [11/16], Loss: 0.0020\n",
      "Epoch: [96/200], Step: [12/16], Loss: 0.0017\n",
      "Epoch: [96/200], Step: [13/16], Loss: 0.0018\n",
      "Epoch: [96/200], Step: [14/16], Loss: 0.0022\n",
      "Epoch: [96/200], Step: [15/16], Loss: 0.0017\n",
      "Epoch: [97/200], Step: [1/16], Loss: 0.0016\n",
      "Epoch: [97/200], Step: [2/16], Loss: 0.0020\n",
      "Epoch: [97/200], Step: [3/16], Loss: 0.0014\n",
      "Epoch: [97/200], Step: [4/16], Loss: 0.0015\n",
      "Epoch: [97/200], Step: [5/16], Loss: 0.0020\n",
      "Epoch: [97/200], Step: [6/16], Loss: 0.0019\n",
      "Epoch: [97/200], Step: [7/16], Loss: 0.0018\n",
      "Epoch: [97/200], Step: [8/16], Loss: 0.0018\n",
      "Epoch: [97/200], Step: [9/16], Loss: 0.0016\n",
      "Epoch: [97/200], Step: [10/16], Loss: 0.0017\n",
      "Epoch: [97/200], Step: [11/16], Loss: 0.0021\n",
      "Epoch: [97/200], Step: [12/16], Loss: 0.0020\n",
      "Epoch: [97/200], Step: [13/16], Loss: 0.0018\n",
      "Epoch: [97/200], Step: [14/16], Loss: 0.0017\n",
      "Epoch: [97/200], Step: [15/16], Loss: 0.0019\n",
      "Epoch: [98/200], Step: [1/16], Loss: 0.0017\n",
      "Epoch: [98/200], Step: [2/16], Loss: 0.0016\n",
      "Epoch: [98/200], Step: [3/16], Loss: 0.0017\n",
      "Epoch: [98/200], Step: [4/16], Loss: 0.0019\n",
      "Epoch: [98/200], Step: [5/16], Loss: 0.0017\n",
      "Epoch: [98/200], Step: [6/16], Loss: 0.0016\n",
      "Epoch: [98/200], Step: [7/16], Loss: 0.0017\n",
      "Epoch: [98/200], Step: [8/16], Loss: 0.0017\n",
      "Epoch: [98/200], Step: [9/16], Loss: 0.0017\n",
      "Epoch: [98/200], Step: [10/16], Loss: 0.0018\n",
      "Epoch: [98/200], Step: [11/16], Loss: 0.0016\n",
      "Epoch: [98/200], Step: [12/16], Loss: 0.0015\n",
      "Epoch: [98/200], Step: [13/16], Loss: 0.0022\n",
      "Epoch: [98/200], Step: [14/16], Loss: 0.0019\n",
      "Epoch: [98/200], Step: [15/16], Loss: 0.0019\n",
      "Epoch: [99/200], Step: [1/16], Loss: 0.0016\n",
      "Epoch: [99/200], Step: [2/16], Loss: 0.0016\n",
      "Epoch: [99/200], Step: [3/16], Loss: 0.0015\n",
      "Epoch: [99/200], Step: [4/16], Loss: 0.0015\n",
      "Epoch: [99/200], Step: [5/16], Loss: 0.0018\n",
      "Epoch: [99/200], Step: [6/16], Loss: 0.0015\n",
      "Epoch: [99/200], Step: [7/16], Loss: 0.0015\n",
      "Epoch: [99/200], Step: [8/16], Loss: 0.0016\n",
      "Epoch: [99/200], Step: [9/16], Loss: 0.0018\n",
      "Epoch: [99/200], Step: [10/16], Loss: 0.0019\n",
      "Epoch: [99/200], Step: [11/16], Loss: 0.0017\n",
      "Epoch: [99/200], Step: [12/16], Loss: 0.0018\n",
      "Epoch: [99/200], Step: [13/16], Loss: 0.0017\n",
      "Epoch: [99/200], Step: [14/16], Loss: 0.0018\n",
      "Epoch: [99/200], Step: [15/16], Loss: 0.0021\n",
      "Epoch: [100/200], Step: [1/16], Loss: 0.0017\n",
      "Epoch: [100/200], Step: [2/16], Loss: 0.0017\n",
      "Epoch: [100/200], Step: [3/16], Loss: 0.0014\n",
      "Epoch: [100/200], Step: [4/16], Loss: 0.0016\n",
      "Epoch: [100/200], Step: [5/16], Loss: 0.0018\n",
      "Epoch: [100/200], Step: [6/16], Loss: 0.0016\n",
      "Epoch: [100/200], Step: [7/16], Loss: 0.0013\n",
      "Epoch: [100/200], Step: [8/16], Loss: 0.0017\n",
      "Epoch: [100/200], Step: [9/16], Loss: 0.0018\n",
      "Epoch: [100/200], Step: [10/16], Loss: 0.0017\n",
      "Epoch: [100/200], Step: [11/16], Loss: 0.0021\n",
      "Epoch: [100/200], Step: [12/16], Loss: 0.0016\n",
      "Epoch: [100/200], Step: [13/16], Loss: 0.0017\n",
      "Epoch: [100/200], Step: [14/16], Loss: 0.0018\n",
      "Epoch: [100/200], Step: [15/16], Loss: 0.0018\n",
      "Epoch: [101/200], Step: [1/16], Loss: 0.0016\n",
      "Epoch: [101/200], Step: [2/16], Loss: 0.0017\n",
      "Epoch: [101/200], Step: [3/16], Loss: 0.0015\n",
      "Epoch: [101/200], Step: [4/16], Loss: 0.0013\n",
      "Epoch: [101/200], Step: [5/16], Loss: 0.0020\n",
      "Epoch: [101/200], Step: [6/16], Loss: 0.0018\n",
      "Epoch: [101/200], Step: [7/16], Loss: 0.0018\n",
      "Epoch: [101/200], Step: [8/16], Loss: 0.0014\n",
      "Epoch: [101/200], Step: [9/16], Loss: 0.0019\n",
      "Epoch: [101/200], Step: [10/16], Loss: 0.0016\n",
      "Epoch: [101/200], Step: [11/16], Loss: 0.0018\n",
      "Epoch: [101/200], Step: [12/16], Loss: 0.0018\n",
      "Epoch: [101/200], Step: [13/16], Loss: 0.0014\n",
      "Epoch: [101/200], Step: [14/16], Loss: 0.0017\n",
      "Epoch: [101/200], Step: [15/16], Loss: 0.0016\n",
      "Epoch: [102/200], Step: [1/16], Loss: 0.0014\n",
      "Epoch: [102/200], Step: [2/16], Loss: 0.0018\n",
      "Epoch: [102/200], Step: [3/16], Loss: 0.0017\n",
      "Epoch: [102/200], Step: [4/16], Loss: 0.0016\n",
      "Epoch: [102/200], Step: [5/16], Loss: 0.0018\n",
      "Epoch: [102/200], Step: [6/16], Loss: 0.0015\n",
      "Epoch: [102/200], Step: [7/16], Loss: 0.0020\n",
      "Epoch: [102/200], Step: [8/16], Loss: 0.0016\n",
      "Epoch: [102/200], Step: [9/16], Loss: 0.0016\n",
      "Epoch: [102/200], Step: [10/16], Loss: 0.0012\n",
      "Epoch: [102/200], Step: [11/16], Loss: 0.0017\n",
      "Epoch: [102/200], Step: [12/16], Loss: 0.0015\n",
      "Epoch: [102/200], Step: [13/16], Loss: 0.0014\n",
      "Epoch: [102/200], Step: [14/16], Loss: 0.0013\n",
      "Epoch: [102/200], Step: [15/16], Loss: 0.0016\n",
      "Epoch: [103/200], Step: [1/16], Loss: 0.0017\n",
      "Epoch: [103/200], Step: [2/16], Loss: 0.0015\n",
      "Epoch: [103/200], Step: [3/16], Loss: 0.0012\n",
      "Epoch: [103/200], Step: [4/16], Loss: 0.0016\n",
      "Epoch: [103/200], Step: [5/16], Loss: 0.0018\n",
      "Epoch: [103/200], Step: [6/16], Loss: 0.0012\n",
      "Epoch: [103/200], Step: [7/16], Loss: 0.0016\n",
      "Epoch: [103/200], Step: [8/16], Loss: 0.0017\n",
      "Epoch: [103/200], Step: [9/16], Loss: 0.0013\n",
      "Epoch: [103/200], Step: [10/16], Loss: 0.0016\n",
      "Epoch: [103/200], Step: [11/16], Loss: 0.0021\n",
      "Epoch: [103/200], Step: [12/16], Loss: 0.0014\n",
      "Epoch: [103/200], Step: [13/16], Loss: 0.0014\n",
      "Epoch: [103/200], Step: [14/16], Loss: 0.0017\n",
      "Epoch: [103/200], Step: [15/16], Loss: 0.0018\n",
      "Epoch: [104/200], Step: [1/16], Loss: 0.0012\n",
      "Epoch: [104/200], Step: [2/16], Loss: 0.0015\n",
      "Epoch: [104/200], Step: [3/16], Loss: 0.0017\n",
      "Epoch: [104/200], Step: [4/16], Loss: 0.0015\n",
      "Epoch: [104/200], Step: [5/16], Loss: 0.0015\n",
      "Epoch: [104/200], Step: [6/16], Loss: 0.0014\n",
      "Epoch: [104/200], Step: [7/16], Loss: 0.0017\n",
      "Epoch: [104/200], Step: [8/16], Loss: 0.0018\n",
      "Epoch: [104/200], Step: [9/16], Loss: 0.0016\n",
      "Epoch: [104/200], Step: [10/16], Loss: 0.0012\n",
      "Epoch: [104/200], Step: [11/16], Loss: 0.0016\n",
      "Epoch: [104/200], Step: [12/16], Loss: 0.0015\n",
      "Epoch: [104/200], Step: [13/16], Loss: 0.0018\n",
      "Epoch: [104/200], Step: [14/16], Loss: 0.0016\n",
      "Epoch: [104/200], Step: [15/16], Loss: 0.0015\n",
      "Epoch: [105/200], Step: [1/16], Loss: 0.0014\n",
      "Epoch: [105/200], Step: [2/16], Loss: 0.0014\n",
      "Epoch: [105/200], Step: [3/16], Loss: 0.0019\n",
      "Epoch: [105/200], Step: [4/16], Loss: 0.0014\n",
      "Epoch: [105/200], Step: [5/16], Loss: 0.0013\n",
      "Epoch: [105/200], Step: [6/16], Loss: 0.0017\n",
      "Epoch: [105/200], Step: [7/16], Loss: 0.0014\n",
      "Epoch: [105/200], Step: [8/16], Loss: 0.0014\n",
      "Epoch: [105/200], Step: [9/16], Loss: 0.0016\n",
      "Epoch: [105/200], Step: [10/16], Loss: 0.0016\n",
      "Epoch: [105/200], Step: [11/16], Loss: 0.0017\n",
      "Epoch: [105/200], Step: [12/16], Loss: 0.0013\n",
      "Epoch: [105/200], Step: [13/16], Loss: 0.0016\n",
      "Epoch: [105/200], Step: [14/16], Loss: 0.0015\n",
      "Epoch: [105/200], Step: [15/16], Loss: 0.0016\n",
      "Epoch: [106/200], Step: [1/16], Loss: 0.0014\n",
      "Epoch: [106/200], Step: [2/16], Loss: 0.0016\n",
      "Epoch: [106/200], Step: [3/16], Loss: 0.0015\n",
      "Epoch: [106/200], Step: [4/16], Loss: 0.0014\n",
      "Epoch: [106/200], Step: [5/16], Loss: 0.0015\n",
      "Epoch: [106/200], Step: [6/16], Loss: 0.0016\n",
      "Epoch: [106/200], Step: [7/16], Loss: 0.0014\n",
      "Epoch: [106/200], Step: [8/16], Loss: 0.0014\n",
      "Epoch: [106/200], Step: [9/16], Loss: 0.0015\n",
      "Epoch: [106/200], Step: [10/16], Loss: 0.0016\n",
      "Epoch: [106/200], Step: [11/16], Loss: 0.0016\n",
      "Epoch: [106/200], Step: [12/16], Loss: 0.0015\n",
      "Epoch: [106/200], Step: [13/16], Loss: 0.0014\n",
      "Epoch: [106/200], Step: [14/16], Loss: 0.0013\n",
      "Epoch: [106/200], Step: [15/16], Loss: 0.0015\n",
      "Epoch: [107/200], Step: [1/16], Loss: 0.0013\n",
      "Epoch: [107/200], Step: [2/16], Loss: 0.0015\n",
      "Epoch: [107/200], Step: [3/16], Loss: 0.0015\n",
      "Epoch: [107/200], Step: [4/16], Loss: 0.0013\n",
      "Epoch: [107/200], Step: [5/16], Loss: 0.0015\n",
      "Epoch: [107/200], Step: [6/16], Loss: 0.0015\n",
      "Epoch: [107/200], Step: [7/16], Loss: 0.0016\n",
      "Epoch: [107/200], Step: [8/16], Loss: 0.0014\n",
      "Epoch: [107/200], Step: [9/16], Loss: 0.0013\n",
      "Epoch: [107/200], Step: [10/16], Loss: 0.0013\n",
      "Epoch: [107/200], Step: [11/16], Loss: 0.0015\n",
      "Epoch: [107/200], Step: [12/16], Loss: 0.0012\n",
      "Epoch: [107/200], Step: [13/16], Loss: 0.0018\n",
      "Epoch: [107/200], Step: [14/16], Loss: 0.0017\n",
      "Epoch: [107/200], Step: [15/16], Loss: 0.0013\n",
      "Epoch: [108/200], Step: [1/16], Loss: 0.0012\n",
      "Epoch: [108/200], Step: [2/16], Loss: 0.0015\n",
      "Epoch: [108/200], Step: [3/16], Loss: 0.0016\n",
      "Epoch: [108/200], Step: [4/16], Loss: 0.0013\n",
      "Epoch: [108/200], Step: [5/16], Loss: 0.0017\n",
      "Epoch: [108/200], Step: [6/16], Loss: 0.0017\n",
      "Epoch: [108/200], Step: [7/16], Loss: 0.0015\n",
      "Epoch: [108/200], Step: [8/16], Loss: 0.0017\n",
      "Epoch: [108/200], Step: [9/16], Loss: 0.0014\n",
      "Epoch: [108/200], Step: [10/16], Loss: 0.0012\n",
      "Epoch: [108/200], Step: [11/16], Loss: 0.0011\n",
      "Epoch: [108/200], Step: [12/16], Loss: 0.0014\n",
      "Epoch: [108/200], Step: [13/16], Loss: 0.0012\n",
      "Epoch: [108/200], Step: [14/16], Loss: 0.0013\n",
      "Epoch: [108/200], Step: [15/16], Loss: 0.0016\n",
      "Epoch: [109/200], Step: [1/16], Loss: 0.0014\n",
      "Epoch: [109/200], Step: [2/16], Loss: 0.0013\n",
      "Epoch: [109/200], Step: [3/16], Loss: 0.0013\n",
      "Epoch: [109/200], Step: [4/16], Loss: 0.0013\n",
      "Epoch: [109/200], Step: [5/16], Loss: 0.0013\n",
      "Epoch: [109/200], Step: [6/16], Loss: 0.0017\n",
      "Epoch: [109/200], Step: [7/16], Loss: 0.0014\n",
      "Epoch: [109/200], Step: [8/16], Loss: 0.0013\n",
      "Epoch: [109/200], Step: [9/16], Loss: 0.0013\n",
      "Epoch: [109/200], Step: [10/16], Loss: 0.0014\n",
      "Epoch: [109/200], Step: [11/16], Loss: 0.0015\n",
      "Epoch: [109/200], Step: [12/16], Loss: 0.0014\n",
      "Epoch: [109/200], Step: [13/16], Loss: 0.0015\n",
      "Epoch: [109/200], Step: [14/16], Loss: 0.0014\n",
      "Epoch: [109/200], Step: [15/16], Loss: 0.0014\n",
      "Epoch: [110/200], Step: [1/16], Loss: 0.0016\n",
      "Epoch: [110/200], Step: [2/16], Loss: 0.0012\n",
      "Epoch: [110/200], Step: [3/16], Loss: 0.0013\n",
      "Epoch: [110/200], Step: [4/16], Loss: 0.0013\n",
      "Epoch: [110/200], Step: [5/16], Loss: 0.0015\n",
      "Epoch: [110/200], Step: [6/16], Loss: 0.0016\n",
      "Epoch: [110/200], Step: [7/16], Loss: 0.0011\n",
      "Epoch: [110/200], Step: [8/16], Loss: 0.0012\n",
      "Epoch: [110/200], Step: [9/16], Loss: 0.0013\n",
      "Epoch: [110/200], Step: [10/16], Loss: 0.0014\n",
      "Epoch: [110/200], Step: [11/16], Loss: 0.0016\n",
      "Epoch: [110/200], Step: [12/16], Loss: 0.0012\n",
      "Epoch: [110/200], Step: [13/16], Loss: 0.0012\n",
      "Epoch: [110/200], Step: [14/16], Loss: 0.0015\n",
      "Epoch: [110/200], Step: [15/16], Loss: 0.0014\n",
      "Epoch: [111/200], Step: [1/16], Loss: 0.0014\n",
      "Epoch: [111/200], Step: [2/16], Loss: 0.0013\n",
      "Epoch: [111/200], Step: [3/16], Loss: 0.0015\n",
      "Epoch: [111/200], Step: [4/16], Loss: 0.0015\n",
      "Epoch: [111/200], Step: [5/16], Loss: 0.0015\n",
      "Epoch: [111/200], Step: [6/16], Loss: 0.0015\n",
      "Epoch: [111/200], Step: [7/16], Loss: 0.0012\n",
      "Epoch: [111/200], Step: [8/16], Loss: 0.0012\n",
      "Epoch: [111/200], Step: [9/16], Loss: 0.0010\n",
      "Epoch: [111/200], Step: [10/16], Loss: 0.0013\n",
      "Epoch: [111/200], Step: [11/16], Loss: 0.0013\n",
      "Epoch: [111/200], Step: [12/16], Loss: 0.0012\n",
      "Epoch: [111/200], Step: [13/16], Loss: 0.0015\n",
      "Epoch: [111/200], Step: [14/16], Loss: 0.0014\n",
      "Epoch: [111/200], Step: [15/16], Loss: 0.0013\n",
      "Epoch: [112/200], Step: [1/16], Loss: 0.0013\n",
      "Epoch: [112/200], Step: [2/16], Loss: 0.0014\n",
      "Epoch: [112/200], Step: [3/16], Loss: 0.0011\n",
      "Epoch: [112/200], Step: [4/16], Loss: 0.0016\n",
      "Epoch: [112/200], Step: [5/16], Loss: 0.0012\n",
      "Epoch: [112/200], Step: [6/16], Loss: 0.0014\n",
      "Epoch: [112/200], Step: [7/16], Loss: 0.0011\n",
      "Epoch: [112/200], Step: [8/16], Loss: 0.0014\n",
      "Epoch: [112/200], Step: [9/16], Loss: 0.0015\n",
      "Epoch: [112/200], Step: [10/16], Loss: 0.0012\n",
      "Epoch: [112/200], Step: [11/16], Loss: 0.0012\n",
      "Epoch: [112/200], Step: [12/16], Loss: 0.0012\n",
      "Epoch: [112/200], Step: [13/16], Loss: 0.0014\n",
      "Epoch: [112/200], Step: [14/16], Loss: 0.0013\n",
      "Epoch: [112/200], Step: [15/16], Loss: 0.0015\n",
      "Epoch: [113/200], Step: [1/16], Loss: 0.0013\n",
      "Epoch: [113/200], Step: [2/16], Loss: 0.0014\n",
      "Epoch: [113/200], Step: [3/16], Loss: 0.0013\n",
      "Epoch: [113/200], Step: [4/16], Loss: 0.0014\n",
      "Epoch: [113/200], Step: [5/16], Loss: 0.0012\n",
      "Epoch: [113/200], Step: [6/16], Loss: 0.0014\n",
      "Epoch: [113/200], Step: [7/16], Loss: 0.0014\n",
      "Epoch: [113/200], Step: [8/16], Loss: 0.0010\n",
      "Epoch: [113/200], Step: [9/16], Loss: 0.0012\n",
      "Epoch: [113/200], Step: [10/16], Loss: 0.0012\n",
      "Epoch: [113/200], Step: [11/16], Loss: 0.0011\n",
      "Epoch: [113/200], Step: [12/16], Loss: 0.0013\n",
      "Epoch: [113/200], Step: [13/16], Loss: 0.0015\n",
      "Epoch: [113/200], Step: [14/16], Loss: 0.0013\n",
      "Epoch: [113/200], Step: [15/16], Loss: 0.0014\n",
      "Epoch: [114/200], Step: [1/16], Loss: 0.0014\n",
      "Epoch: [114/200], Step: [2/16], Loss: 0.0011\n",
      "Epoch: [114/200], Step: [3/16], Loss: 0.0012\n",
      "Epoch: [114/200], Step: [4/16], Loss: 0.0013\n",
      "Epoch: [114/200], Step: [5/16], Loss: 0.0012\n",
      "Epoch: [114/200], Step: [6/16], Loss: 0.0015\n",
      "Epoch: [114/200], Step: [7/16], Loss: 0.0016\n",
      "Epoch: [114/200], Step: [8/16], Loss: 0.0011\n",
      "Epoch: [114/200], Step: [9/16], Loss: 0.0012\n",
      "Epoch: [114/200], Step: [10/16], Loss: 0.0013\n",
      "Epoch: [114/200], Step: [11/16], Loss: 0.0013\n",
      "Epoch: [114/200], Step: [12/16], Loss: 0.0014\n",
      "Epoch: [114/200], Step: [13/16], Loss: 0.0011\n",
      "Epoch: [114/200], Step: [14/16], Loss: 0.0012\n",
      "Epoch: [114/200], Step: [15/16], Loss: 0.0011\n",
      "Epoch: [115/200], Step: [1/16], Loss: 0.0014\n",
      "Epoch: [115/200], Step: [2/16], Loss: 0.0013\n",
      "Epoch: [115/200], Step: [3/16], Loss: 0.0012\n",
      "Epoch: [115/200], Step: [4/16], Loss: 0.0013\n",
      "Epoch: [115/200], Step: [5/16], Loss: 0.0015\n",
      "Epoch: [115/200], Step: [6/16], Loss: 0.0012\n",
      "Epoch: [115/200], Step: [7/16], Loss: 0.0012\n",
      "Epoch: [115/200], Step: [8/16], Loss: 0.0011\n",
      "Epoch: [115/200], Step: [9/16], Loss: 0.0011\n",
      "Epoch: [115/200], Step: [10/16], Loss: 0.0010\n",
      "Epoch: [115/200], Step: [11/16], Loss: 0.0013\n",
      "Epoch: [115/200], Step: [12/16], Loss: 0.0012\n",
      "Epoch: [115/200], Step: [13/16], Loss: 0.0012\n",
      "Epoch: [115/200], Step: [14/16], Loss: 0.0011\n",
      "Epoch: [115/200], Step: [15/16], Loss: 0.0016\n",
      "Epoch: [116/200], Step: [1/16], Loss: 0.0013\n",
      "Epoch: [116/200], Step: [2/16], Loss: 0.0011\n",
      "Epoch: [116/200], Step: [3/16], Loss: 0.0012\n",
      "Epoch: [116/200], Step: [4/16], Loss: 0.0010\n",
      "Epoch: [116/200], Step: [5/16], Loss: 0.0012\n",
      "Epoch: [116/200], Step: [6/16], Loss: 0.0011\n",
      "Epoch: [116/200], Step: [7/16], Loss: 0.0015\n",
      "Epoch: [116/200], Step: [8/16], Loss: 0.0012\n",
      "Epoch: [116/200], Step: [9/16], Loss: 0.0013\n",
      "Epoch: [116/200], Step: [10/16], Loss: 0.0012\n",
      "Epoch: [116/200], Step: [11/16], Loss: 0.0014\n",
      "Epoch: [116/200], Step: [12/16], Loss: 0.0011\n",
      "Epoch: [116/200], Step: [13/16], Loss: 0.0011\n",
      "Epoch: [116/200], Step: [14/16], Loss: 0.0012\n",
      "Epoch: [116/200], Step: [15/16], Loss: 0.0013\n",
      "Epoch: [117/200], Step: [1/16], Loss: 0.0012\n",
      "Epoch: [117/200], Step: [2/16], Loss: 0.0012\n",
      "Epoch: [117/200], Step: [3/16], Loss: 0.0013\n",
      "Epoch: [117/200], Step: [4/16], Loss: 0.0011\n",
      "Epoch: [117/200], Step: [5/16], Loss: 0.0012\n",
      "Epoch: [117/200], Step: [6/16], Loss: 0.0012\n",
      "Epoch: [117/200], Step: [7/16], Loss: 0.0013\n",
      "Epoch: [117/200], Step: [8/16], Loss: 0.0011\n",
      "Epoch: [117/200], Step: [9/16], Loss: 0.0012\n",
      "Epoch: [117/200], Step: [10/16], Loss: 0.0014\n",
      "Epoch: [117/200], Step: [11/16], Loss: 0.0011\n",
      "Epoch: [117/200], Step: [12/16], Loss: 0.0011\n",
      "Epoch: [117/200], Step: [13/16], Loss: 0.0013\n",
      "Epoch: [117/200], Step: [14/16], Loss: 0.0012\n",
      "Epoch: [117/200], Step: [15/16], Loss: 0.0012\n",
      "Epoch: [118/200], Step: [1/16], Loss: 0.0013\n",
      "Epoch: [118/200], Step: [2/16], Loss: 0.0011\n",
      "Epoch: [118/200], Step: [3/16], Loss: 0.0013\n",
      "Epoch: [118/200], Step: [4/16], Loss: 0.0013\n",
      "Epoch: [118/200], Step: [5/16], Loss: 0.0012\n",
      "Epoch: [118/200], Step: [6/16], Loss: 0.0012\n",
      "Epoch: [118/200], Step: [7/16], Loss: 0.0012\n",
      "Epoch: [118/200], Step: [8/16], Loss: 0.0010\n",
      "Epoch: [118/200], Step: [9/16], Loss: 0.0013\n",
      "Epoch: [118/200], Step: [10/16], Loss: 0.0011\n",
      "Epoch: [118/200], Step: [11/16], Loss: 0.0011\n",
      "Epoch: [118/200], Step: [12/16], Loss: 0.0010\n",
      "Epoch: [118/200], Step: [13/16], Loss: 0.0013\n",
      "Epoch: [118/200], Step: [14/16], Loss: 0.0012\n",
      "Epoch: [118/200], Step: [15/16], Loss: 0.0011\n",
      "Epoch: [119/200], Step: [1/16], Loss: 0.0009\n",
      "Epoch: [119/200], Step: [2/16], Loss: 0.0014\n",
      "Epoch: [119/200], Step: [3/16], Loss: 0.0010\n",
      "Epoch: [119/200], Step: [4/16], Loss: 0.0011\n",
      "Epoch: [119/200], Step: [5/16], Loss: 0.0012\n",
      "Epoch: [119/200], Step: [6/16], Loss: 0.0010\n",
      "Epoch: [119/200], Step: [7/16], Loss: 0.0013\n",
      "Epoch: [119/200], Step: [8/16], Loss: 0.0010\n",
      "Epoch: [119/200], Step: [9/16], Loss: 0.0012\n",
      "Epoch: [119/200], Step: [10/16], Loss: 0.0011\n",
      "Epoch: [119/200], Step: [11/16], Loss: 0.0012\n",
      "Epoch: [119/200], Step: [12/16], Loss: 0.0011\n",
      "Epoch: [119/200], Step: [13/16], Loss: 0.0014\n",
      "Epoch: [119/200], Step: [14/16], Loss: 0.0012\n",
      "Epoch: [119/200], Step: [15/16], Loss: 0.0012\n",
      "Epoch: [120/200], Step: [1/16], Loss: 0.0012\n",
      "Epoch: [120/200], Step: [2/16], Loss: 0.0011\n",
      "Epoch: [120/200], Step: [3/16], Loss: 0.0012\n",
      "Epoch: [120/200], Step: [4/16], Loss: 0.0011\n",
      "Epoch: [120/200], Step: [5/16], Loss: 0.0011\n",
      "Epoch: [120/200], Step: [6/16], Loss: 0.0010\n",
      "Epoch: [120/200], Step: [7/16], Loss: 0.0011\n",
      "Epoch: [120/200], Step: [8/16], Loss: 0.0011\n",
      "Epoch: [120/200], Step: [9/16], Loss: 0.0014\n",
      "Epoch: [120/200], Step: [10/16], Loss: 0.0010\n",
      "Epoch: [120/200], Step: [11/16], Loss: 0.0010\n",
      "Epoch: [120/200], Step: [12/16], Loss: 0.0010\n",
      "Epoch: [120/200], Step: [13/16], Loss: 0.0013\n",
      "Epoch: [120/200], Step: [14/16], Loss: 0.0011\n",
      "Epoch: [120/200], Step: [15/16], Loss: 0.0013\n",
      "Epoch: [121/200], Step: [1/16], Loss: 0.0009\n",
      "Epoch: [121/200], Step: [2/16], Loss: 0.0014\n",
      "Epoch: [121/200], Step: [3/16], Loss: 0.0010\n",
      "Epoch: [121/200], Step: [4/16], Loss: 0.0013\n",
      "Epoch: [121/200], Step: [5/16], Loss: 0.0015\n",
      "Epoch: [121/200], Step: [6/16], Loss: 0.0011\n",
      "Epoch: [121/200], Step: [7/16], Loss: 0.0012\n",
      "Epoch: [121/200], Step: [8/16], Loss: 0.0010\n",
      "Epoch: [121/200], Step: [9/16], Loss: 0.0009\n",
      "Epoch: [121/200], Step: [10/16], Loss: 0.0010\n",
      "Epoch: [121/200], Step: [11/16], Loss: 0.0013\n",
      "Epoch: [121/200], Step: [12/16], Loss: 0.0011\n",
      "Epoch: [121/200], Step: [13/16], Loss: 0.0009\n",
      "Epoch: [121/200], Step: [14/16], Loss: 0.0009\n",
      "Epoch: [121/200], Step: [15/16], Loss: 0.0012\n",
      "Epoch: [122/200], Step: [1/16], Loss: 0.0011\n",
      "Epoch: [122/200], Step: [2/16], Loss: 0.0010\n",
      "Epoch: [122/200], Step: [3/16], Loss: 0.0011\n",
      "Epoch: [122/200], Step: [4/16], Loss: 0.0010\n",
      "Epoch: [122/200], Step: [5/16], Loss: 0.0011\n",
      "Epoch: [122/200], Step: [6/16], Loss: 0.0012\n",
      "Epoch: [122/200], Step: [7/16], Loss: 0.0011\n",
      "Epoch: [122/200], Step: [8/16], Loss: 0.0010\n",
      "Epoch: [122/200], Step: [9/16], Loss: 0.0010\n",
      "Epoch: [122/200], Step: [10/16], Loss: 0.0010\n",
      "Epoch: [122/200], Step: [11/16], Loss: 0.0012\n",
      "Epoch: [122/200], Step: [12/16], Loss: 0.0011\n",
      "Epoch: [122/200], Step: [13/16], Loss: 0.0012\n",
      "Epoch: [122/200], Step: [14/16], Loss: 0.0011\n",
      "Epoch: [122/200], Step: [15/16], Loss: 0.0010\n",
      "Epoch: [123/200], Step: [1/16], Loss: 0.0011\n",
      "Epoch: [123/200], Step: [2/16], Loss: 0.0010\n",
      "Epoch: [123/200], Step: [3/16], Loss: 0.0012\n",
      "Epoch: [123/200], Step: [4/16], Loss: 0.0011\n",
      "Epoch: [123/200], Step: [5/16], Loss: 0.0012\n",
      "Epoch: [123/200], Step: [6/16], Loss: 0.0012\n",
      "Epoch: [123/200], Step: [7/16], Loss: 0.0011\n",
      "Epoch: [123/200], Step: [8/16], Loss: 0.0009\n",
      "Epoch: [123/200], Step: [9/16], Loss: 0.0010\n",
      "Epoch: [123/200], Step: [10/16], Loss: 0.0010\n",
      "Epoch: [123/200], Step: [11/16], Loss: 0.0012\n",
      "Epoch: [123/200], Step: [12/16], Loss: 0.0009\n",
      "Epoch: [123/200], Step: [13/16], Loss: 0.0010\n",
      "Epoch: [123/200], Step: [14/16], Loss: 0.0010\n",
      "Epoch: [123/200], Step: [15/16], Loss: 0.0011\n",
      "Epoch: [124/200], Step: [1/16], Loss: 0.0009\n",
      "Epoch: [124/200], Step: [2/16], Loss: 0.0012\n",
      "Epoch: [124/200], Step: [3/16], Loss: 0.0012\n",
      "Epoch: [124/200], Step: [4/16], Loss: 0.0010\n",
      "Epoch: [124/200], Step: [5/16], Loss: 0.0011\n",
      "Epoch: [124/200], Step: [6/16], Loss: 0.0010\n",
      "Epoch: [124/200], Step: [7/16], Loss: 0.0012\n",
      "Epoch: [124/200], Step: [8/16], Loss: 0.0009\n",
      "Epoch: [124/200], Step: [9/16], Loss: 0.0011\n",
      "Epoch: [124/200], Step: [10/16], Loss: 0.0012\n",
      "Epoch: [124/200], Step: [11/16], Loss: 0.0009\n",
      "Epoch: [124/200], Step: [12/16], Loss: 0.0009\n",
      "Epoch: [124/200], Step: [13/16], Loss: 0.0011\n",
      "Epoch: [124/200], Step: [14/16], Loss: 0.0011\n",
      "Epoch: [124/200], Step: [15/16], Loss: 0.0011\n",
      "Epoch: [125/200], Step: [1/16], Loss: 0.0011\n",
      "Epoch: [125/200], Step: [2/16], Loss: 0.0011\n",
      "Epoch: [125/200], Step: [3/16], Loss: 0.0010\n",
      "Epoch: [125/200], Step: [4/16], Loss: 0.0010\n",
      "Epoch: [125/200], Step: [5/16], Loss: 0.0012\n",
      "Epoch: [125/200], Step: [6/16], Loss: 0.0009\n",
      "Epoch: [125/200], Step: [7/16], Loss: 0.0011\n",
      "Epoch: [125/200], Step: [8/16], Loss: 0.0010\n",
      "Epoch: [125/200], Step: [9/16], Loss: 0.0011\n",
      "Epoch: [125/200], Step: [10/16], Loss: 0.0011\n",
      "Epoch: [125/200], Step: [11/16], Loss: 0.0012\n",
      "Epoch: [125/200], Step: [12/16], Loss: 0.0010\n",
      "Epoch: [125/200], Step: [13/16], Loss: 0.0010\n",
      "Epoch: [125/200], Step: [14/16], Loss: 0.0009\n",
      "Epoch: [125/200], Step: [15/16], Loss: 0.0010\n",
      "Epoch: [126/200], Step: [1/16], Loss: 0.0009\n",
      "Epoch: [126/200], Step: [2/16], Loss: 0.0011\n",
      "Epoch: [126/200], Step: [3/16], Loss: 0.0011\n",
      "Epoch: [126/200], Step: [4/16], Loss: 0.0010\n",
      "Epoch: [126/200], Step: [5/16], Loss: 0.0009\n",
      "Epoch: [126/200], Step: [6/16], Loss: 0.0011\n",
      "Epoch: [126/200], Step: [7/16], Loss: 0.0011\n",
      "Epoch: [126/200], Step: [8/16], Loss: 0.0009\n",
      "Epoch: [126/200], Step: [9/16], Loss: 0.0010\n",
      "Epoch: [126/200], Step: [10/16], Loss: 0.0012\n",
      "Epoch: [126/200], Step: [11/16], Loss: 0.0011\n",
      "Epoch: [126/200], Step: [12/16], Loss: 0.0010\n",
      "Epoch: [126/200], Step: [13/16], Loss: 0.0010\n",
      "Epoch: [126/200], Step: [14/16], Loss: 0.0008\n",
      "Epoch: [126/200], Step: [15/16], Loss: 0.0012\n",
      "Epoch: [127/200], Step: [1/16], Loss: 0.0010\n",
      "Epoch: [127/200], Step: [2/16], Loss: 0.0008\n",
      "Epoch: [127/200], Step: [3/16], Loss: 0.0011\n",
      "Epoch: [127/200], Step: [4/16], Loss: 0.0010\n",
      "Epoch: [127/200], Step: [5/16], Loss: 0.0011\n",
      "Epoch: [127/200], Step: [6/16], Loss: 0.0009\n",
      "Epoch: [127/200], Step: [7/16], Loss: 0.0008\n",
      "Epoch: [127/200], Step: [8/16], Loss: 0.0011\n",
      "Epoch: [127/200], Step: [9/16], Loss: 0.0011\n",
      "Epoch: [127/200], Step: [10/16], Loss: 0.0010\n",
      "Epoch: [127/200], Step: [11/16], Loss: 0.0011\n",
      "Epoch: [127/200], Step: [12/16], Loss: 0.0009\n",
      "Epoch: [127/200], Step: [13/16], Loss: 0.0012\n",
      "Epoch: [127/200], Step: [14/16], Loss: 0.0011\n",
      "Epoch: [127/200], Step: [15/16], Loss: 0.0010\n",
      "Epoch: [128/200], Step: [1/16], Loss: 0.0010\n",
      "Epoch: [128/200], Step: [2/16], Loss: 0.0012\n",
      "Epoch: [128/200], Step: [3/16], Loss: 0.0009\n",
      "Epoch: [128/200], Step: [4/16], Loss: 0.0009\n",
      "Epoch: [128/200], Step: [5/16], Loss: 0.0009\n",
      "Epoch: [128/200], Step: [6/16], Loss: 0.0008\n",
      "Epoch: [128/200], Step: [7/16], Loss: 0.0008\n",
      "Epoch: [128/200], Step: [8/16], Loss: 0.0010\n",
      "Epoch: [128/200], Step: [9/16], Loss: 0.0011\n",
      "Epoch: [128/200], Step: [10/16], Loss: 0.0013\n",
      "Epoch: [128/200], Step: [11/16], Loss: 0.0011\n",
      "Epoch: [128/200], Step: [12/16], Loss: 0.0010\n",
      "Epoch: [128/200], Step: [13/16], Loss: 0.0009\n",
      "Epoch: [128/200], Step: [14/16], Loss: 0.0011\n",
      "Epoch: [128/200], Step: [15/16], Loss: 0.0009\n",
      "Epoch: [129/200], Step: [1/16], Loss: 0.0010\n",
      "Epoch: [129/200], Step: [2/16], Loss: 0.0010\n",
      "Epoch: [129/200], Step: [3/16], Loss: 0.0011\n",
      "Epoch: [129/200], Step: [4/16], Loss: 0.0011\n",
      "Epoch: [129/200], Step: [5/16], Loss: 0.0011\n",
      "Epoch: [129/200], Step: [6/16], Loss: 0.0010\n",
      "Epoch: [129/200], Step: [7/16], Loss: 0.0009\n",
      "Epoch: [129/200], Step: [8/16], Loss: 0.0010\n",
      "Epoch: [129/200], Step: [9/16], Loss: 0.0009\n",
      "Epoch: [129/200], Step: [10/16], Loss: 0.0009\n",
      "Epoch: [129/200], Step: [11/16], Loss: 0.0008\n",
      "Epoch: [129/200], Step: [12/16], Loss: 0.0009\n",
      "Epoch: [129/200], Step: [13/16], Loss: 0.0010\n",
      "Epoch: [129/200], Step: [14/16], Loss: 0.0009\n",
      "Epoch: [129/200], Step: [15/16], Loss: 0.0011\n",
      "Epoch: [130/200], Step: [1/16], Loss: 0.0009\n",
      "Epoch: [130/200], Step: [2/16], Loss: 0.0009\n",
      "Epoch: [130/200], Step: [3/16], Loss: 0.0010\n",
      "Epoch: [130/200], Step: [4/16], Loss: 0.0011\n",
      "Epoch: [130/200], Step: [5/16], Loss: 0.0009\n",
      "Epoch: [130/200], Step: [6/16], Loss: 0.0007\n",
      "Epoch: [130/200], Step: [7/16], Loss: 0.0010\n",
      "Epoch: [130/200], Step: [8/16], Loss: 0.0010\n",
      "Epoch: [130/200], Step: [9/16], Loss: 0.0009\n",
      "Epoch: [130/200], Step: [10/16], Loss: 0.0008\n",
      "Epoch: [130/200], Step: [11/16], Loss: 0.0010\n",
      "Epoch: [130/200], Step: [12/16], Loss: 0.0010\n",
      "Epoch: [130/200], Step: [13/16], Loss: 0.0009\n",
      "Epoch: [130/200], Step: [14/16], Loss: 0.0010\n",
      "Epoch: [130/200], Step: [15/16], Loss: 0.0010\n",
      "Epoch: [131/200], Step: [1/16], Loss: 0.0011\n",
      "Epoch: [131/200], Step: [2/16], Loss: 0.0009\n",
      "Epoch: [131/200], Step: [3/16], Loss: 0.0011\n",
      "Epoch: [131/200], Step: [4/16], Loss: 0.0008\n",
      "Epoch: [131/200], Step: [5/16], Loss: 0.0009\n",
      "Epoch: [131/200], Step: [6/16], Loss: 0.0010\n",
      "Epoch: [131/200], Step: [7/16], Loss: 0.0009\n",
      "Epoch: [131/200], Step: [8/16], Loss: 0.0009\n",
      "Epoch: [131/200], Step: [9/16], Loss: 0.0009\n",
      "Epoch: [131/200], Step: [10/16], Loss: 0.0009\n",
      "Epoch: [131/200], Step: [11/16], Loss: 0.0009\n",
      "Epoch: [131/200], Step: [12/16], Loss: 0.0010\n",
      "Epoch: [131/200], Step: [13/16], Loss: 0.0011\n",
      "Epoch: [131/200], Step: [14/16], Loss: 0.0008\n",
      "Epoch: [131/200], Step: [15/16], Loss: 0.0009\n",
      "Epoch: [132/200], Step: [1/16], Loss: 0.0010\n",
      "Epoch: [132/200], Step: [2/16], Loss: 0.0012\n",
      "Epoch: [132/200], Step: [3/16], Loss: 0.0010\n",
      "Epoch: [132/200], Step: [4/16], Loss: 0.0009\n",
      "Epoch: [132/200], Step: [5/16], Loss: 0.0008\n",
      "Epoch: [132/200], Step: [6/16], Loss: 0.0009\n",
      "Epoch: [132/200], Step: [7/16], Loss: 0.0010\n",
      "Epoch: [132/200], Step: [8/16], Loss: 0.0009\n",
      "Epoch: [132/200], Step: [9/16], Loss: 0.0007\n",
      "Epoch: [132/200], Step: [10/16], Loss: 0.0009\n",
      "Epoch: [132/200], Step: [11/16], Loss: 0.0009\n",
      "Epoch: [132/200], Step: [12/16], Loss: 0.0010\n",
      "Epoch: [132/200], Step: [13/16], Loss: 0.0008\n",
      "Epoch: [132/200], Step: [14/16], Loss: 0.0009\n",
      "Epoch: [132/200], Step: [15/16], Loss: 0.0010\n",
      "Epoch: [133/200], Step: [1/16], Loss: 0.0007\n",
      "Epoch: [133/200], Step: [2/16], Loss: 0.0009\n",
      "Epoch: [133/200], Step: [3/16], Loss: 0.0010\n",
      "Epoch: [133/200], Step: [4/16], Loss: 0.0009\n",
      "Epoch: [133/200], Step: [5/16], Loss: 0.0009\n",
      "Epoch: [133/200], Step: [6/16], Loss: 0.0009\n",
      "Epoch: [133/200], Step: [7/16], Loss: 0.0010\n",
      "Epoch: [133/200], Step: [8/16], Loss: 0.0008\n",
      "Epoch: [133/200], Step: [9/16], Loss: 0.0008\n",
      "Epoch: [133/200], Step: [10/16], Loss: 0.0010\n",
      "Epoch: [133/200], Step: [11/16], Loss: 0.0011\n",
      "Epoch: [133/200], Step: [12/16], Loss: 0.0008\n",
      "Epoch: [133/200], Step: [13/16], Loss: 0.0009\n",
      "Epoch: [133/200], Step: [14/16], Loss: 0.0009\n",
      "Epoch: [133/200], Step: [15/16], Loss: 0.0008\n",
      "Epoch: [134/200], Step: [1/16], Loss: 0.0009\n",
      "Epoch: [134/200], Step: [2/16], Loss: 0.0008\n",
      "Epoch: [134/200], Step: [3/16], Loss: 0.0007\n",
      "Epoch: [134/200], Step: [4/16], Loss: 0.0010\n",
      "Epoch: [134/200], Step: [5/16], Loss: 0.0010\n",
      "Epoch: [134/200], Step: [6/16], Loss: 0.0009\n",
      "Epoch: [134/200], Step: [7/16], Loss: 0.0008\n",
      "Epoch: [134/200], Step: [8/16], Loss: 0.0010\n",
      "Epoch: [134/200], Step: [9/16], Loss: 0.0011\n",
      "Epoch: [134/200], Step: [10/16], Loss: 0.0009\n",
      "Epoch: [134/200], Step: [11/16], Loss: 0.0009\n",
      "Epoch: [134/200], Step: [12/16], Loss: 0.0008\n",
      "Epoch: [134/200], Step: [13/16], Loss: 0.0007\n",
      "Epoch: [134/200], Step: [14/16], Loss: 0.0010\n",
      "Epoch: [134/200], Step: [15/16], Loss: 0.0008\n",
      "Epoch: [135/200], Step: [1/16], Loss: 0.0010\n",
      "Epoch: [135/200], Step: [2/16], Loss: 0.0007\n",
      "Epoch: [135/200], Step: [3/16], Loss: 0.0009\n",
      "Epoch: [135/200], Step: [4/16], Loss: 0.0009\n",
      "Epoch: [135/200], Step: [5/16], Loss: 0.0009\n",
      "Epoch: [135/200], Step: [6/16], Loss: 0.0009\n",
      "Epoch: [135/200], Step: [7/16], Loss: 0.0009\n",
      "Epoch: [135/200], Step: [8/16], Loss: 0.0009\n",
      "Epoch: [135/200], Step: [9/16], Loss: 0.0008\n",
      "Epoch: [135/200], Step: [10/16], Loss: 0.0009\n",
      "Epoch: [135/200], Step: [11/16], Loss: 0.0008\n",
      "Epoch: [135/200], Step: [12/16], Loss: 0.0008\n",
      "Epoch: [135/200], Step: [13/16], Loss: 0.0009\n",
      "Epoch: [135/200], Step: [14/16], Loss: 0.0010\n",
      "Epoch: [135/200], Step: [15/16], Loss: 0.0008\n",
      "Epoch: [136/200], Step: [1/16], Loss: 0.0009\n",
      "Epoch: [136/200], Step: [2/16], Loss: 0.0009\n",
      "Epoch: [136/200], Step: [3/16], Loss: 0.0009\n",
      "Epoch: [136/200], Step: [4/16], Loss: 0.0010\n",
      "Epoch: [136/200], Step: [5/16], Loss: 0.0009\n",
      "Epoch: [136/200], Step: [6/16], Loss: 0.0008\n",
      "Epoch: [136/200], Step: [7/16], Loss: 0.0009\n",
      "Epoch: [136/200], Step: [8/16], Loss: 0.0009\n",
      "Epoch: [136/200], Step: [9/16], Loss: 0.0008\n",
      "Epoch: [136/200], Step: [10/16], Loss: 0.0009\n",
      "Epoch: [136/200], Step: [11/16], Loss: 0.0008\n",
      "Epoch: [136/200], Step: [12/16], Loss: 0.0008\n",
      "Epoch: [136/200], Step: [13/16], Loss: 0.0008\n",
      "Epoch: [136/200], Step: [14/16], Loss: 0.0008\n",
      "Epoch: [136/200], Step: [15/16], Loss: 0.0008\n",
      "Epoch: [137/200], Step: [1/16], Loss: 0.0009\n",
      "Epoch: [137/200], Step: [2/16], Loss: 0.0009\n",
      "Epoch: [137/200], Step: [3/16], Loss: 0.0008\n",
      "Epoch: [137/200], Step: [4/16], Loss: 0.0008\n",
      "Epoch: [137/200], Step: [5/16], Loss: 0.0009\n",
      "Epoch: [137/200], Step: [6/16], Loss: 0.0009\n",
      "Epoch: [137/200], Step: [7/16], Loss: 0.0008\n",
      "Epoch: [137/200], Step: [8/16], Loss: 0.0009\n",
      "Epoch: [137/200], Step: [9/16], Loss: 0.0008\n",
      "Epoch: [137/200], Step: [10/16], Loss: 0.0008\n",
      "Epoch: [137/200], Step: [11/16], Loss: 0.0007\n",
      "Epoch: [137/200], Step: [12/16], Loss: 0.0009\n",
      "Epoch: [137/200], Step: [13/16], Loss: 0.0010\n",
      "Epoch: [137/200], Step: [14/16], Loss: 0.0009\n",
      "Epoch: [137/200], Step: [15/16], Loss: 0.0008\n",
      "Epoch: [138/200], Step: [1/16], Loss: 0.0008\n",
      "Epoch: [138/200], Step: [2/16], Loss: 0.0009\n",
      "Epoch: [138/200], Step: [3/16], Loss: 0.0008\n",
      "Epoch: [138/200], Step: [4/16], Loss: 0.0009\n",
      "Epoch: [138/200], Step: [5/16], Loss: 0.0008\n",
      "Epoch: [138/200], Step: [6/16], Loss: 0.0009\n",
      "Epoch: [138/200], Step: [7/16], Loss: 0.0006\n",
      "Epoch: [138/200], Step: [8/16], Loss: 0.0011\n",
      "Epoch: [138/200], Step: [9/16], Loss: 0.0009\n",
      "Epoch: [138/200], Step: [10/16], Loss: 0.0009\n",
      "Epoch: [138/200], Step: [11/16], Loss: 0.0009\n",
      "Epoch: [138/200], Step: [12/16], Loss: 0.0009\n",
      "Epoch: [138/200], Step: [13/16], Loss: 0.0008\n",
      "Epoch: [138/200], Step: [14/16], Loss: 0.0008\n",
      "Epoch: [138/200], Step: [15/16], Loss: 0.0007\n",
      "Epoch: [139/200], Step: [1/16], Loss: 0.0008\n",
      "Epoch: [139/200], Step: [2/16], Loss: 0.0009\n",
      "Epoch: [139/200], Step: [3/16], Loss: 0.0007\n",
      "Epoch: [139/200], Step: [4/16], Loss: 0.0009\n",
      "Epoch: [139/200], Step: [5/16], Loss: 0.0008\n",
      "Epoch: [139/200], Step: [6/16], Loss: 0.0009\n",
      "Epoch: [139/200], Step: [7/16], Loss: 0.0009\n",
      "Epoch: [139/200], Step: [8/16], Loss: 0.0009\n",
      "Epoch: [139/200], Step: [9/16], Loss: 0.0008\n",
      "Epoch: [139/200], Step: [10/16], Loss: 0.0010\n",
      "Epoch: [139/200], Step: [11/16], Loss: 0.0008\n",
      "Epoch: [139/200], Step: [12/16], Loss: 0.0008\n",
      "Epoch: [139/200], Step: [13/16], Loss: 0.0007\n",
      "Epoch: [139/200], Step: [14/16], Loss: 0.0008\n",
      "Epoch: [139/200], Step: [15/16], Loss: 0.0007\n",
      "Epoch: [140/200], Step: [1/16], Loss: 0.0008\n",
      "Epoch: [140/200], Step: [2/16], Loss: 0.0007\n",
      "Epoch: [140/200], Step: [3/16], Loss: 0.0009\n",
      "Epoch: [140/200], Step: [4/16], Loss: 0.0008\n",
      "Epoch: [140/200], Step: [5/16], Loss: 0.0008\n",
      "Epoch: [140/200], Step: [6/16], Loss: 0.0008\n",
      "Epoch: [140/200], Step: [7/16], Loss: 0.0007\n",
      "Epoch: [140/200], Step: [8/16], Loss: 0.0007\n",
      "Epoch: [140/200], Step: [9/16], Loss: 0.0006\n",
      "Epoch: [140/200], Step: [10/16], Loss: 0.0009\n",
      "Epoch: [140/200], Step: [11/16], Loss: 0.0010\n",
      "Epoch: [140/200], Step: [12/16], Loss: 0.0008\n",
      "Epoch: [140/200], Step: [13/16], Loss: 0.0008\n",
      "Epoch: [140/200], Step: [14/16], Loss: 0.0008\n",
      "Epoch: [140/200], Step: [15/16], Loss: 0.0010\n",
      "Epoch: [141/200], Step: [1/16], Loss: 0.0008\n",
      "Epoch: [141/200], Step: [2/16], Loss: 0.0008\n",
      "Epoch: [141/200], Step: [3/16], Loss: 0.0007\n",
      "Epoch: [141/200], Step: [4/16], Loss: 0.0009\n",
      "Epoch: [141/200], Step: [5/16], Loss: 0.0008\n",
      "Epoch: [141/200], Step: [6/16], Loss: 0.0008\n",
      "Epoch: [141/200], Step: [7/16], Loss: 0.0008\n",
      "Epoch: [141/200], Step: [8/16], Loss: 0.0009\n",
      "Epoch: [141/200], Step: [9/16], Loss: 0.0009\n",
      "Epoch: [141/200], Step: [10/16], Loss: 0.0007\n",
      "Epoch: [141/200], Step: [11/16], Loss: 0.0007\n",
      "Epoch: [141/200], Step: [12/16], Loss: 0.0008\n",
      "Epoch: [141/200], Step: [13/16], Loss: 0.0008\n",
      "Epoch: [141/200], Step: [14/16], Loss: 0.0007\n",
      "Epoch: [141/200], Step: [15/16], Loss: 0.0008\n",
      "Epoch: [142/200], Step: [1/16], Loss: 0.0007\n",
      "Epoch: [142/200], Step: [2/16], Loss: 0.0007\n",
      "Epoch: [142/200], Step: [3/16], Loss: 0.0008\n",
      "Epoch: [142/200], Step: [4/16], Loss: 0.0007\n",
      "Epoch: [142/200], Step: [5/16], Loss: 0.0007\n",
      "Epoch: [142/200], Step: [6/16], Loss: 0.0009\n",
      "Epoch: [142/200], Step: [7/16], Loss: 0.0007\n",
      "Epoch: [142/200], Step: [8/16], Loss: 0.0008\n",
      "Epoch: [142/200], Step: [9/16], Loss: 0.0009\n",
      "Epoch: [142/200], Step: [10/16], Loss: 0.0007\n",
      "Epoch: [142/200], Step: [11/16], Loss: 0.0008\n",
      "Epoch: [142/200], Step: [12/16], Loss: 0.0008\n",
      "Epoch: [142/200], Step: [13/16], Loss: 0.0008\n",
      "Epoch: [142/200], Step: [14/16], Loss: 0.0009\n",
      "Epoch: [142/200], Step: [15/16], Loss: 0.0007\n",
      "Epoch: [143/200], Step: [1/16], Loss: 0.0007\n",
      "Epoch: [143/200], Step: [2/16], Loss: 0.0006\n",
      "Epoch: [143/200], Step: [3/16], Loss: 0.0009\n",
      "Epoch: [143/200], Step: [4/16], Loss: 0.0008\n",
      "Epoch: [143/200], Step: [5/16], Loss: 0.0008\n",
      "Epoch: [143/200], Step: [6/16], Loss: 0.0006\n",
      "Epoch: [143/200], Step: [7/16], Loss: 0.0008\n",
      "Epoch: [143/200], Step: [8/16], Loss: 0.0007\n",
      "Epoch: [143/200], Step: [9/16], Loss: 0.0010\n",
      "Epoch: [143/200], Step: [10/16], Loss: 0.0008\n",
      "Epoch: [143/200], Step: [11/16], Loss: 0.0009\n",
      "Epoch: [143/200], Step: [12/16], Loss: 0.0007\n",
      "Epoch: [143/200], Step: [13/16], Loss: 0.0007\n",
      "Epoch: [143/200], Step: [14/16], Loss: 0.0008\n",
      "Epoch: [143/200], Step: [15/16], Loss: 0.0008\n",
      "Epoch: [144/200], Step: [1/16], Loss: 0.0007\n",
      "Epoch: [144/200], Step: [2/16], Loss: 0.0006\n",
      "Epoch: [144/200], Step: [3/16], Loss: 0.0008\n",
      "Epoch: [144/200], Step: [4/16], Loss: 0.0007\n",
      "Epoch: [144/200], Step: [5/16], Loss: 0.0008\n",
      "Epoch: [144/200], Step: [6/16], Loss: 0.0007\n",
      "Epoch: [144/200], Step: [7/16], Loss: 0.0008\n",
      "Epoch: [144/200], Step: [8/16], Loss: 0.0008\n",
      "Epoch: [144/200], Step: [9/16], Loss: 0.0007\n",
      "Epoch: [144/200], Step: [10/16], Loss: 0.0009\n",
      "Epoch: [144/200], Step: [11/16], Loss: 0.0008\n",
      "Epoch: [144/200], Step: [12/16], Loss: 0.0007\n",
      "Epoch: [144/200], Step: [13/16], Loss: 0.0009\n",
      "Epoch: [144/200], Step: [14/16], Loss: 0.0008\n",
      "Epoch: [144/200], Step: [15/16], Loss: 0.0008\n",
      "Epoch: [145/200], Step: [1/16], Loss: 0.0009\n",
      "Epoch: [145/200], Step: [2/16], Loss: 0.0009\n",
      "Epoch: [145/200], Step: [3/16], Loss: 0.0006\n",
      "Epoch: [145/200], Step: [4/16], Loss: 0.0008\n",
      "Epoch: [145/200], Step: [5/16], Loss: 0.0008\n",
      "Epoch: [145/200], Step: [6/16], Loss: 0.0008\n",
      "Epoch: [145/200], Step: [7/16], Loss: 0.0007\n",
      "Epoch: [145/200], Step: [8/16], Loss: 0.0006\n",
      "Epoch: [145/200], Step: [9/16], Loss: 0.0009\n",
      "Epoch: [145/200], Step: [10/16], Loss: 0.0007\n",
      "Epoch: [145/200], Step: [11/16], Loss: 0.0007\n",
      "Epoch: [145/200], Step: [12/16], Loss: 0.0006\n",
      "Epoch: [145/200], Step: [13/16], Loss: 0.0007\n",
      "Epoch: [145/200], Step: [14/16], Loss: 0.0008\n",
      "Epoch: [145/200], Step: [15/16], Loss: 0.0008\n",
      "Epoch: [146/200], Step: [1/16], Loss: 0.0008\n",
      "Epoch: [146/200], Step: [2/16], Loss: 0.0007\n",
      "Epoch: [146/200], Step: [3/16], Loss: 0.0007\n",
      "Epoch: [146/200], Step: [4/16], Loss: 0.0008\n",
      "Epoch: [146/200], Step: [5/16], Loss: 0.0008\n",
      "Epoch: [146/200], Step: [6/16], Loss: 0.0009\n",
      "Epoch: [146/200], Step: [7/16], Loss: 0.0006\n",
      "Epoch: [146/200], Step: [8/16], Loss: 0.0007\n",
      "Epoch: [146/200], Step: [9/16], Loss: 0.0007\n",
      "Epoch: [146/200], Step: [10/16], Loss: 0.0006\n",
      "Epoch: [146/200], Step: [11/16], Loss: 0.0008\n",
      "Epoch: [146/200], Step: [12/16], Loss: 0.0008\n",
      "Epoch: [146/200], Step: [13/16], Loss: 0.0007\n",
      "Epoch: [146/200], Step: [14/16], Loss: 0.0008\n",
      "Epoch: [146/200], Step: [15/16], Loss: 0.0007\n",
      "Epoch: [147/200], Step: [1/16], Loss: 0.0008\n",
      "Epoch: [147/200], Step: [2/16], Loss: 0.0007\n",
      "Epoch: [147/200], Step: [3/16], Loss: 0.0008\n",
      "Epoch: [147/200], Step: [4/16], Loss: 0.0008\n",
      "Epoch: [147/200], Step: [5/16], Loss: 0.0008\n",
      "Epoch: [147/200], Step: [6/16], Loss: 0.0006\n",
      "Epoch: [147/200], Step: [7/16], Loss: 0.0008\n",
      "Epoch: [147/200], Step: [8/16], Loss: 0.0007\n",
      "Epoch: [147/200], Step: [9/16], Loss: 0.0007\n",
      "Epoch: [147/200], Step: [10/16], Loss: 0.0008\n",
      "Epoch: [147/200], Step: [11/16], Loss: 0.0008\n",
      "Epoch: [147/200], Step: [12/16], Loss: 0.0006\n",
      "Epoch: [147/200], Step: [13/16], Loss: 0.0008\n",
      "Epoch: [147/200], Step: [14/16], Loss: 0.0006\n",
      "Epoch: [147/200], Step: [15/16], Loss: 0.0007\n",
      "Epoch: [148/200], Step: [1/16], Loss: 0.0007\n",
      "Epoch: [148/200], Step: [2/16], Loss: 0.0007\n",
      "Epoch: [148/200], Step: [3/16], Loss: 0.0007\n",
      "Epoch: [148/200], Step: [4/16], Loss: 0.0006\n",
      "Epoch: [148/200], Step: [5/16], Loss: 0.0007\n",
      "Epoch: [148/200], Step: [6/16], Loss: 0.0007\n",
      "Epoch: [148/200], Step: [7/16], Loss: 0.0007\n",
      "Epoch: [148/200], Step: [8/16], Loss: 0.0007\n",
      "Epoch: [148/200], Step: [9/16], Loss: 0.0009\n",
      "Epoch: [148/200], Step: [10/16], Loss: 0.0007\n",
      "Epoch: [148/200], Step: [11/16], Loss: 0.0007\n",
      "Epoch: [148/200], Step: [12/16], Loss: 0.0008\n",
      "Epoch: [148/200], Step: [13/16], Loss: 0.0008\n",
      "Epoch: [148/200], Step: [14/16], Loss: 0.0007\n",
      "Epoch: [148/200], Step: [15/16], Loss: 0.0007\n",
      "Epoch: [149/200], Step: [1/16], Loss: 0.0006\n",
      "Epoch: [149/200], Step: [2/16], Loss: 0.0007\n",
      "Epoch: [149/200], Step: [3/16], Loss: 0.0007\n",
      "Epoch: [149/200], Step: [4/16], Loss: 0.0007\n",
      "Epoch: [149/200], Step: [5/16], Loss: 0.0007\n",
      "Epoch: [149/200], Step: [6/16], Loss: 0.0008\n",
      "Epoch: [149/200], Step: [7/16], Loss: 0.0006\n",
      "Epoch: [149/200], Step: [8/16], Loss: 0.0008\n",
      "Epoch: [149/200], Step: [9/16], Loss: 0.0007\n",
      "Epoch: [149/200], Step: [10/16], Loss: 0.0008\n",
      "Epoch: [149/200], Step: [11/16], Loss: 0.0005\n",
      "Epoch: [149/200], Step: [12/16], Loss: 0.0008\n",
      "Epoch: [149/200], Step: [13/16], Loss: 0.0007\n",
      "Epoch: [149/200], Step: [14/16], Loss: 0.0008\n",
      "Epoch: [149/200], Step: [15/16], Loss: 0.0007\n",
      "Epoch: [150/200], Step: [1/16], Loss: 0.0007\n",
      "Epoch: [150/200], Step: [2/16], Loss: 0.0007\n",
      "Epoch: [150/200], Step: [3/16], Loss: 0.0006\n",
      "Epoch: [150/200], Step: [4/16], Loss: 0.0007\n",
      "Epoch: [150/200], Step: [5/16], Loss: 0.0007\n",
      "Epoch: [150/200], Step: [6/16], Loss: 0.0006\n",
      "Epoch: [150/200], Step: [7/16], Loss: 0.0006\n",
      "Epoch: [150/200], Step: [8/16], Loss: 0.0008\n",
      "Epoch: [150/200], Step: [9/16], Loss: 0.0007\n",
      "Epoch: [150/200], Step: [10/16], Loss: 0.0007\n",
      "Epoch: [150/200], Step: [11/16], Loss: 0.0008\n",
      "Epoch: [150/200], Step: [12/16], Loss: 0.0008\n",
      "Epoch: [150/200], Step: [13/16], Loss: 0.0008\n",
      "Epoch: [150/200], Step: [14/16], Loss: 0.0007\n",
      "Epoch: [150/200], Step: [15/16], Loss: 0.0007\n",
      "Epoch: [151/200], Step: [1/16], Loss: 0.0007\n",
      "Epoch: [151/200], Step: [2/16], Loss: 0.0006\n",
      "Epoch: [151/200], Step: [3/16], Loss: 0.0007\n",
      "Epoch: [151/200], Step: [4/16], Loss: 0.0008\n",
      "Epoch: [151/200], Step: [5/16], Loss: 0.0008\n",
      "Epoch: [151/200], Step: [6/16], Loss: 0.0007\n",
      "Epoch: [151/200], Step: [7/16], Loss: 0.0005\n",
      "Epoch: [151/200], Step: [8/16], Loss: 0.0007\n",
      "Epoch: [151/200], Step: [9/16], Loss: 0.0008\n",
      "Epoch: [151/200], Step: [10/16], Loss: 0.0007\n",
      "Epoch: [151/200], Step: [11/16], Loss: 0.0007\n",
      "Epoch: [151/200], Step: [12/16], Loss: 0.0007\n",
      "Epoch: [151/200], Step: [13/16], Loss: 0.0007\n",
      "Epoch: [151/200], Step: [14/16], Loss: 0.0007\n",
      "Epoch: [151/200], Step: [15/16], Loss: 0.0007\n",
      "Epoch: [152/200], Step: [1/16], Loss: 0.0006\n",
      "Epoch: [152/200], Step: [2/16], Loss: 0.0007\n",
      "Epoch: [152/200], Step: [3/16], Loss: 0.0007\n",
      "Epoch: [152/200], Step: [4/16], Loss: 0.0005\n",
      "Epoch: [152/200], Step: [5/16], Loss: 0.0006\n",
      "Epoch: [152/200], Step: [6/16], Loss: 0.0007\n",
      "Epoch: [152/200], Step: [7/16], Loss: 0.0007\n",
      "Epoch: [152/200], Step: [8/16], Loss: 0.0006\n",
      "Epoch: [152/200], Step: [9/16], Loss: 0.0006\n",
      "Epoch: [152/200], Step: [10/16], Loss: 0.0009\n",
      "Epoch: [152/200], Step: [11/16], Loss: 0.0007\n",
      "Epoch: [152/200], Step: [12/16], Loss: 0.0007\n",
      "Epoch: [152/200], Step: [13/16], Loss: 0.0007\n",
      "Epoch: [152/200], Step: [14/16], Loss: 0.0007\n",
      "Epoch: [152/200], Step: [15/16], Loss: 0.0008\n",
      "Epoch: [153/200], Step: [1/16], Loss: 0.0006\n",
      "Epoch: [153/200], Step: [2/16], Loss: 0.0006\n",
      "Epoch: [153/200], Step: [3/16], Loss: 0.0006\n",
      "Epoch: [153/200], Step: [4/16], Loss: 0.0007\n",
      "Epoch: [153/200], Step: [5/16], Loss: 0.0007\n",
      "Epoch: [153/200], Step: [6/16], Loss: 0.0006\n",
      "Epoch: [153/200], Step: [7/16], Loss: 0.0007\n",
      "Epoch: [153/200], Step: [8/16], Loss: 0.0008\n",
      "Epoch: [153/200], Step: [9/16], Loss: 0.0007\n",
      "Epoch: [153/200], Step: [10/16], Loss: 0.0007\n",
      "Epoch: [153/200], Step: [11/16], Loss: 0.0007\n",
      "Epoch: [153/200], Step: [12/16], Loss: 0.0007\n",
      "Epoch: [153/200], Step: [13/16], Loss: 0.0006\n",
      "Epoch: [153/200], Step: [14/16], Loss: 0.0006\n",
      "Epoch: [153/200], Step: [15/16], Loss: 0.0006\n",
      "Epoch: [154/200], Step: [1/16], Loss: 0.0005\n",
      "Epoch: [154/200], Step: [2/16], Loss: 0.0007\n",
      "Epoch: [154/200], Step: [3/16], Loss: 0.0006\n",
      "Epoch: [154/200], Step: [4/16], Loss: 0.0005\n",
      "Epoch: [154/200], Step: [5/16], Loss: 0.0007\n",
      "Epoch: [154/200], Step: [6/16], Loss: 0.0007\n",
      "Epoch: [154/200], Step: [7/16], Loss: 0.0007\n",
      "Epoch: [154/200], Step: [8/16], Loss: 0.0007\n",
      "Epoch: [154/200], Step: [9/16], Loss: 0.0007\n",
      "Epoch: [154/200], Step: [10/16], Loss: 0.0006\n",
      "Epoch: [154/200], Step: [11/16], Loss: 0.0006\n",
      "Epoch: [154/200], Step: [12/16], Loss: 0.0007\n",
      "Epoch: [154/200], Step: [13/16], Loss: 0.0008\n",
      "Epoch: [154/200], Step: [14/16], Loss: 0.0007\n",
      "Epoch: [154/200], Step: [15/16], Loss: 0.0006\n",
      "Epoch: [155/200], Step: [1/16], Loss: 0.0006\n",
      "Epoch: [155/200], Step: [2/16], Loss: 0.0007\n",
      "Epoch: [155/200], Step: [3/16], Loss: 0.0005\n",
      "Epoch: [155/200], Step: [4/16], Loss: 0.0008\n",
      "Epoch: [155/200], Step: [5/16], Loss: 0.0007\n",
      "Epoch: [155/200], Step: [6/16], Loss: 0.0007\n",
      "Epoch: [155/200], Step: [7/16], Loss: 0.0007\n",
      "Epoch: [155/200], Step: [8/16], Loss: 0.0005\n",
      "Epoch: [155/200], Step: [9/16], Loss: 0.0007\n",
      "Epoch: [155/200], Step: [10/16], Loss: 0.0007\n",
      "Epoch: [155/200], Step: [11/16], Loss: 0.0006\n",
      "Epoch: [155/200], Step: [12/16], Loss: 0.0005\n",
      "Epoch: [155/200], Step: [13/16], Loss: 0.0007\n",
      "Epoch: [155/200], Step: [14/16], Loss: 0.0007\n",
      "Epoch: [155/200], Step: [15/16], Loss: 0.0007\n",
      "Epoch: [156/200], Step: [1/16], Loss: 0.0006\n",
      "Epoch: [156/200], Step: [2/16], Loss: 0.0006\n",
      "Epoch: [156/200], Step: [3/16], Loss: 0.0006\n",
      "Epoch: [156/200], Step: [4/16], Loss: 0.0006\n",
      "Epoch: [156/200], Step: [5/16], Loss: 0.0006\n",
      "Epoch: [156/200], Step: [6/16], Loss: 0.0006\n",
      "Epoch: [156/200], Step: [7/16], Loss: 0.0006\n",
      "Epoch: [156/200], Step: [8/16], Loss: 0.0006\n",
      "Epoch: [156/200], Step: [9/16], Loss: 0.0006\n",
      "Epoch: [156/200], Step: [10/16], Loss: 0.0007\n",
      "Epoch: [156/200], Step: [11/16], Loss: 0.0007\n",
      "Epoch: [156/200], Step: [12/16], Loss: 0.0006\n",
      "Epoch: [156/200], Step: [13/16], Loss: 0.0007\n",
      "Epoch: [156/200], Step: [14/16], Loss: 0.0006\n",
      "Epoch: [156/200], Step: [15/16], Loss: 0.0008\n",
      "Epoch: [157/200], Step: [1/16], Loss: 0.0006\n",
      "Epoch: [157/200], Step: [2/16], Loss: 0.0006\n",
      "Epoch: [157/200], Step: [3/16], Loss: 0.0006\n",
      "Epoch: [157/200], Step: [4/16], Loss: 0.0007\n",
      "Epoch: [157/200], Step: [5/16], Loss: 0.0007\n",
      "Epoch: [157/200], Step: [6/16], Loss: 0.0007\n",
      "Epoch: [157/200], Step: [7/16], Loss: 0.0007\n",
      "Epoch: [157/200], Step: [8/16], Loss: 0.0006\n",
      "Epoch: [157/200], Step: [9/16], Loss: 0.0005\n",
      "Epoch: [157/200], Step: [10/16], Loss: 0.0006\n",
      "Epoch: [157/200], Step: [11/16], Loss: 0.0007\n",
      "Epoch: [157/200], Step: [12/16], Loss: 0.0005\n",
      "Epoch: [157/200], Step: [13/16], Loss: 0.0007\n",
      "Epoch: [157/200], Step: [14/16], Loss: 0.0009\n",
      "Epoch: [157/200], Step: [15/16], Loss: 0.0005\n",
      "Epoch: [158/200], Step: [1/16], Loss: 0.0006\n",
      "Epoch: [158/200], Step: [2/16], Loss: 0.0007\n",
      "Epoch: [158/200], Step: [3/16], Loss: 0.0006\n",
      "Epoch: [158/200], Step: [4/16], Loss: 0.0005\n",
      "Epoch: [158/200], Step: [5/16], Loss: 0.0008\n",
      "Epoch: [158/200], Step: [6/16], Loss: 0.0006\n",
      "Epoch: [158/200], Step: [7/16], Loss: 0.0006\n",
      "Epoch: [158/200], Step: [8/16], Loss: 0.0006\n",
      "Epoch: [158/200], Step: [9/16], Loss: 0.0006\n",
      "Epoch: [158/200], Step: [10/16], Loss: 0.0005\n",
      "Epoch: [158/200], Step: [11/16], Loss: 0.0006\n",
      "Epoch: [158/200], Step: [12/16], Loss: 0.0007\n",
      "Epoch: [158/200], Step: [13/16], Loss: 0.0006\n",
      "Epoch: [158/200], Step: [14/16], Loss: 0.0007\n",
      "Epoch: [158/200], Step: [15/16], Loss: 0.0006\n",
      "Epoch: [159/200], Step: [1/16], Loss: 0.0006\n",
      "Epoch: [159/200], Step: [2/16], Loss: 0.0006\n",
      "Epoch: [159/200], Step: [3/16], Loss: 0.0006\n",
      "Epoch: [159/200], Step: [4/16], Loss: 0.0005\n",
      "Epoch: [159/200], Step: [5/16], Loss: 0.0006\n",
      "Epoch: [159/200], Step: [6/16], Loss: 0.0006\n",
      "Epoch: [159/200], Step: [7/16], Loss: 0.0006\n",
      "Epoch: [159/200], Step: [8/16], Loss: 0.0006\n",
      "Epoch: [159/200], Step: [9/16], Loss: 0.0006\n",
      "Epoch: [159/200], Step: [10/16], Loss: 0.0007\n",
      "Epoch: [159/200], Step: [11/16], Loss: 0.0006\n",
      "Epoch: [159/200], Step: [12/16], Loss: 0.0005\n",
      "Epoch: [159/200], Step: [13/16], Loss: 0.0007\n",
      "Epoch: [159/200], Step: [14/16], Loss: 0.0006\n",
      "Epoch: [159/200], Step: [15/16], Loss: 0.0006\n",
      "Epoch: [160/200], Step: [1/16], Loss: 0.0007\n",
      "Epoch: [160/200], Step: [2/16], Loss: 0.0006\n",
      "Epoch: [160/200], Step: [3/16], Loss: 0.0006\n",
      "Epoch: [160/200], Step: [4/16], Loss: 0.0006\n",
      "Epoch: [160/200], Step: [5/16], Loss: 0.0005\n",
      "Epoch: [160/200], Step: [6/16], Loss: 0.0005\n",
      "Epoch: [160/200], Step: [7/16], Loss: 0.0006\n",
      "Epoch: [160/200], Step: [8/16], Loss: 0.0005\n",
      "Epoch: [160/200], Step: [9/16], Loss: 0.0006\n",
      "Epoch: [160/200], Step: [10/16], Loss: 0.0007\n",
      "Epoch: [160/200], Step: [11/16], Loss: 0.0006\n",
      "Epoch: [160/200], Step: [12/16], Loss: 0.0006\n",
      "Epoch: [160/200], Step: [13/16], Loss: 0.0006\n",
      "Epoch: [160/200], Step: [14/16], Loss: 0.0006\n",
      "Epoch: [160/200], Step: [15/16], Loss: 0.0007\n",
      "Epoch: [161/200], Step: [1/16], Loss: 0.0005\n",
      "Epoch: [161/200], Step: [2/16], Loss: 0.0005\n",
      "Epoch: [161/200], Step: [3/16], Loss: 0.0007\n",
      "Epoch: [161/200], Step: [4/16], Loss: 0.0005\n",
      "Epoch: [161/200], Step: [5/16], Loss: 0.0006\n",
      "Epoch: [161/200], Step: [6/16], Loss: 0.0007\n",
      "Epoch: [161/200], Step: [7/16], Loss: 0.0005\n",
      "Epoch: [161/200], Step: [8/16], Loss: 0.0007\n",
      "Epoch: [161/200], Step: [9/16], Loss: 0.0005\n",
      "Epoch: [161/200], Step: [10/16], Loss: 0.0006\n",
      "Epoch: [161/200], Step: [11/16], Loss: 0.0006\n",
      "Epoch: [161/200], Step: [12/16], Loss: 0.0006\n",
      "Epoch: [161/200], Step: [13/16], Loss: 0.0006\n",
      "Epoch: [161/200], Step: [14/16], Loss: 0.0006\n",
      "Epoch: [161/200], Step: [15/16], Loss: 0.0006\n",
      "Epoch: [162/200], Step: [1/16], Loss: 0.0006\n",
      "Epoch: [162/200], Step: [2/16], Loss: 0.0006\n",
      "Epoch: [162/200], Step: [3/16], Loss: 0.0006\n",
      "Epoch: [162/200], Step: [4/16], Loss: 0.0006\n",
      "Epoch: [162/200], Step: [5/16], Loss: 0.0006\n",
      "Epoch: [162/200], Step: [6/16], Loss: 0.0005\n",
      "Epoch: [162/200], Step: [7/16], Loss: 0.0006\n",
      "Epoch: [162/200], Step: [8/16], Loss: 0.0006\n",
      "Epoch: [162/200], Step: [9/16], Loss: 0.0006\n",
      "Epoch: [162/200], Step: [10/16], Loss: 0.0005\n",
      "Epoch: [162/200], Step: [11/16], Loss: 0.0007\n",
      "Epoch: [162/200], Step: [12/16], Loss: 0.0006\n",
      "Epoch: [162/200], Step: [13/16], Loss: 0.0005\n",
      "Epoch: [162/200], Step: [14/16], Loss: 0.0006\n",
      "Epoch: [162/200], Step: [15/16], Loss: 0.0006\n",
      "Epoch: [163/200], Step: [1/16], Loss: 0.0006\n",
      "Epoch: [163/200], Step: [2/16], Loss: 0.0005\n",
      "Epoch: [163/200], Step: [3/16], Loss: 0.0006\n",
      "Epoch: [163/200], Step: [4/16], Loss: 0.0004\n",
      "Epoch: [163/200], Step: [5/16], Loss: 0.0006\n",
      "Epoch: [163/200], Step: [6/16], Loss: 0.0007\n",
      "Epoch: [163/200], Step: [7/16], Loss: 0.0007\n",
      "Epoch: [163/200], Step: [8/16], Loss: 0.0006\n",
      "Epoch: [163/200], Step: [9/16], Loss: 0.0007\n",
      "Epoch: [163/200], Step: [10/16], Loss: 0.0006\n",
      "Epoch: [163/200], Step: [11/16], Loss: 0.0005\n",
      "Epoch: [163/200], Step: [12/16], Loss: 0.0005\n",
      "Epoch: [163/200], Step: [13/16], Loss: 0.0005\n",
      "Epoch: [163/200], Step: [14/16], Loss: 0.0004\n",
      "Epoch: [163/200], Step: [15/16], Loss: 0.0006\n",
      "Epoch: [164/200], Step: [1/16], Loss: 0.0005\n",
      "Epoch: [164/200], Step: [2/16], Loss: 0.0006\n",
      "Epoch: [164/200], Step: [3/16], Loss: 0.0006\n",
      "Epoch: [164/200], Step: [4/16], Loss: 0.0006\n",
      "Epoch: [164/200], Step: [5/16], Loss: 0.0005\n",
      "Epoch: [164/200], Step: [6/16], Loss: 0.0006\n",
      "Epoch: [164/200], Step: [7/16], Loss: 0.0007\n",
      "Epoch: [164/200], Step: [8/16], Loss: 0.0006\n",
      "Epoch: [164/200], Step: [9/16], Loss: 0.0006\n",
      "Epoch: [164/200], Step: [10/16], Loss: 0.0005\n",
      "Epoch: [164/200], Step: [11/16], Loss: 0.0005\n",
      "Epoch: [164/200], Step: [12/16], Loss: 0.0006\n",
      "Epoch: [164/200], Step: [13/16], Loss: 0.0005\n",
      "Epoch: [164/200], Step: [14/16], Loss: 0.0005\n",
      "Epoch: [164/200], Step: [15/16], Loss: 0.0006\n",
      "Epoch: [165/200], Step: [1/16], Loss: 0.0006\n",
      "Epoch: [165/200], Step: [2/16], Loss: 0.0006\n",
      "Epoch: [165/200], Step: [3/16], Loss: 0.0004\n",
      "Epoch: [165/200], Step: [4/16], Loss: 0.0005\n",
      "Epoch: [165/200], Step: [5/16], Loss: 0.0004\n",
      "Epoch: [165/200], Step: [6/16], Loss: 0.0006\n",
      "Epoch: [165/200], Step: [7/16], Loss: 0.0005\n",
      "Epoch: [165/200], Step: [8/16], Loss: 0.0006\n",
      "Epoch: [165/200], Step: [9/16], Loss: 0.0006\n",
      "Epoch: [165/200], Step: [10/16], Loss: 0.0006\n",
      "Epoch: [165/200], Step: [11/16], Loss: 0.0006\n",
      "Epoch: [165/200], Step: [12/16], Loss: 0.0006\n",
      "Epoch: [165/200], Step: [13/16], Loss: 0.0006\n",
      "Epoch: [165/200], Step: [14/16], Loss: 0.0006\n",
      "Epoch: [165/200], Step: [15/16], Loss: 0.0006\n",
      "Epoch: [166/200], Step: [1/16], Loss: 0.0006\n",
      "Epoch: [166/200], Step: [2/16], Loss: 0.0005\n",
      "Epoch: [166/200], Step: [3/16], Loss: 0.0005\n",
      "Epoch: [166/200], Step: [4/16], Loss: 0.0006\n",
      "Epoch: [166/200], Step: [5/16], Loss: 0.0006\n",
      "Epoch: [166/200], Step: [6/16], Loss: 0.0005\n",
      "Epoch: [166/200], Step: [7/16], Loss: 0.0006\n",
      "Epoch: [166/200], Step: [8/16], Loss: 0.0007\n",
      "Epoch: [166/200], Step: [9/16], Loss: 0.0006\n",
      "Epoch: [166/200], Step: [10/16], Loss: 0.0005\n",
      "Epoch: [166/200], Step: [11/16], Loss: 0.0005\n",
      "Epoch: [166/200], Step: [12/16], Loss: 0.0006\n",
      "Epoch: [166/200], Step: [13/16], Loss: 0.0006\n",
      "Epoch: [166/200], Step: [14/16], Loss: 0.0005\n",
      "Epoch: [166/200], Step: [15/16], Loss: 0.0006\n",
      "Epoch: [167/200], Step: [1/16], Loss: 0.0007\n",
      "Epoch: [167/200], Step: [2/16], Loss: 0.0005\n",
      "Epoch: [167/200], Step: [3/16], Loss: 0.0005\n",
      "Epoch: [167/200], Step: [4/16], Loss: 0.0006\n",
      "Epoch: [167/200], Step: [5/16], Loss: 0.0006\n",
      "Epoch: [167/200], Step: [6/16], Loss: 0.0005\n",
      "Epoch: [167/200], Step: [7/16], Loss: 0.0005\n",
      "Epoch: [167/200], Step: [8/16], Loss: 0.0006\n",
      "Epoch: [167/200], Step: [9/16], Loss: 0.0006\n",
      "Epoch: [167/200], Step: [10/16], Loss: 0.0005\n",
      "Epoch: [167/200], Step: [11/16], Loss: 0.0005\n",
      "Epoch: [167/200], Step: [12/16], Loss: 0.0006\n",
      "Epoch: [167/200], Step: [13/16], Loss: 0.0005\n",
      "Epoch: [167/200], Step: [14/16], Loss: 0.0006\n",
      "Epoch: [167/200], Step: [15/16], Loss: 0.0006\n",
      "Epoch: [168/200], Step: [1/16], Loss: 0.0005\n",
      "Epoch: [168/200], Step: [2/16], Loss: 0.0006\n",
      "Epoch: [168/200], Step: [3/16], Loss: 0.0006\n",
      "Epoch: [168/200], Step: [4/16], Loss: 0.0005\n",
      "Epoch: [168/200], Step: [5/16], Loss: 0.0005\n",
      "Epoch: [168/200], Step: [6/16], Loss: 0.0006\n",
      "Epoch: [168/200], Step: [7/16], Loss: 0.0006\n",
      "Epoch: [168/200], Step: [8/16], Loss: 0.0005\n",
      "Epoch: [168/200], Step: [9/16], Loss: 0.0005\n",
      "Epoch: [168/200], Step: [10/16], Loss: 0.0006\n",
      "Epoch: [168/200], Step: [11/16], Loss: 0.0006\n",
      "Epoch: [168/200], Step: [12/16], Loss: 0.0004\n",
      "Epoch: [168/200], Step: [13/16], Loss: 0.0005\n",
      "Epoch: [168/200], Step: [14/16], Loss: 0.0006\n",
      "Epoch: [168/200], Step: [15/16], Loss: 0.0005\n",
      "Epoch: [169/200], Step: [1/16], Loss: 0.0005\n",
      "Epoch: [169/200], Step: [2/16], Loss: 0.0006\n",
      "Epoch: [169/200], Step: [3/16], Loss: 0.0005\n",
      "Epoch: [169/200], Step: [4/16], Loss: 0.0005\n",
      "Epoch: [169/200], Step: [5/16], Loss: 0.0005\n",
      "Epoch: [169/200], Step: [6/16], Loss: 0.0005\n",
      "Epoch: [169/200], Step: [7/16], Loss: 0.0005\n",
      "Epoch: [169/200], Step: [8/16], Loss: 0.0006\n",
      "Epoch: [169/200], Step: [9/16], Loss: 0.0006\n",
      "Epoch: [169/200], Step: [10/16], Loss: 0.0006\n",
      "Epoch: [169/200], Step: [11/16], Loss: 0.0006\n",
      "Epoch: [169/200], Step: [12/16], Loss: 0.0005\n",
      "Epoch: [169/200], Step: [13/16], Loss: 0.0005\n",
      "Epoch: [169/200], Step: [14/16], Loss: 0.0005\n",
      "Epoch: [169/200], Step: [15/16], Loss: 0.0006\n",
      "Epoch: [170/200], Step: [1/16], Loss: 0.0006\n",
      "Epoch: [170/200], Step: [2/16], Loss: 0.0005\n",
      "Epoch: [170/200], Step: [3/16], Loss: 0.0005\n",
      "Epoch: [170/200], Step: [4/16], Loss: 0.0006\n",
      "Epoch: [170/200], Step: [5/16], Loss: 0.0004\n",
      "Epoch: [170/200], Step: [6/16], Loss: 0.0005\n",
      "Epoch: [170/200], Step: [7/16], Loss: 0.0005\n",
      "Epoch: [170/200], Step: [8/16], Loss: 0.0006\n",
      "Epoch: [170/200], Step: [9/16], Loss: 0.0004\n",
      "Epoch: [170/200], Step: [10/16], Loss: 0.0005\n",
      "Epoch: [170/200], Step: [11/16], Loss: 0.0006\n",
      "Epoch: [170/200], Step: [12/16], Loss: 0.0005\n",
      "Epoch: [170/200], Step: [13/16], Loss: 0.0005\n",
      "Epoch: [170/200], Step: [14/16], Loss: 0.0005\n",
      "Epoch: [170/200], Step: [15/16], Loss: 0.0005\n",
      "Epoch: [171/200], Step: [1/16], Loss: 0.0006\n",
      "Epoch: [171/200], Step: [2/16], Loss: 0.0006\n",
      "Epoch: [171/200], Step: [3/16], Loss: 0.0005\n",
      "Epoch: [171/200], Step: [4/16], Loss: 0.0005\n",
      "Epoch: [171/200], Step: [5/16], Loss: 0.0005\n",
      "Epoch: [171/200], Step: [6/16], Loss: 0.0005\n",
      "Epoch: [171/200], Step: [7/16], Loss: 0.0005\n",
      "Epoch: [171/200], Step: [8/16], Loss: 0.0005\n",
      "Epoch: [171/200], Step: [9/16], Loss: 0.0005\n",
      "Epoch: [171/200], Step: [10/16], Loss: 0.0005\n",
      "Epoch: [171/200], Step: [11/16], Loss: 0.0006\n",
      "Epoch: [171/200], Step: [12/16], Loss: 0.0005\n",
      "Epoch: [171/200], Step: [13/16], Loss: 0.0006\n",
      "Epoch: [171/200], Step: [14/16], Loss: 0.0005\n",
      "Epoch: [171/200], Step: [15/16], Loss: 0.0004\n",
      "Epoch: [172/200], Step: [1/16], Loss: 0.0006\n",
      "Epoch: [172/200], Step: [2/16], Loss: 0.0005\n",
      "Epoch: [172/200], Step: [3/16], Loss: 0.0006\n",
      "Epoch: [172/200], Step: [4/16], Loss: 0.0004\n",
      "Epoch: [172/200], Step: [5/16], Loss: 0.0005\n",
      "Epoch: [172/200], Step: [6/16], Loss: 0.0005\n",
      "Epoch: [172/200], Step: [7/16], Loss: 0.0005\n",
      "Epoch: [172/200], Step: [8/16], Loss: 0.0005\n",
      "Epoch: [172/200], Step: [9/16], Loss: 0.0005\n",
      "Epoch: [172/200], Step: [10/16], Loss: 0.0005\n",
      "Epoch: [172/200], Step: [11/16], Loss: 0.0005\n",
      "Epoch: [172/200], Step: [12/16], Loss: 0.0004\n",
      "Epoch: [172/200], Step: [13/16], Loss: 0.0006\n",
      "Epoch: [172/200], Step: [14/16], Loss: 0.0005\n",
      "Epoch: [172/200], Step: [15/16], Loss: 0.0004\n",
      "Epoch: [173/200], Step: [1/16], Loss: 0.0005\n",
      "Epoch: [173/200], Step: [2/16], Loss: 0.0006\n",
      "Epoch: [173/200], Step: [3/16], Loss: 0.0005\n",
      "Epoch: [173/200], Step: [4/16], Loss: 0.0004\n",
      "Epoch: [173/200], Step: [5/16], Loss: 0.0005\n",
      "Epoch: [173/200], Step: [6/16], Loss: 0.0005\n",
      "Epoch: [173/200], Step: [7/16], Loss: 0.0005\n",
      "Epoch: [173/200], Step: [8/16], Loss: 0.0006\n",
      "Epoch: [173/200], Step: [9/16], Loss: 0.0005\n",
      "Epoch: [173/200], Step: [10/16], Loss: 0.0005\n",
      "Epoch: [173/200], Step: [11/16], Loss: 0.0005\n",
      "Epoch: [173/200], Step: [12/16], Loss: 0.0005\n",
      "Epoch: [173/200], Step: [13/16], Loss: 0.0005\n",
      "Epoch: [173/200], Step: [14/16], Loss: 0.0005\n",
      "Epoch: [173/200], Step: [15/16], Loss: 0.0005\n",
      "Epoch: [174/200], Step: [1/16], Loss: 0.0005\n",
      "Epoch: [174/200], Step: [2/16], Loss: 0.0005\n",
      "Epoch: [174/200], Step: [3/16], Loss: 0.0005\n",
      "Epoch: [174/200], Step: [4/16], Loss: 0.0005\n",
      "Epoch: [174/200], Step: [5/16], Loss: 0.0005\n",
      "Epoch: [174/200], Step: [6/16], Loss: 0.0006\n",
      "Epoch: [174/200], Step: [7/16], Loss: 0.0006\n",
      "Epoch: [174/200], Step: [8/16], Loss: 0.0006\n",
      "Epoch: [174/200], Step: [9/16], Loss: 0.0004\n",
      "Epoch: [174/200], Step: [10/16], Loss: 0.0006\n",
      "Epoch: [174/200], Step: [11/16], Loss: 0.0005\n",
      "Epoch: [174/200], Step: [12/16], Loss: 0.0005\n",
      "Epoch: [174/200], Step: [13/16], Loss: 0.0004\n",
      "Epoch: [174/200], Step: [14/16], Loss: 0.0005\n",
      "Epoch: [174/200], Step: [15/16], Loss: 0.0005\n",
      "Epoch: [175/200], Step: [1/16], Loss: 0.0005\n",
      "Epoch: [175/200], Step: [2/16], Loss: 0.0006\n",
      "Epoch: [175/200], Step: [3/16], Loss: 0.0005\n",
      "Epoch: [175/200], Step: [4/16], Loss: 0.0005\n",
      "Epoch: [175/200], Step: [5/16], Loss: 0.0004\n",
      "Epoch: [175/200], Step: [6/16], Loss: 0.0005\n",
      "Epoch: [175/200], Step: [7/16], Loss: 0.0005\n",
      "Epoch: [175/200], Step: [8/16], Loss: 0.0005\n",
      "Epoch: [175/200], Step: [9/16], Loss: 0.0004\n",
      "Epoch: [175/200], Step: [10/16], Loss: 0.0005\n",
      "Epoch: [175/200], Step: [11/16], Loss: 0.0005\n",
      "Epoch: [175/200], Step: [12/16], Loss: 0.0005\n",
      "Epoch: [175/200], Step: [13/16], Loss: 0.0004\n",
      "Epoch: [175/200], Step: [14/16], Loss: 0.0004\n",
      "Epoch: [175/200], Step: [15/16], Loss: 0.0005\n",
      "Epoch: [176/200], Step: [1/16], Loss: 0.0005\n",
      "Epoch: [176/200], Step: [2/16], Loss: 0.0005\n",
      "Epoch: [176/200], Step: [3/16], Loss: 0.0005\n",
      "Epoch: [176/200], Step: [4/16], Loss: 0.0005\n",
      "Epoch: [176/200], Step: [5/16], Loss: 0.0004\n",
      "Epoch: [176/200], Step: [6/16], Loss: 0.0005\n",
      "Epoch: [176/200], Step: [7/16], Loss: 0.0005\n",
      "Epoch: [176/200], Step: [8/16], Loss: 0.0006\n",
      "Epoch: [176/200], Step: [9/16], Loss: 0.0004\n",
      "Epoch: [176/200], Step: [10/16], Loss: 0.0005\n",
      "Epoch: [176/200], Step: [11/16], Loss: 0.0005\n",
      "Epoch: [176/200], Step: [12/16], Loss: 0.0006\n",
      "Epoch: [176/200], Step: [13/16], Loss: 0.0005\n",
      "Epoch: [176/200], Step: [14/16], Loss: 0.0004\n",
      "Epoch: [176/200], Step: [15/16], Loss: 0.0005\n",
      "Epoch: [177/200], Step: [1/16], Loss: 0.0005\n",
      "Epoch: [177/200], Step: [2/16], Loss: 0.0005\n",
      "Epoch: [177/200], Step: [3/16], Loss: 0.0005\n",
      "Epoch: [177/200], Step: [4/16], Loss: 0.0005\n",
      "Epoch: [177/200], Step: [5/16], Loss: 0.0004\n",
      "Epoch: [177/200], Step: [6/16], Loss: 0.0005\n",
      "Epoch: [177/200], Step: [7/16], Loss: 0.0005\n",
      "Epoch: [177/200], Step: [8/16], Loss: 0.0004\n",
      "Epoch: [177/200], Step: [9/16], Loss: 0.0005\n",
      "Epoch: [177/200], Step: [10/16], Loss: 0.0005\n",
      "Epoch: [177/200], Step: [11/16], Loss: 0.0004\n",
      "Epoch: [177/200], Step: [12/16], Loss: 0.0004\n",
      "Epoch: [177/200], Step: [13/16], Loss: 0.0005\n",
      "Epoch: [177/200], Step: [14/16], Loss: 0.0005\n",
      "Epoch: [177/200], Step: [15/16], Loss: 0.0005\n",
      "Epoch: [178/200], Step: [1/16], Loss: 0.0005\n",
      "Epoch: [178/200], Step: [2/16], Loss: 0.0005\n",
      "Epoch: [178/200], Step: [3/16], Loss: 0.0004\n",
      "Epoch: [178/200], Step: [4/16], Loss: 0.0005\n",
      "Epoch: [178/200], Step: [5/16], Loss: 0.0005\n",
      "Epoch: [178/200], Step: [6/16], Loss: 0.0004\n",
      "Epoch: [178/200], Step: [7/16], Loss: 0.0005\n",
      "Epoch: [178/200], Step: [8/16], Loss: 0.0005\n",
      "Epoch: [178/200], Step: [9/16], Loss: 0.0006\n",
      "Epoch: [178/200], Step: [10/16], Loss: 0.0005\n",
      "Epoch: [178/200], Step: [11/16], Loss: 0.0005\n",
      "Epoch: [178/200], Step: [12/16], Loss: 0.0004\n",
      "Epoch: [178/200], Step: [13/16], Loss: 0.0005\n",
      "Epoch: [178/200], Step: [14/16], Loss: 0.0004\n",
      "Epoch: [178/200], Step: [15/16], Loss: 0.0005\n",
      "Epoch: [179/200], Step: [1/16], Loss: 0.0005\n",
      "Epoch: [179/200], Step: [2/16], Loss: 0.0005\n",
      "Epoch: [179/200], Step: [3/16], Loss: 0.0004\n",
      "Epoch: [179/200], Step: [4/16], Loss: 0.0005\n",
      "Epoch: [179/200], Step: [5/16], Loss: 0.0005\n",
      "Epoch: [179/200], Step: [6/16], Loss: 0.0005\n",
      "Epoch: [179/200], Step: [7/16], Loss: 0.0004\n",
      "Epoch: [179/200], Step: [8/16], Loss: 0.0005\n",
      "Epoch: [179/200], Step: [9/16], Loss: 0.0005\n",
      "Epoch: [179/200], Step: [10/16], Loss: 0.0004\n",
      "Epoch: [179/200], Step: [11/16], Loss: 0.0005\n",
      "Epoch: [179/200], Step: [12/16], Loss: 0.0005\n",
      "Epoch: [179/200], Step: [13/16], Loss: 0.0004\n",
      "Epoch: [179/200], Step: [14/16], Loss: 0.0005\n",
      "Epoch: [179/200], Step: [15/16], Loss: 0.0004\n",
      "Epoch: [180/200], Step: [1/16], Loss: 0.0005\n",
      "Epoch: [180/200], Step: [2/16], Loss: 0.0005\n",
      "Epoch: [180/200], Step: [3/16], Loss: 0.0005\n",
      "Epoch: [180/200], Step: [4/16], Loss: 0.0004\n",
      "Epoch: [180/200], Step: [5/16], Loss: 0.0005\n",
      "Epoch: [180/200], Step: [6/16], Loss: 0.0005\n",
      "Epoch: [180/200], Step: [7/16], Loss: 0.0005\n",
      "Epoch: [180/200], Step: [8/16], Loss: 0.0004\n",
      "Epoch: [180/200], Step: [9/16], Loss: 0.0004\n",
      "Epoch: [180/200], Step: [10/16], Loss: 0.0005\n",
      "Epoch: [180/200], Step: [11/16], Loss: 0.0004\n",
      "Epoch: [180/200], Step: [12/16], Loss: 0.0005\n",
      "Epoch: [180/200], Step: [13/16], Loss: 0.0005\n",
      "Epoch: [180/200], Step: [14/16], Loss: 0.0004\n",
      "Epoch: [180/200], Step: [15/16], Loss: 0.0004\n",
      "Epoch: [181/200], Step: [1/16], Loss: 0.0005\n",
      "Epoch: [181/200], Step: [2/16], Loss: 0.0004\n",
      "Epoch: [181/200], Step: [3/16], Loss: 0.0006\n",
      "Epoch: [181/200], Step: [4/16], Loss: 0.0005\n",
      "Epoch: [181/200], Step: [5/16], Loss: 0.0004\n",
      "Epoch: [181/200], Step: [6/16], Loss: 0.0004\n",
      "Epoch: [181/200], Step: [7/16], Loss: 0.0004\n",
      "Epoch: [181/200], Step: [8/16], Loss: 0.0004\n",
      "Epoch: [181/200], Step: [9/16], Loss: 0.0004\n",
      "Epoch: [181/200], Step: [10/16], Loss: 0.0004\n",
      "Epoch: [181/200], Step: [11/16], Loss: 0.0005\n",
      "Epoch: [181/200], Step: [12/16], Loss: 0.0005\n",
      "Epoch: [181/200], Step: [13/16], Loss: 0.0004\n",
      "Epoch: [181/200], Step: [14/16], Loss: 0.0005\n",
      "Epoch: [181/200], Step: [15/16], Loss: 0.0004\n",
      "Epoch: [182/200], Step: [1/16], Loss: 0.0004\n",
      "Epoch: [182/200], Step: [2/16], Loss: 0.0005\n",
      "Epoch: [182/200], Step: [3/16], Loss: 0.0004\n",
      "Epoch: [182/200], Step: [4/16], Loss: 0.0003\n",
      "Epoch: [182/200], Step: [5/16], Loss: 0.0005\n",
      "Epoch: [182/200], Step: [6/16], Loss: 0.0004\n",
      "Epoch: [182/200], Step: [7/16], Loss: 0.0004\n",
      "Epoch: [182/200], Step: [8/16], Loss: 0.0005\n",
      "Epoch: [182/200], Step: [9/16], Loss: 0.0004\n",
      "Epoch: [182/200], Step: [10/16], Loss: 0.0005\n",
      "Epoch: [182/200], Step: [11/16], Loss: 0.0004\n",
      "Epoch: [182/200], Step: [12/16], Loss: 0.0004\n",
      "Epoch: [182/200], Step: [13/16], Loss: 0.0004\n",
      "Epoch: [182/200], Step: [14/16], Loss: 0.0005\n",
      "Epoch: [182/200], Step: [15/16], Loss: 0.0005\n",
      "Epoch: [183/200], Step: [1/16], Loss: 0.0004\n",
      "Epoch: [183/200], Step: [2/16], Loss: 0.0005\n",
      "Epoch: [183/200], Step: [3/16], Loss: 0.0003\n",
      "Epoch: [183/200], Step: [4/16], Loss: 0.0004\n",
      "Epoch: [183/200], Step: [5/16], Loss: 0.0005\n",
      "Epoch: [183/200], Step: [6/16], Loss: 0.0004\n",
      "Epoch: [183/200], Step: [7/16], Loss: 0.0005\n",
      "Epoch: [183/200], Step: [8/16], Loss: 0.0005\n",
      "Epoch: [183/200], Step: [9/16], Loss: 0.0005\n",
      "Epoch: [183/200], Step: [10/16], Loss: 0.0005\n",
      "Epoch: [183/200], Step: [11/16], Loss: 0.0004\n",
      "Epoch: [183/200], Step: [12/16], Loss: 0.0005\n",
      "Epoch: [183/200], Step: [13/16], Loss: 0.0004\n",
      "Epoch: [183/200], Step: [14/16], Loss: 0.0005\n",
      "Epoch: [183/200], Step: [15/16], Loss: 0.0004\n",
      "Epoch: [184/200], Step: [1/16], Loss: 0.0004\n",
      "Epoch: [184/200], Step: [2/16], Loss: 0.0005\n",
      "Epoch: [184/200], Step: [3/16], Loss: 0.0005\n",
      "Epoch: [184/200], Step: [4/16], Loss: 0.0004\n",
      "Epoch: [184/200], Step: [5/16], Loss: 0.0005\n",
      "Epoch: [184/200], Step: [6/16], Loss: 0.0003\n",
      "Epoch: [184/200], Step: [7/16], Loss: 0.0004\n",
      "Epoch: [184/200], Step: [8/16], Loss: 0.0004\n",
      "Epoch: [184/200], Step: [9/16], Loss: 0.0004\n",
      "Epoch: [184/200], Step: [10/16], Loss: 0.0005\n",
      "Epoch: [184/200], Step: [11/16], Loss: 0.0004\n",
      "Epoch: [184/200], Step: [12/16], Loss: 0.0004\n",
      "Epoch: [184/200], Step: [13/16], Loss: 0.0004\n",
      "Epoch: [184/200], Step: [14/16], Loss: 0.0005\n",
      "Epoch: [184/200], Step: [15/16], Loss: 0.0004\n",
      "Epoch: [185/200], Step: [1/16], Loss: 0.0004\n",
      "Epoch: [185/200], Step: [2/16], Loss: 0.0003\n",
      "Epoch: [185/200], Step: [3/16], Loss: 0.0004\n",
      "Epoch: [185/200], Step: [4/16], Loss: 0.0005\n",
      "Epoch: [185/200], Step: [5/16], Loss: 0.0003\n",
      "Epoch: [185/200], Step: [6/16], Loss: 0.0004\n",
      "Epoch: [185/200], Step: [7/16], Loss: 0.0004\n",
      "Epoch: [185/200], Step: [8/16], Loss: 0.0005\n",
      "Epoch: [185/200], Step: [9/16], Loss: 0.0004\n",
      "Epoch: [185/200], Step: [10/16], Loss: 0.0005\n",
      "Epoch: [185/200], Step: [11/16], Loss: 0.0005\n",
      "Epoch: [185/200], Step: [12/16], Loss: 0.0004\n",
      "Epoch: [185/200], Step: [13/16], Loss: 0.0005\n",
      "Epoch: [185/200], Step: [14/16], Loss: 0.0005\n",
      "Epoch: [185/200], Step: [15/16], Loss: 0.0005\n",
      "Epoch: [186/200], Step: [1/16], Loss: 0.0004\n",
      "Epoch: [186/200], Step: [2/16], Loss: 0.0003\n",
      "Epoch: [186/200], Step: [3/16], Loss: 0.0005\n",
      "Epoch: [186/200], Step: [4/16], Loss: 0.0004\n",
      "Epoch: [186/200], Step: [5/16], Loss: 0.0005\n",
      "Epoch: [186/200], Step: [6/16], Loss: 0.0004\n",
      "Epoch: [186/200], Step: [7/16], Loss: 0.0004\n",
      "Epoch: [186/200], Step: [8/16], Loss: 0.0005\n",
      "Epoch: [186/200], Step: [9/16], Loss: 0.0005\n",
      "Epoch: [186/200], Step: [10/16], Loss: 0.0004\n",
      "Epoch: [186/200], Step: [11/16], Loss: 0.0005\n",
      "Epoch: [186/200], Step: [12/16], Loss: 0.0004\n",
      "Epoch: [186/200], Step: [13/16], Loss: 0.0004\n",
      "Epoch: [186/200], Step: [14/16], Loss: 0.0004\n",
      "Epoch: [186/200], Step: [15/16], Loss: 0.0004\n",
      "Epoch: [187/200], Step: [1/16], Loss: 0.0004\n",
      "Epoch: [187/200], Step: [2/16], Loss: 0.0004\n",
      "Epoch: [187/200], Step: [3/16], Loss: 0.0004\n",
      "Epoch: [187/200], Step: [4/16], Loss: 0.0005\n",
      "Epoch: [187/200], Step: [5/16], Loss: 0.0003\n",
      "Epoch: [187/200], Step: [6/16], Loss: 0.0005\n",
      "Epoch: [187/200], Step: [7/16], Loss: 0.0003\n",
      "Epoch: [187/200], Step: [8/16], Loss: 0.0004\n",
      "Epoch: [187/200], Step: [9/16], Loss: 0.0005\n",
      "Epoch: [187/200], Step: [10/16], Loss: 0.0004\n",
      "Epoch: [187/200], Step: [11/16], Loss: 0.0004\n",
      "Epoch: [187/200], Step: [12/16], Loss: 0.0004\n",
      "Epoch: [187/200], Step: [13/16], Loss: 0.0004\n",
      "Epoch: [187/200], Step: [14/16], Loss: 0.0004\n",
      "Epoch: [187/200], Step: [15/16], Loss: 0.0004\n",
      "Epoch: [188/200], Step: [1/16], Loss: 0.0004\n",
      "Epoch: [188/200], Step: [2/16], Loss: 0.0004\n",
      "Epoch: [188/200], Step: [3/16], Loss: 0.0005\n",
      "Epoch: [188/200], Step: [4/16], Loss: 0.0004\n",
      "Epoch: [188/200], Step: [5/16], Loss: 0.0004\n",
      "Epoch: [188/200], Step: [6/16], Loss: 0.0004\n",
      "Epoch: [188/200], Step: [7/16], Loss: 0.0004\n",
      "Epoch: [188/200], Step: [8/16], Loss: 0.0004\n",
      "Epoch: [188/200], Step: [9/16], Loss: 0.0004\n",
      "Epoch: [188/200], Step: [10/16], Loss: 0.0004\n",
      "Epoch: [188/200], Step: [11/16], Loss: 0.0004\n",
      "Epoch: [188/200], Step: [12/16], Loss: 0.0005\n",
      "Epoch: [188/200], Step: [13/16], Loss: 0.0003\n",
      "Epoch: [188/200], Step: [14/16], Loss: 0.0004\n",
      "Epoch: [188/200], Step: [15/16], Loss: 0.0004\n",
      "Epoch: [189/200], Step: [1/16], Loss: 0.0004\n",
      "Epoch: [189/200], Step: [2/16], Loss: 0.0004\n",
      "Epoch: [189/200], Step: [3/16], Loss: 0.0004\n",
      "Epoch: [189/200], Step: [4/16], Loss: 0.0004\n",
      "Epoch: [189/200], Step: [5/16], Loss: 0.0004\n",
      "Epoch: [189/200], Step: [6/16], Loss: 0.0004\n",
      "Epoch: [189/200], Step: [7/16], Loss: 0.0004\n",
      "Epoch: [189/200], Step: [8/16], Loss: 0.0004\n",
      "Epoch: [189/200], Step: [9/16], Loss: 0.0004\n",
      "Epoch: [189/200], Step: [10/16], Loss: 0.0004\n",
      "Epoch: [189/200], Step: [11/16], Loss: 0.0004\n",
      "Epoch: [189/200], Step: [12/16], Loss: 0.0005\n",
      "Epoch: [189/200], Step: [13/16], Loss: 0.0003\n",
      "Epoch: [189/200], Step: [14/16], Loss: 0.0005\n",
      "Epoch: [189/200], Step: [15/16], Loss: 0.0004\n",
      "Epoch: [190/200], Step: [1/16], Loss: 0.0004\n",
      "Epoch: [190/200], Step: [2/16], Loss: 0.0004\n",
      "Epoch: [190/200], Step: [3/16], Loss: 0.0004\n",
      "Epoch: [190/200], Step: [4/16], Loss: 0.0004\n",
      "Epoch: [190/200], Step: [5/16], Loss: 0.0005\n",
      "Epoch: [190/200], Step: [6/16], Loss: 0.0004\n",
      "Epoch: [190/200], Step: [7/16], Loss: 0.0004\n",
      "Epoch: [190/200], Step: [8/16], Loss: 0.0004\n",
      "Epoch: [190/200], Step: [9/16], Loss: 0.0004\n",
      "Epoch: [190/200], Step: [10/16], Loss: 0.0004\n",
      "Epoch: [190/200], Step: [11/16], Loss: 0.0004\n",
      "Epoch: [190/200], Step: [12/16], Loss: 0.0004\n",
      "Epoch: [190/200], Step: [13/16], Loss: 0.0004\n",
      "Epoch: [190/200], Step: [14/16], Loss: 0.0004\n",
      "Epoch: [190/200], Step: [15/16], Loss: 0.0004\n",
      "Epoch: [191/200], Step: [1/16], Loss: 0.0004\n",
      "Epoch: [191/200], Step: [2/16], Loss: 0.0004\n",
      "Epoch: [191/200], Step: [3/16], Loss: 0.0005\n",
      "Epoch: [191/200], Step: [4/16], Loss: 0.0004\n",
      "Epoch: [191/200], Step: [5/16], Loss: 0.0003\n",
      "Epoch: [191/200], Step: [6/16], Loss: 0.0004\n",
      "Epoch: [191/200], Step: [7/16], Loss: 0.0004\n",
      "Epoch: [191/200], Step: [8/16], Loss: 0.0004\n",
      "Epoch: [191/200], Step: [9/16], Loss: 0.0004\n",
      "Epoch: [191/200], Step: [10/16], Loss: 0.0004\n",
      "Epoch: [191/200], Step: [11/16], Loss: 0.0004\n",
      "Epoch: [191/200], Step: [12/16], Loss: 0.0004\n",
      "Epoch: [191/200], Step: [13/16], Loss: 0.0003\n",
      "Epoch: [191/200], Step: [14/16], Loss: 0.0004\n",
      "Epoch: [191/200], Step: [15/16], Loss: 0.0005\n",
      "Epoch: [192/200], Step: [1/16], Loss: 0.0004\n",
      "Epoch: [192/200], Step: [2/16], Loss: 0.0004\n",
      "Epoch: [192/200], Step: [3/16], Loss: 0.0004\n",
      "Epoch: [192/200], Step: [4/16], Loss: 0.0004\n",
      "Epoch: [192/200], Step: [5/16], Loss: 0.0004\n",
      "Epoch: [192/200], Step: [6/16], Loss: 0.0004\n",
      "Epoch: [192/200], Step: [7/16], Loss: 0.0004\n",
      "Epoch: [192/200], Step: [8/16], Loss: 0.0005\n",
      "Epoch: [192/200], Step: [9/16], Loss: 0.0004\n",
      "Epoch: [192/200], Step: [10/16], Loss: 0.0003\n",
      "Epoch: [192/200], Step: [11/16], Loss: 0.0003\n",
      "Epoch: [192/200], Step: [12/16], Loss: 0.0004\n",
      "Epoch: [192/200], Step: [13/16], Loss: 0.0003\n",
      "Epoch: [192/200], Step: [14/16], Loss: 0.0004\n",
      "Epoch: [192/200], Step: [15/16], Loss: 0.0004\n",
      "Epoch: [193/200], Step: [1/16], Loss: 0.0003\n",
      "Epoch: [193/200], Step: [2/16], Loss: 0.0004\n",
      "Epoch: [193/200], Step: [3/16], Loss: 0.0004\n",
      "Epoch: [193/200], Step: [4/16], Loss: 0.0004\n",
      "Epoch: [193/200], Step: [5/16], Loss: 0.0004\n",
      "Epoch: [193/200], Step: [6/16], Loss: 0.0004\n",
      "Epoch: [193/200], Step: [7/16], Loss: 0.0004\n",
      "Epoch: [193/200], Step: [8/16], Loss: 0.0003\n",
      "Epoch: [193/200], Step: [9/16], Loss: 0.0004\n",
      "Epoch: [193/200], Step: [10/16], Loss: 0.0004\n",
      "Epoch: [193/200], Step: [11/16], Loss: 0.0004\n",
      "Epoch: [193/200], Step: [12/16], Loss: 0.0004\n",
      "Epoch: [193/200], Step: [13/16], Loss: 0.0004\n",
      "Epoch: [193/200], Step: [14/16], Loss: 0.0005\n",
      "Epoch: [193/200], Step: [15/16], Loss: 0.0004\n",
      "Epoch: [194/200], Step: [1/16], Loss: 0.0004\n",
      "Epoch: [194/200], Step: [2/16], Loss: 0.0004\n",
      "Epoch: [194/200], Step: [3/16], Loss: 0.0004\n",
      "Epoch: [194/200], Step: [4/16], Loss: 0.0004\n",
      "Epoch: [194/200], Step: [5/16], Loss: 0.0004\n",
      "Epoch: [194/200], Step: [6/16], Loss: 0.0004\n",
      "Epoch: [194/200], Step: [7/16], Loss: 0.0003\n",
      "Epoch: [194/200], Step: [8/16], Loss: 0.0004\n",
      "Epoch: [194/200], Step: [9/16], Loss: 0.0004\n",
      "Epoch: [194/200], Step: [10/16], Loss: 0.0004\n",
      "Epoch: [194/200], Step: [11/16], Loss: 0.0003\n",
      "Epoch: [194/200], Step: [12/16], Loss: 0.0004\n",
      "Epoch: [194/200], Step: [13/16], Loss: 0.0004\n",
      "Epoch: [194/200], Step: [14/16], Loss: 0.0004\n",
      "Epoch: [194/200], Step: [15/16], Loss: 0.0004\n",
      "Epoch: [195/200], Step: [1/16], Loss: 0.0004\n",
      "Epoch: [195/200], Step: [2/16], Loss: 0.0004\n",
      "Epoch: [195/200], Step: [3/16], Loss: 0.0004\n",
      "Epoch: [195/200], Step: [4/16], Loss: 0.0004\n",
      "Epoch: [195/200], Step: [5/16], Loss: 0.0004\n",
      "Epoch: [195/200], Step: [6/16], Loss: 0.0004\n",
      "Epoch: [195/200], Step: [7/16], Loss: 0.0004\n",
      "Epoch: [195/200], Step: [8/16], Loss: 0.0003\n",
      "Epoch: [195/200], Step: [9/16], Loss: 0.0004\n",
      "Epoch: [195/200], Step: [10/16], Loss: 0.0003\n",
      "Epoch: [195/200], Step: [11/16], Loss: 0.0004\n",
      "Epoch: [195/200], Step: [12/16], Loss: 0.0004\n",
      "Epoch: [195/200], Step: [13/16], Loss: 0.0004\n",
      "Epoch: [195/200], Step: [14/16], Loss: 0.0004\n",
      "Epoch: [195/200], Step: [15/16], Loss: 0.0003\n",
      "Epoch: [196/200], Step: [1/16], Loss: 0.0004\n",
      "Epoch: [196/200], Step: [2/16], Loss: 0.0003\n",
      "Epoch: [196/200], Step: [3/16], Loss: 0.0004\n",
      "Epoch: [196/200], Step: [4/16], Loss: 0.0004\n",
      "Epoch: [196/200], Step: [5/16], Loss: 0.0004\n",
      "Epoch: [196/200], Step: [6/16], Loss: 0.0004\n",
      "Epoch: [196/200], Step: [7/16], Loss: 0.0003\n",
      "Epoch: [196/200], Step: [8/16], Loss: 0.0003\n",
      "Epoch: [196/200], Step: [9/16], Loss: 0.0003\n",
      "Epoch: [196/200], Step: [10/16], Loss: 0.0004\n",
      "Epoch: [196/200], Step: [11/16], Loss: 0.0004\n",
      "Epoch: [196/200], Step: [12/16], Loss: 0.0004\n",
      "Epoch: [196/200], Step: [13/16], Loss: 0.0004\n",
      "Epoch: [196/200], Step: [14/16], Loss: 0.0003\n",
      "Epoch: [196/200], Step: [15/16], Loss: 0.0003\n",
      "Epoch: [197/200], Step: [1/16], Loss: 0.0003\n",
      "Epoch: [197/200], Step: [2/16], Loss: 0.0004\n",
      "Epoch: [197/200], Step: [3/16], Loss: 0.0003\n",
      "Epoch: [197/200], Step: [4/16], Loss: 0.0004\n",
      "Epoch: [197/200], Step: [5/16], Loss: 0.0004\n",
      "Epoch: [197/200], Step: [6/16], Loss: 0.0004\n",
      "Epoch: [197/200], Step: [7/16], Loss: 0.0003\n",
      "Epoch: [197/200], Step: [8/16], Loss: 0.0003\n",
      "Epoch: [197/200], Step: [9/16], Loss: 0.0004\n",
      "Epoch: [197/200], Step: [10/16], Loss: 0.0004\n",
      "Epoch: [197/200], Step: [11/16], Loss: 0.0005\n",
      "Epoch: [197/200], Step: [12/16], Loss: 0.0004\n",
      "Epoch: [197/200], Step: [13/16], Loss: 0.0004\n",
      "Epoch: [197/200], Step: [14/16], Loss: 0.0004\n",
      "Epoch: [197/200], Step: [15/16], Loss: 0.0004\n",
      "Epoch: [198/200], Step: [1/16], Loss: 0.0004\n",
      "Epoch: [198/200], Step: [2/16], Loss: 0.0004\n",
      "Epoch: [198/200], Step: [3/16], Loss: 0.0004\n",
      "Epoch: [198/200], Step: [4/16], Loss: 0.0004\n",
      "Epoch: [198/200], Step: [5/16], Loss: 0.0004\n",
      "Epoch: [198/200], Step: [6/16], Loss: 0.0003\n",
      "Epoch: [198/200], Step: [7/16], Loss: 0.0004\n",
      "Epoch: [198/200], Step: [8/16], Loss: 0.0003\n",
      "Epoch: [198/200], Step: [9/16], Loss: 0.0004\n",
      "Epoch: [198/200], Step: [10/16], Loss: 0.0004\n",
      "Epoch: [198/200], Step: [11/16], Loss: 0.0003\n",
      "Epoch: [198/200], Step: [12/16], Loss: 0.0004\n",
      "Epoch: [198/200], Step: [13/16], Loss: 0.0004\n",
      "Epoch: [198/200], Step: [14/16], Loss: 0.0004\n",
      "Epoch: [198/200], Step: [15/16], Loss: 0.0003\n",
      "Epoch: [199/200], Step: [1/16], Loss: 0.0004\n",
      "Epoch: [199/200], Step: [2/16], Loss: 0.0003\n",
      "Epoch: [199/200], Step: [3/16], Loss: 0.0004\n",
      "Epoch: [199/200], Step: [4/16], Loss: 0.0003\n",
      "Epoch: [199/200], Step: [5/16], Loss: 0.0004\n",
      "Epoch: [199/200], Step: [6/16], Loss: 0.0003\n",
      "Epoch: [199/200], Step: [7/16], Loss: 0.0003\n",
      "Epoch: [199/200], Step: [8/16], Loss: 0.0004\n",
      "Epoch: [199/200], Step: [9/16], Loss: 0.0003\n",
      "Epoch: [199/200], Step: [10/16], Loss: 0.0004\n",
      "Epoch: [199/200], Step: [11/16], Loss: 0.0003\n",
      "Epoch: [199/200], Step: [12/16], Loss: 0.0004\n",
      "Epoch: [199/200], Step: [13/16], Loss: 0.0004\n",
      "Epoch: [199/200], Step: [14/16], Loss: 0.0003\n",
      "Epoch: [199/200], Step: [15/16], Loss: 0.0004\n",
      "Epoch: [200/200], Step: [1/16], Loss: 0.0003\n",
      "Epoch: [200/200], Step: [2/16], Loss: 0.0004\n",
      "Epoch: [200/200], Step: [3/16], Loss: 0.0004\n",
      "Epoch: [200/200], Step: [4/16], Loss: 0.0004\n",
      "Epoch: [200/200], Step: [5/16], Loss: 0.0004\n",
      "Epoch: [200/200], Step: [6/16], Loss: 0.0004\n",
      "Epoch: [200/200], Step: [7/16], Loss: 0.0003\n",
      "Epoch: [200/200], Step: [8/16], Loss: 0.0004\n",
      "Epoch: [200/200], Step: [9/16], Loss: 0.0004\n",
      "Epoch: [200/200], Step: [10/16], Loss: 0.0004\n",
      "Epoch: [200/200], Step: [11/16], Loss: 0.0004\n",
      "Epoch: [200/200], Step: [12/16], Loss: 0.0003\n",
      "Epoch: [200/200], Step: [13/16], Loss: 0.0003\n",
      "Epoch: [200/200], Step: [14/16], Loss: 0.0003\n",
      "Epoch: [200/200], Step: [15/16], Loss: 0.0003\n"
     ]
    }
   ],
   "source": [
    "# 训练模型\n",
    "for epoch in range(args.epochs):\n",
    "    for i, (images, labels) in enumerate(train_loader):\n",
    "        images = images.reshape(-1, 784)# .to(device)\n",
    "        labels = labels# .to(device)\n",
    "        # 前向传播\n",
    "        output = model(images)\n",
    "        # Loss = loss(output, labels.long())\n",
    "        # 如果采用第二种数据加载思路，请用下面这行代码\n",
    "        Loss = loss(output, labels)\n",
    "        # 反向传播\n",
    "        optimizer.zero_grad()\n",
    "        Loss.backward()\n",
    "        optimizer.step()\n",
    "        # 输出\n",
    "        print('Epoch: [{}/{}], Step: [{}/{}], Loss: {:.4f}'\n",
    "                  .format(epoch + 1, args.epochs, i + 1, int(len(train_image) / size_batch) + 1, Loss.item()))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy:：48.8%\n"
     ]
    }
   ],
   "source": [
    "# 模型测试\n",
    "with torch.no_grad():\n",
    "    total_cor = 0\n",
    "    total_num = 0\n",
    "    for images, labels in test_loader:\n",
    "        images = images.reshape(-1, 784)\n",
    "        labels = labels\n",
    "        output = model(images)\n",
    "        _, predict_result = torch.max(output.data, 1)\n",
    "        total_num += labels.size(0)\n",
    "        total_cor += (predict_result == labels).sum().item()\n",
    "    print(\"Test Accuracy:：{}%\".format(100 * total_cor / total_num))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# 3.实现卷积神经网络CNN"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 300,
   "outputs": [],
   "source": [
    "# 外部超参数设置\n",
    "parser = argparse.ArgumentParser()\n",
    "parser.add_argument('--num_classes', type=int, default=50,\n",
    "                    help='number of classes used')\n",
    "parser.add_argument('--num_samples_train', type=int, default=15,\n",
    "                    help='number of samples per class used for training')\n",
    "parser.add_argument('--num_samples_test', type=int, default=5,\n",
    "                    help='number of samples per class used for testing')\n",
    "parser.add_argument('--seed', type=int, default=1,\n",
    "                    help='random seed')\n",
    "parser.add_argument('--epochs', type=int, default=200,\n",
    "                    help='number of epochs')\n",
    "# args = parser.parse_args()\n",
    "args = parser.parse_known_args()[0]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 301,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(750, 1, 28, 28)\n",
      "(250, 1, 28, 28)\n"
     ]
    }
   ],
   "source": [
    "# 加载数据\n",
    "train_image, train_label, test_image, test_label = LoadData(args.num_classes, args.num_samples_train,\n",
    "                                                            args.num_samples_test, args.seed)\n",
    "# 训练用图像应该为二维，即28*28\n",
    "train_images_list = list()\n",
    "for i in range(0, len(train_image)):\n",
    "    temp_image = np.reshape(train_image[i], (1, 28, 28))\n",
    "    train_images_list.append(temp_image)\n",
    "train_images = np.array(train_images_list)\n",
    "print(train_images.shape)\n",
    "\n",
    "# 测试用图像应该为二维，即28*28\n",
    "test_images_list = list()\n",
    "for i in range(0, len(test_image)):\n",
    "    temp_image = np.reshape(test_image[i], (1, 28, 28))\n",
    "    test_images_list.append(temp_image)\n",
    "test_images = np.array(test_images_list)\n",
    "print(test_images.shape)\n",
    "\n",
    "# 转换为pytorch可处理数据集\n",
    "train_dataset = da.TensorDataset(torch.from_numpy(train_images), torch.from_numpy(train_label))\n",
    "test_dataset = da.TensorDataset(torch.from_numpy(test_images), torch.from_numpy(test_label))\n",
    "# 数据分批\n",
    "size_batch = 100\n",
    "train_loader = da.DataLoader(dataset=train_dataset, batch_size=size_batch, shuffle=True)\n",
    "test_loader = da.DataLoader(dataset=test_dataset, batch_size=size_batch, shuffle=False)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 340,
   "outputs": [],
   "source": [
    "# 定义超参数, 模型, 损失函数 和 优化器\n",
    "# 定义内部超参数\n",
    "learning_rate = 1e-3\n",
    "# 定义模型\n",
    "class CNN(nn.Module):\n",
    "    def __init__(self, size_out):\n",
    "        super(CNN, self).__init__()\n",
    "        self.layer1 = torch.nn.Sequential(\n",
    "            torch.nn.Conv2d(1, 10, kernel_size=5),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.MaxPool2d(kernel_size=2)\n",
    "        )\n",
    "        self.layer2 = torch.nn.Sequential(\n",
    "            torch.nn.Conv2d(10, 20, kernel_size=5),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.MaxPool2d(kernel_size=2)\n",
    "        )\n",
    "        self.layer3 = torch.nn.Sequential(\n",
    "            torch.nn.Linear(320, 100),\n",
    "            torch.nn.Linear(100, 50)\n",
    "        )\n",
    "    def forward(self, x):\n",
    "        x = x.to(torch.float32)\n",
    "        layer1_out = self.layer1(x)\n",
    "        layer2_out = self.layer2(layer1_out)\n",
    "        layer2_out_solve = layer2_out.view(-1, 320)\n",
    "        layer3_out = self.layer3(layer2_out_solve)\n",
    "        return layer3_out\n",
    "model = CNN(size_out)  # .to(device)\n",
    "# 定义损失函数\n",
    "loss = nn.CrossEntropyLoss()\n",
    "# 定义优化器\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "# optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 341,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [1/200], Step: [1/8], Loss: 3.8941\n",
      "Epoch: [1/200], Step: [2/8], Loss: 3.9044\n",
      "Epoch: [1/200], Step: [3/8], Loss: 3.9101\n",
      "Epoch: [1/200], Step: [4/8], Loss: 3.9228\n",
      "Epoch: [1/200], Step: [5/8], Loss: 3.9516\n",
      "Epoch: [1/200], Step: [6/8], Loss: 3.9341\n",
      "Epoch: [1/200], Step: [7/8], Loss: 3.9223\n",
      "Epoch: [1/200], Step: [8/8], Loss: 3.9209\n",
      "Epoch: [2/200], Step: [1/8], Loss: 3.8754\n",
      "Epoch: [2/200], Step: [2/8], Loss: 3.8747\n",
      "Epoch: [2/200], Step: [3/8], Loss: 3.8778\n",
      "Epoch: [2/200], Step: [4/8], Loss: 3.8617\n",
      "Epoch: [2/200], Step: [5/8], Loss: 3.8581\n",
      "Epoch: [2/200], Step: [6/8], Loss: 3.8536\n",
      "Epoch: [2/200], Step: [7/8], Loss: 3.8464\n",
      "Epoch: [2/200], Step: [8/8], Loss: 3.8631\n",
      "Epoch: [3/200], Step: [1/8], Loss: 3.8180\n",
      "Epoch: [3/200], Step: [2/8], Loss: 3.8032\n",
      "Epoch: [3/200], Step: [3/8], Loss: 3.8012\n",
      "Epoch: [3/200], Step: [4/8], Loss: 3.7648\n",
      "Epoch: [3/200], Step: [5/8], Loss: 3.7375\n",
      "Epoch: [3/200], Step: [6/8], Loss: 3.7628\n",
      "Epoch: [3/200], Step: [7/8], Loss: 3.7633\n",
      "Epoch: [3/200], Step: [8/8], Loss: 3.7297\n",
      "Epoch: [4/200], Step: [1/8], Loss: 3.6662\n",
      "Epoch: [4/200], Step: [2/8], Loss: 3.6593\n",
      "Epoch: [4/200], Step: [3/8], Loss: 3.5943\n",
      "Epoch: [4/200], Step: [4/8], Loss: 3.5706\n",
      "Epoch: [4/200], Step: [5/8], Loss: 3.5364\n",
      "Epoch: [4/200], Step: [6/8], Loss: 3.5933\n",
      "Epoch: [4/200], Step: [7/8], Loss: 3.5284\n",
      "Epoch: [4/200], Step: [8/8], Loss: 3.4850\n",
      "Epoch: [5/200], Step: [1/8], Loss: 3.3280\n",
      "Epoch: [5/200], Step: [2/8], Loss: 3.3991\n",
      "Epoch: [5/200], Step: [3/8], Loss: 3.2462\n",
      "Epoch: [5/200], Step: [4/8], Loss: 3.2273\n",
      "Epoch: [5/200], Step: [5/8], Loss: 3.2184\n",
      "Epoch: [5/200], Step: [6/8], Loss: 3.1741\n",
      "Epoch: [5/200], Step: [7/8], Loss: 3.0659\n",
      "Epoch: [5/200], Step: [8/8], Loss: 2.8121\n",
      "Epoch: [6/200], Step: [1/8], Loss: 2.9843\n",
      "Epoch: [6/200], Step: [2/8], Loss: 2.7553\n",
      "Epoch: [6/200], Step: [3/8], Loss: 2.7428\n",
      "Epoch: [6/200], Step: [4/8], Loss: 2.5432\n",
      "Epoch: [6/200], Step: [5/8], Loss: 2.5474\n",
      "Epoch: [6/200], Step: [6/8], Loss: 2.5077\n",
      "Epoch: [6/200], Step: [7/8], Loss: 2.4685\n",
      "Epoch: [6/200], Step: [8/8], Loss: 2.5414\n",
      "Epoch: [7/200], Step: [1/8], Loss: 2.2947\n",
      "Epoch: [7/200], Step: [2/8], Loss: 2.1416\n",
      "Epoch: [7/200], Step: [3/8], Loss: 2.0734\n",
      "Epoch: [7/200], Step: [4/8], Loss: 2.0515\n",
      "Epoch: [7/200], Step: [5/8], Loss: 2.0587\n",
      "Epoch: [7/200], Step: [6/8], Loss: 1.9695\n",
      "Epoch: [7/200], Step: [7/8], Loss: 2.0442\n",
      "Epoch: [7/200], Step: [8/8], Loss: 1.5664\n",
      "Epoch: [8/200], Step: [1/8], Loss: 1.8549\n",
      "Epoch: [8/200], Step: [2/8], Loss: 1.6934\n",
      "Epoch: [8/200], Step: [3/8], Loss: 1.6291\n",
      "Epoch: [8/200], Step: [4/8], Loss: 1.8674\n",
      "Epoch: [8/200], Step: [5/8], Loss: 1.5368\n",
      "Epoch: [8/200], Step: [6/8], Loss: 1.6281\n",
      "Epoch: [8/200], Step: [7/8], Loss: 1.6757\n",
      "Epoch: [8/200], Step: [8/8], Loss: 1.9563\n",
      "Epoch: [9/200], Step: [1/8], Loss: 1.4779\n",
      "Epoch: [9/200], Step: [2/8], Loss: 1.5362\n",
      "Epoch: [9/200], Step: [3/8], Loss: 1.4419\n",
      "Epoch: [9/200], Step: [4/8], Loss: 1.5555\n",
      "Epoch: [9/200], Step: [5/8], Loss: 1.5992\n",
      "Epoch: [9/200], Step: [6/8], Loss: 1.3722\n",
      "Epoch: [9/200], Step: [7/8], Loss: 1.4318\n",
      "Epoch: [9/200], Step: [8/8], Loss: 1.5066\n",
      "Epoch: [10/200], Step: [1/8], Loss: 1.1896\n",
      "Epoch: [10/200], Step: [2/8], Loss: 1.2330\n",
      "Epoch: [10/200], Step: [3/8], Loss: 1.2868\n",
      "Epoch: [10/200], Step: [4/8], Loss: 1.2107\n",
      "Epoch: [10/200], Step: [5/8], Loss: 1.3709\n",
      "Epoch: [10/200], Step: [6/8], Loss: 1.6575\n",
      "Epoch: [10/200], Step: [7/8], Loss: 1.2420\n",
      "Epoch: [10/200], Step: [8/8], Loss: 1.2465\n",
      "Epoch: [11/200], Step: [1/8], Loss: 1.1484\n",
      "Epoch: [11/200], Step: [2/8], Loss: 1.2395\n",
      "Epoch: [11/200], Step: [3/8], Loss: 1.0845\n",
      "Epoch: [11/200], Step: [4/8], Loss: 1.1453\n",
      "Epoch: [11/200], Step: [5/8], Loss: 1.1617\n",
      "Epoch: [11/200], Step: [6/8], Loss: 1.2855\n",
      "Epoch: [11/200], Step: [7/8], Loss: 1.1074\n",
      "Epoch: [11/200], Step: [8/8], Loss: 1.2443\n",
      "Epoch: [12/200], Step: [1/8], Loss: 1.0355\n",
      "Epoch: [12/200], Step: [2/8], Loss: 0.9383\n",
      "Epoch: [12/200], Step: [3/8], Loss: 1.0296\n",
      "Epoch: [12/200], Step: [4/8], Loss: 1.2510\n",
      "Epoch: [12/200], Step: [5/8], Loss: 0.8924\n",
      "Epoch: [12/200], Step: [6/8], Loss: 0.9522\n",
      "Epoch: [12/200], Step: [7/8], Loss: 1.1024\n",
      "Epoch: [12/200], Step: [8/8], Loss: 0.9040\n",
      "Epoch: [13/200], Step: [1/8], Loss: 1.0483\n",
      "Epoch: [13/200], Step: [2/8], Loss: 0.8296\n",
      "Epoch: [13/200], Step: [3/8], Loss: 0.8229\n",
      "Epoch: [13/200], Step: [4/8], Loss: 0.9711\n",
      "Epoch: [13/200], Step: [5/8], Loss: 0.9487\n",
      "Epoch: [13/200], Step: [6/8], Loss: 0.9135\n",
      "Epoch: [13/200], Step: [7/8], Loss: 1.1070\n",
      "Epoch: [13/200], Step: [8/8], Loss: 0.8047\n",
      "Epoch: [14/200], Step: [1/8], Loss: 0.7719\n",
      "Epoch: [14/200], Step: [2/8], Loss: 0.9317\n",
      "Epoch: [14/200], Step: [3/8], Loss: 0.9850\n",
      "Epoch: [14/200], Step: [4/8], Loss: 0.9166\n",
      "Epoch: [14/200], Step: [5/8], Loss: 0.8944\n",
      "Epoch: [14/200], Step: [6/8], Loss: 0.7638\n",
      "Epoch: [14/200], Step: [7/8], Loss: 0.7881\n",
      "Epoch: [14/200], Step: [8/8], Loss: 0.9054\n",
      "Epoch: [15/200], Step: [1/8], Loss: 0.7037\n",
      "Epoch: [15/200], Step: [2/8], Loss: 0.6765\n",
      "Epoch: [15/200], Step: [3/8], Loss: 0.7919\n",
      "Epoch: [15/200], Step: [4/8], Loss: 0.7850\n",
      "Epoch: [15/200], Step: [5/8], Loss: 0.8082\n",
      "Epoch: [15/200], Step: [6/8], Loss: 0.7279\n",
      "Epoch: [15/200], Step: [7/8], Loss: 0.9482\n",
      "Epoch: [15/200], Step: [8/8], Loss: 1.0834\n",
      "Epoch: [16/200], Step: [1/8], Loss: 0.8492\n",
      "Epoch: [16/200], Step: [2/8], Loss: 0.6587\n",
      "Epoch: [16/200], Step: [3/8], Loss: 0.6840\n",
      "Epoch: [16/200], Step: [4/8], Loss: 0.8448\n",
      "Epoch: [16/200], Step: [5/8], Loss: 0.6424\n",
      "Epoch: [16/200], Step: [6/8], Loss: 0.6177\n",
      "Epoch: [16/200], Step: [7/8], Loss: 0.7490\n",
      "Epoch: [16/200], Step: [8/8], Loss: 0.6198\n",
      "Epoch: [17/200], Step: [1/8], Loss: 0.6754\n",
      "Epoch: [17/200], Step: [2/8], Loss: 0.6674\n",
      "Epoch: [17/200], Step: [3/8], Loss: 0.6682\n",
      "Epoch: [17/200], Step: [4/8], Loss: 0.5765\n",
      "Epoch: [17/200], Step: [5/8], Loss: 0.8261\n",
      "Epoch: [17/200], Step: [6/8], Loss: 0.6391\n",
      "Epoch: [17/200], Step: [7/8], Loss: 0.5954\n",
      "Epoch: [17/200], Step: [8/8], Loss: 0.6678\n",
      "Epoch: [18/200], Step: [1/8], Loss: 0.4099\n",
      "Epoch: [18/200], Step: [2/8], Loss: 0.5334\n",
      "Epoch: [18/200], Step: [3/8], Loss: 0.6171\n",
      "Epoch: [18/200], Step: [4/8], Loss: 0.7161\n",
      "Epoch: [18/200], Step: [5/8], Loss: 0.8192\n",
      "Epoch: [18/200], Step: [6/8], Loss: 0.6969\n",
      "Epoch: [18/200], Step: [7/8], Loss: 0.7296\n",
      "Epoch: [18/200], Step: [8/8], Loss: 0.5669\n",
      "Epoch: [19/200], Step: [1/8], Loss: 0.5285\n",
      "Epoch: [19/200], Step: [2/8], Loss: 0.4135\n",
      "Epoch: [19/200], Step: [3/8], Loss: 0.6198\n",
      "Epoch: [19/200], Step: [4/8], Loss: 0.6705\n",
      "Epoch: [19/200], Step: [5/8], Loss: 0.7300\n",
      "Epoch: [19/200], Step: [6/8], Loss: 0.4726\n",
      "Epoch: [19/200], Step: [7/8], Loss: 0.6670\n",
      "Epoch: [19/200], Step: [8/8], Loss: 0.5271\n",
      "Epoch: [20/200], Step: [1/8], Loss: 0.5096\n",
      "Epoch: [20/200], Step: [2/8], Loss: 0.5286\n",
      "Epoch: [20/200], Step: [3/8], Loss: 0.6186\n",
      "Epoch: [20/200], Step: [4/8], Loss: 0.7551\n",
      "Epoch: [20/200], Step: [5/8], Loss: 0.5363\n",
      "Epoch: [20/200], Step: [6/8], Loss: 0.4546\n",
      "Epoch: [20/200], Step: [7/8], Loss: 0.5143\n",
      "Epoch: [20/200], Step: [8/8], Loss: 0.3709\n",
      "Epoch: [21/200], Step: [1/8], Loss: 0.5526\n",
      "Epoch: [21/200], Step: [2/8], Loss: 0.5296\n",
      "Epoch: [21/200], Step: [3/8], Loss: 0.5901\n",
      "Epoch: [21/200], Step: [4/8], Loss: 0.4912\n",
      "Epoch: [21/200], Step: [5/8], Loss: 0.4702\n",
      "Epoch: [21/200], Step: [6/8], Loss: 0.3840\n",
      "Epoch: [21/200], Step: [7/8], Loss: 0.4454\n",
      "Epoch: [21/200], Step: [8/8], Loss: 0.5123\n",
      "Epoch: [22/200], Step: [1/8], Loss: 0.3417\n",
      "Epoch: [22/200], Step: [2/8], Loss: 0.4002\n",
      "Epoch: [22/200], Step: [3/8], Loss: 0.3993\n",
      "Epoch: [22/200], Step: [4/8], Loss: 0.4753\n",
      "Epoch: [22/200], Step: [5/8], Loss: 0.4694\n",
      "Epoch: [22/200], Step: [6/8], Loss: 0.7151\n",
      "Epoch: [22/200], Step: [7/8], Loss: 0.5296\n",
      "Epoch: [22/200], Step: [8/8], Loss: 0.4704\n",
      "Epoch: [23/200], Step: [1/8], Loss: 0.4278\n",
      "Epoch: [23/200], Step: [2/8], Loss: 0.4649\n",
      "Epoch: [23/200], Step: [3/8], Loss: 0.3736\n",
      "Epoch: [23/200], Step: [4/8], Loss: 0.4674\n",
      "Epoch: [23/200], Step: [5/8], Loss: 0.5327\n",
      "Epoch: [23/200], Step: [6/8], Loss: 0.3388\n",
      "Epoch: [23/200], Step: [7/8], Loss: 0.3882\n",
      "Epoch: [23/200], Step: [8/8], Loss: 0.5126\n",
      "Epoch: [24/200], Step: [1/8], Loss: 0.3796\n",
      "Epoch: [24/200], Step: [2/8], Loss: 0.4040\n",
      "Epoch: [24/200], Step: [3/8], Loss: 0.4210\n",
      "Epoch: [24/200], Step: [4/8], Loss: 0.5062\n",
      "Epoch: [24/200], Step: [5/8], Loss: 0.3167\n",
      "Epoch: [24/200], Step: [6/8], Loss: 0.3850\n",
      "Epoch: [24/200], Step: [7/8], Loss: 0.3999\n",
      "Epoch: [24/200], Step: [8/8], Loss: 0.2721\n",
      "Epoch: [25/200], Step: [1/8], Loss: 0.3386\n",
      "Epoch: [25/200], Step: [2/8], Loss: 0.4578\n",
      "Epoch: [25/200], Step: [3/8], Loss: 0.3083\n",
      "Epoch: [25/200], Step: [4/8], Loss: 0.2862\n",
      "Epoch: [25/200], Step: [5/8], Loss: 0.3242\n",
      "Epoch: [25/200], Step: [6/8], Loss: 0.2607\n",
      "Epoch: [25/200], Step: [7/8], Loss: 0.4234\n",
      "Epoch: [25/200], Step: [8/8], Loss: 0.3913\n",
      "Epoch: [26/200], Step: [1/8], Loss: 0.3900\n",
      "Epoch: [26/200], Step: [2/8], Loss: 0.2339\n",
      "Epoch: [26/200], Step: [3/8], Loss: 0.3072\n",
      "Epoch: [26/200], Step: [4/8], Loss: 0.3189\n",
      "Epoch: [26/200], Step: [5/8], Loss: 0.3220\n",
      "Epoch: [26/200], Step: [6/8], Loss: 0.2514\n",
      "Epoch: [26/200], Step: [7/8], Loss: 0.3651\n",
      "Epoch: [26/200], Step: [8/8], Loss: 0.2700\n",
      "Epoch: [27/200], Step: [1/8], Loss: 0.2056\n",
      "Epoch: [27/200], Step: [2/8], Loss: 0.2374\n",
      "Epoch: [27/200], Step: [3/8], Loss: 0.3312\n",
      "Epoch: [27/200], Step: [4/8], Loss: 0.2798\n",
      "Epoch: [27/200], Step: [5/8], Loss: 0.3304\n",
      "Epoch: [27/200], Step: [6/8], Loss: 0.3174\n",
      "Epoch: [27/200], Step: [7/8], Loss: 0.3491\n",
      "Epoch: [27/200], Step: [8/8], Loss: 0.1689\n",
      "Epoch: [28/200], Step: [1/8], Loss: 0.2335\n",
      "Epoch: [28/200], Step: [2/8], Loss: 0.2343\n",
      "Epoch: [28/200], Step: [3/8], Loss: 0.4565\n",
      "Epoch: [28/200], Step: [4/8], Loss: 0.2250\n",
      "Epoch: [28/200], Step: [5/8], Loss: 0.2327\n",
      "Epoch: [28/200], Step: [6/8], Loss: 0.2687\n",
      "Epoch: [28/200], Step: [7/8], Loss: 0.2318\n",
      "Epoch: [28/200], Step: [8/8], Loss: 0.2481\n",
      "Epoch: [29/200], Step: [1/8], Loss: 0.2998\n",
      "Epoch: [29/200], Step: [2/8], Loss: 0.3005\n",
      "Epoch: [29/200], Step: [3/8], Loss: 0.2231\n",
      "Epoch: [29/200], Step: [4/8], Loss: 0.3020\n",
      "Epoch: [29/200], Step: [5/8], Loss: 0.2176\n",
      "Epoch: [29/200], Step: [6/8], Loss: 0.1843\n",
      "Epoch: [29/200], Step: [7/8], Loss: 0.1895\n",
      "Epoch: [29/200], Step: [8/8], Loss: 0.2158\n",
      "Epoch: [30/200], Step: [1/8], Loss: 0.1741\n",
      "Epoch: [30/200], Step: [2/8], Loss: 0.2168\n",
      "Epoch: [30/200], Step: [3/8], Loss: 0.1704\n",
      "Epoch: [30/200], Step: [4/8], Loss: 0.2279\n",
      "Epoch: [30/200], Step: [5/8], Loss: 0.2530\n",
      "Epoch: [30/200], Step: [6/8], Loss: 0.2180\n",
      "Epoch: [30/200], Step: [7/8], Loss: 0.2487\n",
      "Epoch: [30/200], Step: [8/8], Loss: 0.1254\n",
      "Epoch: [31/200], Step: [1/8], Loss: 0.2502\n",
      "Epoch: [31/200], Step: [2/8], Loss: 0.2375\n",
      "Epoch: [31/200], Step: [3/8], Loss: 0.1415\n",
      "Epoch: [31/200], Step: [4/8], Loss: 0.1813\n",
      "Epoch: [31/200], Step: [5/8], Loss: 0.1379\n",
      "Epoch: [31/200], Step: [6/8], Loss: 0.1798\n",
      "Epoch: [31/200], Step: [7/8], Loss: 0.1946\n",
      "Epoch: [31/200], Step: [8/8], Loss: 0.0941\n",
      "Epoch: [32/200], Step: [1/8], Loss: 0.1246\n",
      "Epoch: [32/200], Step: [2/8], Loss: 0.1522\n",
      "Epoch: [32/200], Step: [3/8], Loss: 0.1250\n",
      "Epoch: [32/200], Step: [4/8], Loss: 0.2639\n",
      "Epoch: [32/200], Step: [5/8], Loss: 0.1528\n",
      "Epoch: [32/200], Step: [6/8], Loss: 0.1318\n",
      "Epoch: [32/200], Step: [7/8], Loss: 0.2114\n",
      "Epoch: [32/200], Step: [8/8], Loss: 0.2440\n",
      "Epoch: [33/200], Step: [1/8], Loss: 0.2040\n",
      "Epoch: [33/200], Step: [2/8], Loss: 0.2229\n",
      "Epoch: [33/200], Step: [3/8], Loss: 0.1423\n",
      "Epoch: [33/200], Step: [4/8], Loss: 0.1157\n",
      "Epoch: [33/200], Step: [5/8], Loss: 0.1932\n",
      "Epoch: [33/200], Step: [6/8], Loss: 0.1529\n",
      "Epoch: [33/200], Step: [7/8], Loss: 0.1348\n",
      "Epoch: [33/200], Step: [8/8], Loss: 0.2834\n",
      "Epoch: [34/200], Step: [1/8], Loss: 0.1439\n",
      "Epoch: [34/200], Step: [2/8], Loss: 0.1746\n",
      "Epoch: [34/200], Step: [3/8], Loss: 0.1547\n",
      "Epoch: [34/200], Step: [4/8], Loss: 0.1450\n",
      "Epoch: [34/200], Step: [5/8], Loss: 0.0943\n",
      "Epoch: [34/200], Step: [6/8], Loss: 0.2205\n",
      "Epoch: [34/200], Step: [7/8], Loss: 0.1152\n",
      "Epoch: [34/200], Step: [8/8], Loss: 0.1770\n",
      "Epoch: [35/200], Step: [1/8], Loss: 0.1334\n",
      "Epoch: [35/200], Step: [2/8], Loss: 0.1675\n",
      "Epoch: [35/200], Step: [3/8], Loss: 0.1711\n",
      "Epoch: [35/200], Step: [4/8], Loss: 0.1117\n",
      "Epoch: [35/200], Step: [5/8], Loss: 0.0962\n",
      "Epoch: [35/200], Step: [6/8], Loss: 0.0870\n",
      "Epoch: [35/200], Step: [7/8], Loss: 0.1188\n",
      "Epoch: [35/200], Step: [8/8], Loss: 0.1543\n",
      "Epoch: [36/200], Step: [1/8], Loss: 0.0972\n",
      "Epoch: [36/200], Step: [2/8], Loss: 0.1426\n",
      "Epoch: [36/200], Step: [3/8], Loss: 0.1421\n",
      "Epoch: [36/200], Step: [4/8], Loss: 0.1211\n",
      "Epoch: [36/200], Step: [5/8], Loss: 0.0797\n",
      "Epoch: [36/200], Step: [6/8], Loss: 0.0789\n",
      "Epoch: [36/200], Step: [7/8], Loss: 0.1638\n",
      "Epoch: [36/200], Step: [8/8], Loss: 0.1609\n",
      "Epoch: [37/200], Step: [1/8], Loss: 0.1162\n",
      "Epoch: [37/200], Step: [2/8], Loss: 0.0896\n",
      "Epoch: [37/200], Step: [3/8], Loss: 0.0832\n",
      "Epoch: [37/200], Step: [4/8], Loss: 0.1523\n",
      "Epoch: [37/200], Step: [5/8], Loss: 0.1165\n",
      "Epoch: [37/200], Step: [6/8], Loss: 0.1380\n",
      "Epoch: [37/200], Step: [7/8], Loss: 0.1130\n",
      "Epoch: [37/200], Step: [8/8], Loss: 0.0606\n",
      "Epoch: [38/200], Step: [1/8], Loss: 0.1151\n",
      "Epoch: [38/200], Step: [2/8], Loss: 0.0797\n",
      "Epoch: [38/200], Step: [3/8], Loss: 0.0970\n",
      "Epoch: [38/200], Step: [4/8], Loss: 0.1316\n",
      "Epoch: [38/200], Step: [5/8], Loss: 0.1139\n",
      "Epoch: [38/200], Step: [6/8], Loss: 0.1024\n",
      "Epoch: [38/200], Step: [7/8], Loss: 0.1191\n",
      "Epoch: [38/200], Step: [8/8], Loss: 0.1130\n",
      "Epoch: [39/200], Step: [1/8], Loss: 0.0745\n",
      "Epoch: [39/200], Step: [2/8], Loss: 0.1096\n",
      "Epoch: [39/200], Step: [3/8], Loss: 0.1062\n",
      "Epoch: [39/200], Step: [4/8], Loss: 0.0944\n",
      "Epoch: [39/200], Step: [5/8], Loss: 0.0743\n",
      "Epoch: [39/200], Step: [6/8], Loss: 0.0866\n",
      "Epoch: [39/200], Step: [7/8], Loss: 0.1001\n",
      "Epoch: [39/200], Step: [8/8], Loss: 0.0618\n",
      "Epoch: [40/200], Step: [1/8], Loss: 0.0911\n",
      "Epoch: [40/200], Step: [2/8], Loss: 0.1119\n",
      "Epoch: [40/200], Step: [3/8], Loss: 0.0826\n",
      "Epoch: [40/200], Step: [4/8], Loss: 0.0714\n",
      "Epoch: [40/200], Step: [5/8], Loss: 0.0689\n",
      "Epoch: [40/200], Step: [6/8], Loss: 0.0805\n",
      "Epoch: [40/200], Step: [7/8], Loss: 0.0839\n",
      "Epoch: [40/200], Step: [8/8], Loss: 0.0602\n",
      "Epoch: [41/200], Step: [1/8], Loss: 0.0852\n",
      "Epoch: [41/200], Step: [2/8], Loss: 0.0999\n",
      "Epoch: [41/200], Step: [3/8], Loss: 0.0786\n",
      "Epoch: [41/200], Step: [4/8], Loss: 0.0793\n",
      "Epoch: [41/200], Step: [5/8], Loss: 0.0816\n",
      "Epoch: [41/200], Step: [6/8], Loss: 0.0813\n",
      "Epoch: [41/200], Step: [7/8], Loss: 0.0497\n",
      "Epoch: [41/200], Step: [8/8], Loss: 0.0465\n",
      "Epoch: [42/200], Step: [1/8], Loss: 0.0668\n",
      "Epoch: [42/200], Step: [2/8], Loss: 0.0996\n",
      "Epoch: [42/200], Step: [3/8], Loss: 0.0667\n",
      "Epoch: [42/200], Step: [4/8], Loss: 0.0592\n",
      "Epoch: [42/200], Step: [5/8], Loss: 0.0429\n",
      "Epoch: [42/200], Step: [6/8], Loss: 0.0699\n",
      "Epoch: [42/200], Step: [7/8], Loss: 0.0790\n",
      "Epoch: [42/200], Step: [8/8], Loss: 0.1225\n",
      "Epoch: [43/200], Step: [1/8], Loss: 0.0667\n",
      "Epoch: [43/200], Step: [2/8], Loss: 0.0446\n",
      "Epoch: [43/200], Step: [3/8], Loss: 0.0636\n",
      "Epoch: [43/200], Step: [4/8], Loss: 0.0887\n",
      "Epoch: [43/200], Step: [5/8], Loss: 0.0527\n",
      "Epoch: [43/200], Step: [6/8], Loss: 0.0464\n",
      "Epoch: [43/200], Step: [7/8], Loss: 0.0746\n",
      "Epoch: [43/200], Step: [8/8], Loss: 0.1123\n",
      "Epoch: [44/200], Step: [1/8], Loss: 0.0792\n",
      "Epoch: [44/200], Step: [2/8], Loss: 0.0611\n",
      "Epoch: [44/200], Step: [3/8], Loss: 0.0854\n",
      "Epoch: [44/200], Step: [4/8], Loss: 0.0708\n",
      "Epoch: [44/200], Step: [5/8], Loss: 0.0498\n",
      "Epoch: [44/200], Step: [6/8], Loss: 0.0670\n",
      "Epoch: [44/200], Step: [7/8], Loss: 0.0551\n",
      "Epoch: [44/200], Step: [8/8], Loss: 0.0526\n",
      "Epoch: [45/200], Step: [1/8], Loss: 0.0353\n",
      "Epoch: [45/200], Step: [2/8], Loss: 0.0545\n",
      "Epoch: [45/200], Step: [3/8], Loss: 0.0445\n",
      "Epoch: [45/200], Step: [4/8], Loss: 0.0630\n",
      "Epoch: [45/200], Step: [5/8], Loss: 0.0676\n",
      "Epoch: [45/200], Step: [6/8], Loss: 0.0585\n",
      "Epoch: [45/200], Step: [7/8], Loss: 0.0598\n",
      "Epoch: [45/200], Step: [8/8], Loss: 0.0523\n",
      "Epoch: [46/200], Step: [1/8], Loss: 0.0572\n",
      "Epoch: [46/200], Step: [2/8], Loss: 0.0809\n",
      "Epoch: [46/200], Step: [3/8], Loss: 0.0467\n",
      "Epoch: [46/200], Step: [4/8], Loss: 0.0679\n",
      "Epoch: [46/200], Step: [5/8], Loss: 0.0404\n",
      "Epoch: [46/200], Step: [6/8], Loss: 0.0367\n",
      "Epoch: [46/200], Step: [7/8], Loss: 0.0523\n",
      "Epoch: [46/200], Step: [8/8], Loss: 0.0399\n",
      "Epoch: [47/200], Step: [1/8], Loss: 0.0584\n",
      "Epoch: [47/200], Step: [2/8], Loss: 0.0376\n",
      "Epoch: [47/200], Step: [3/8], Loss: 0.0612\n",
      "Epoch: [47/200], Step: [4/8], Loss: 0.0468\n",
      "Epoch: [47/200], Step: [5/8], Loss: 0.0433\n",
      "Epoch: [47/200], Step: [6/8], Loss: 0.0485\n",
      "Epoch: [47/200], Step: [7/8], Loss: 0.0333\n",
      "Epoch: [47/200], Step: [8/8], Loss: 0.0378\n",
      "Epoch: [48/200], Step: [1/8], Loss: 0.0454\n",
      "Epoch: [48/200], Step: [2/8], Loss: 0.0414\n",
      "Epoch: [48/200], Step: [3/8], Loss: 0.0606\n",
      "Epoch: [48/200], Step: [4/8], Loss: 0.0336\n",
      "Epoch: [48/200], Step: [5/8], Loss: 0.0304\n",
      "Epoch: [48/200], Step: [6/8], Loss: 0.0390\n",
      "Epoch: [48/200], Step: [7/8], Loss: 0.0408\n",
      "Epoch: [48/200], Step: [8/8], Loss: 0.0191\n",
      "Epoch: [49/200], Step: [1/8], Loss: 0.0418\n",
      "Epoch: [49/200], Step: [2/8], Loss: 0.0289\n",
      "Epoch: [49/200], Step: [3/8], Loss: 0.0284\n",
      "Epoch: [49/200], Step: [4/8], Loss: 0.0380\n",
      "Epoch: [49/200], Step: [5/8], Loss: 0.0422\n",
      "Epoch: [49/200], Step: [6/8], Loss: 0.0459\n",
      "Epoch: [49/200], Step: [7/8], Loss: 0.0273\n",
      "Epoch: [49/200], Step: [8/8], Loss: 0.0571\n",
      "Epoch: [50/200], Step: [1/8], Loss: 0.0274\n",
      "Epoch: [50/200], Step: [2/8], Loss: 0.0256\n",
      "Epoch: [50/200], Step: [3/8], Loss: 0.0433\n",
      "Epoch: [50/200], Step: [4/8], Loss: 0.0645\n",
      "Epoch: [50/200], Step: [5/8], Loss: 0.0607\n",
      "Epoch: [50/200], Step: [6/8], Loss: 0.0590\n",
      "Epoch: [50/200], Step: [7/8], Loss: 0.0422\n",
      "Epoch: [50/200], Step: [8/8], Loss: 0.0728\n",
      "Epoch: [51/200], Step: [1/8], Loss: 0.0563\n",
      "Epoch: [51/200], Step: [2/8], Loss: 0.0610\n",
      "Epoch: [51/200], Step: [3/8], Loss: 0.0356\n",
      "Epoch: [51/200], Step: [4/8], Loss: 0.0322\n",
      "Epoch: [51/200], Step: [5/8], Loss: 0.0396\n",
      "Epoch: [51/200], Step: [6/8], Loss: 0.0500\n",
      "Epoch: [51/200], Step: [7/8], Loss: 0.0324\n",
      "Epoch: [51/200], Step: [8/8], Loss: 0.0322\n",
      "Epoch: [52/200], Step: [1/8], Loss: 0.0429\n",
      "Epoch: [52/200], Step: [2/8], Loss: 0.0254\n",
      "Epoch: [52/200], Step: [3/8], Loss: 0.0472\n",
      "Epoch: [52/200], Step: [4/8], Loss: 0.0530\n",
      "Epoch: [52/200], Step: [5/8], Loss: 0.0247\n",
      "Epoch: [52/200], Step: [6/8], Loss: 0.0323\n",
      "Epoch: [52/200], Step: [7/8], Loss: 0.0386\n",
      "Epoch: [52/200], Step: [8/8], Loss: 0.0306\n",
      "Epoch: [53/200], Step: [1/8], Loss: 0.0188\n",
      "Epoch: [53/200], Step: [2/8], Loss: 0.0312\n",
      "Epoch: [53/200], Step: [3/8], Loss: 0.0502\n",
      "Epoch: [53/200], Step: [4/8], Loss: 0.0375\n",
      "Epoch: [53/200], Step: [5/8], Loss: 0.0435\n",
      "Epoch: [53/200], Step: [6/8], Loss: 0.0497\n",
      "Epoch: [53/200], Step: [7/8], Loss: 0.0387\n",
      "Epoch: [53/200], Step: [8/8], Loss: 0.0870\n",
      "Epoch: [54/200], Step: [1/8], Loss: 0.0373\n",
      "Epoch: [54/200], Step: [2/8], Loss: 0.0236\n",
      "Epoch: [54/200], Step: [3/8], Loss: 0.0434\n",
      "Epoch: [54/200], Step: [4/8], Loss: 0.0315\n",
      "Epoch: [54/200], Step: [5/8], Loss: 0.0663\n",
      "Epoch: [54/200], Step: [6/8], Loss: 0.1191\n",
      "Epoch: [54/200], Step: [7/8], Loss: 0.0286\n",
      "Epoch: [54/200], Step: [8/8], Loss: 0.0156\n",
      "Epoch: [55/200], Step: [1/8], Loss: 0.0886\n",
      "Epoch: [55/200], Step: [2/8], Loss: 0.1438\n",
      "Epoch: [55/200], Step: [3/8], Loss: 0.0335\n",
      "Epoch: [55/200], Step: [4/8], Loss: 0.0246\n",
      "Epoch: [55/200], Step: [5/8], Loss: 0.0225\n",
      "Epoch: [55/200], Step: [6/8], Loss: 0.0267\n",
      "Epoch: [55/200], Step: [7/8], Loss: 0.0559\n",
      "Epoch: [55/200], Step: [8/8], Loss: 0.0383\n",
      "Epoch: [56/200], Step: [1/8], Loss: 0.0266\n",
      "Epoch: [56/200], Step: [2/8], Loss: 0.0321\n",
      "Epoch: [56/200], Step: [3/8], Loss: 0.0280\n",
      "Epoch: [56/200], Step: [4/8], Loss: 0.0334\n",
      "Epoch: [56/200], Step: [5/8], Loss: 0.0247\n",
      "Epoch: [56/200], Step: [6/8], Loss: 0.0301\n",
      "Epoch: [56/200], Step: [7/8], Loss: 0.0281\n",
      "Epoch: [56/200], Step: [8/8], Loss: 0.0276\n",
      "Epoch: [57/200], Step: [1/8], Loss: 0.0185\n",
      "Epoch: [57/200], Step: [2/8], Loss: 0.0184\n",
      "Epoch: [57/200], Step: [3/8], Loss: 0.0199\n",
      "Epoch: [57/200], Step: [4/8], Loss: 0.0195\n",
      "Epoch: [57/200], Step: [5/8], Loss: 0.0266\n",
      "Epoch: [57/200], Step: [6/8], Loss: 0.0317\n",
      "Epoch: [57/200], Step: [7/8], Loss: 0.0434\n",
      "Epoch: [57/200], Step: [8/8], Loss: 0.0216\n",
      "Epoch: [58/200], Step: [1/8], Loss: 0.0201\n",
      "Epoch: [58/200], Step: [2/8], Loss: 0.0319\n",
      "Epoch: [58/200], Step: [3/8], Loss: 0.0203\n",
      "Epoch: [58/200], Step: [4/8], Loss: 0.0241\n",
      "Epoch: [58/200], Step: [5/8], Loss: 0.0307\n",
      "Epoch: [58/200], Step: [6/8], Loss: 0.0147\n",
      "Epoch: [58/200], Step: [7/8], Loss: 0.0297\n",
      "Epoch: [58/200], Step: [8/8], Loss: 0.0188\n",
      "Epoch: [59/200], Step: [1/8], Loss: 0.0190\n",
      "Epoch: [59/200], Step: [2/8], Loss: 0.0203\n",
      "Epoch: [59/200], Step: [3/8], Loss: 0.0186\n",
      "Epoch: [59/200], Step: [4/8], Loss: 0.0183\n",
      "Epoch: [59/200], Step: [5/8], Loss: 0.0259\n",
      "Epoch: [59/200], Step: [6/8], Loss: 0.0253\n",
      "Epoch: [59/200], Step: [7/8], Loss: 0.0201\n",
      "Epoch: [59/200], Step: [8/8], Loss: 0.0313\n",
      "Epoch: [60/200], Step: [1/8], Loss: 0.0230\n",
      "Epoch: [60/200], Step: [2/8], Loss: 0.0161\n",
      "Epoch: [60/200], Step: [3/8], Loss: 0.0252\n",
      "Epoch: [60/200], Step: [4/8], Loss: 0.0148\n",
      "Epoch: [60/200], Step: [5/8], Loss: 0.0152\n",
      "Epoch: [60/200], Step: [6/8], Loss: 0.0245\n",
      "Epoch: [60/200], Step: [7/8], Loss: 0.0324\n",
      "Epoch: [60/200], Step: [8/8], Loss: 0.0161\n",
      "Epoch: [61/200], Step: [1/8], Loss: 0.0241\n",
      "Epoch: [61/200], Step: [2/8], Loss: 0.0243\n",
      "Epoch: [61/200], Step: [3/8], Loss: 0.0210\n",
      "Epoch: [61/200], Step: [4/8], Loss: 0.0153\n",
      "Epoch: [61/200], Step: [5/8], Loss: 0.0175\n",
      "Epoch: [61/200], Step: [6/8], Loss: 0.0313\n",
      "Epoch: [61/200], Step: [7/8], Loss: 0.0251\n",
      "Epoch: [61/200], Step: [8/8], Loss: 0.0153\n",
      "Epoch: [62/200], Step: [1/8], Loss: 0.0209\n",
      "Epoch: [62/200], Step: [2/8], Loss: 0.0174\n",
      "Epoch: [62/200], Step: [3/8], Loss: 0.0126\n",
      "Epoch: [62/200], Step: [4/8], Loss: 0.0229\n",
      "Epoch: [62/200], Step: [5/8], Loss: 0.0311\n",
      "Epoch: [62/200], Step: [6/8], Loss: 0.0200\n",
      "Epoch: [62/200], Step: [7/8], Loss: 0.0143\n",
      "Epoch: [62/200], Step: [8/8], Loss: 0.0153\n",
      "Epoch: [63/200], Step: [1/8], Loss: 0.0206\n",
      "Epoch: [63/200], Step: [2/8], Loss: 0.0132\n",
      "Epoch: [63/200], Step: [3/8], Loss: 0.0213\n",
      "Epoch: [63/200], Step: [4/8], Loss: 0.0158\n",
      "Epoch: [63/200], Step: [5/8], Loss: 0.0208\n",
      "Epoch: [63/200], Step: [6/8], Loss: 0.0166\n",
      "Epoch: [63/200], Step: [7/8], Loss: 0.0182\n",
      "Epoch: [63/200], Step: [8/8], Loss: 0.0162\n",
      "Epoch: [64/200], Step: [1/8], Loss: 0.0137\n",
      "Epoch: [64/200], Step: [2/8], Loss: 0.0174\n",
      "Epoch: [64/200], Step: [3/8], Loss: 0.0202\n",
      "Epoch: [64/200], Step: [4/8], Loss: 0.0197\n",
      "Epoch: [64/200], Step: [5/8], Loss: 0.0096\n",
      "Epoch: [64/200], Step: [6/8], Loss: 0.0211\n",
      "Epoch: [64/200], Step: [7/8], Loss: 0.0194\n",
      "Epoch: [64/200], Step: [8/8], Loss: 0.0125\n",
      "Epoch: [65/200], Step: [1/8], Loss: 0.0164\n",
      "Epoch: [65/200], Step: [2/8], Loss: 0.0168\n",
      "Epoch: [65/200], Step: [3/8], Loss: 0.0099\n",
      "Epoch: [65/200], Step: [4/8], Loss: 0.0112\n",
      "Epoch: [65/200], Step: [5/8], Loss: 0.0147\n",
      "Epoch: [65/200], Step: [6/8], Loss: 0.0189\n",
      "Epoch: [65/200], Step: [7/8], Loss: 0.0172\n",
      "Epoch: [65/200], Step: [8/8], Loss: 0.0216\n",
      "Epoch: [66/200], Step: [1/8], Loss: 0.0132\n",
      "Epoch: [66/200], Step: [2/8], Loss: 0.0088\n",
      "Epoch: [66/200], Step: [3/8], Loss: 0.0220\n",
      "Epoch: [66/200], Step: [4/8], Loss: 0.0108\n",
      "Epoch: [66/200], Step: [5/8], Loss: 0.0146\n",
      "Epoch: [66/200], Step: [6/8], Loss: 0.0124\n",
      "Epoch: [66/200], Step: [7/8], Loss: 0.0163\n",
      "Epoch: [66/200], Step: [8/8], Loss: 0.0131\n",
      "Epoch: [67/200], Step: [1/8], Loss: 0.0135\n",
      "Epoch: [67/200], Step: [2/8], Loss: 0.0118\n",
      "Epoch: [67/200], Step: [3/8], Loss: 0.0115\n",
      "Epoch: [67/200], Step: [4/8], Loss: 0.0251\n",
      "Epoch: [67/200], Step: [5/8], Loss: 0.0113\n",
      "Epoch: [67/200], Step: [6/8], Loss: 0.0146\n",
      "Epoch: [67/200], Step: [7/8], Loss: 0.0114\n",
      "Epoch: [67/200], Step: [8/8], Loss: 0.0161\n",
      "Epoch: [68/200], Step: [1/8], Loss: 0.0103\n",
      "Epoch: [68/200], Step: [2/8], Loss: 0.0129\n",
      "Epoch: [68/200], Step: [3/8], Loss: 0.0119\n",
      "Epoch: [68/200], Step: [4/8], Loss: 0.0104\n",
      "Epoch: [68/200], Step: [5/8], Loss: 0.0162\n",
      "Epoch: [68/200], Step: [6/8], Loss: 0.0127\n",
      "Epoch: [68/200], Step: [7/8], Loss: 0.0133\n",
      "Epoch: [68/200], Step: [8/8], Loss: 0.0148\n",
      "Epoch: [69/200], Step: [1/8], Loss: 0.0147\n",
      "Epoch: [69/200], Step: [2/8], Loss: 0.0104\n",
      "Epoch: [69/200], Step: [3/8], Loss: 0.0091\n",
      "Epoch: [69/200], Step: [4/8], Loss: 0.0308\n",
      "Epoch: [69/200], Step: [5/8], Loss: 0.0275\n",
      "Epoch: [69/200], Step: [6/8], Loss: 0.0155\n",
      "Epoch: [69/200], Step: [7/8], Loss: 0.0170\n",
      "Epoch: [69/200], Step: [8/8], Loss: 0.0113\n",
      "Epoch: [70/200], Step: [1/8], Loss: 0.0118\n",
      "Epoch: [70/200], Step: [2/8], Loss: 0.0290\n",
      "Epoch: [70/200], Step: [3/8], Loss: 0.0122\n",
      "Epoch: [70/200], Step: [4/8], Loss: 0.0085\n",
      "Epoch: [70/200], Step: [5/8], Loss: 0.0137\n",
      "Epoch: [70/200], Step: [6/8], Loss: 0.0128\n",
      "Epoch: [70/200], Step: [7/8], Loss: 0.0205\n",
      "Epoch: [70/200], Step: [8/8], Loss: 0.0111\n",
      "Epoch: [71/200], Step: [1/8], Loss: 0.0150\n",
      "Epoch: [71/200], Step: [2/8], Loss: 0.0160\n",
      "Epoch: [71/200], Step: [3/8], Loss: 0.0254\n",
      "Epoch: [71/200], Step: [4/8], Loss: 0.0148\n",
      "Epoch: [71/200], Step: [5/8], Loss: 0.0124\n",
      "Epoch: [71/200], Step: [6/8], Loss: 0.0130\n",
      "Epoch: [71/200], Step: [7/8], Loss: 0.0170\n",
      "Epoch: [71/200], Step: [8/8], Loss: 0.0136\n",
      "Epoch: [72/200], Step: [1/8], Loss: 0.0121\n",
      "Epoch: [72/200], Step: [2/8], Loss: 0.0210\n",
      "Epoch: [72/200], Step: [3/8], Loss: 0.0064\n",
      "Epoch: [72/200], Step: [4/8], Loss: 0.0095\n",
      "Epoch: [72/200], Step: [5/8], Loss: 0.0157\n",
      "Epoch: [72/200], Step: [6/8], Loss: 0.0408\n",
      "Epoch: [72/200], Step: [7/8], Loss: 0.0245\n",
      "Epoch: [72/200], Step: [8/8], Loss: 0.0106\n",
      "Epoch: [73/200], Step: [1/8], Loss: 0.0122\n",
      "Epoch: [73/200], Step: [2/8], Loss: 0.0639\n",
      "Epoch: [73/200], Step: [3/8], Loss: 0.0207\n",
      "Epoch: [73/200], Step: [4/8], Loss: 0.0104\n",
      "Epoch: [73/200], Step: [5/8], Loss: 0.0155\n",
      "Epoch: [73/200], Step: [6/8], Loss: 0.0098\n",
      "Epoch: [73/200], Step: [7/8], Loss: 0.0118\n",
      "Epoch: [73/200], Step: [8/8], Loss: 0.0133\n",
      "Epoch: [74/200], Step: [1/8], Loss: 0.0110\n",
      "Epoch: [74/200], Step: [2/8], Loss: 0.0240\n",
      "Epoch: [74/200], Step: [3/8], Loss: 0.0095\n",
      "Epoch: [74/200], Step: [4/8], Loss: 0.0149\n",
      "Epoch: [74/200], Step: [5/8], Loss: 0.0098\n",
      "Epoch: [74/200], Step: [6/8], Loss: 0.0068\n",
      "Epoch: [74/200], Step: [7/8], Loss: 0.0183\n",
      "Epoch: [74/200], Step: [8/8], Loss: 0.0083\n",
      "Epoch: [75/200], Step: [1/8], Loss: 0.0095\n",
      "Epoch: [75/200], Step: [2/8], Loss: 0.0088\n",
      "Epoch: [75/200], Step: [3/8], Loss: 0.0076\n",
      "Epoch: [75/200], Step: [4/8], Loss: 0.0130\n",
      "Epoch: [75/200], Step: [5/8], Loss: 0.0230\n",
      "Epoch: [75/200], Step: [6/8], Loss: 0.0147\n",
      "Epoch: [75/200], Step: [7/8], Loss: 0.0086\n",
      "Epoch: [75/200], Step: [8/8], Loss: 0.0070\n",
      "Epoch: [76/200], Step: [1/8], Loss: 0.0147\n",
      "Epoch: [76/200], Step: [2/8], Loss: 0.0120\n",
      "Epoch: [76/200], Step: [3/8], Loss: 0.0082\n",
      "Epoch: [76/200], Step: [4/8], Loss: 0.0066\n",
      "Epoch: [76/200], Step: [5/8], Loss: 0.0079\n",
      "Epoch: [76/200], Step: [6/8], Loss: 0.0162\n",
      "Epoch: [76/200], Step: [7/8], Loss: 0.0091\n",
      "Epoch: [76/200], Step: [8/8], Loss: 0.0090\n",
      "Epoch: [77/200], Step: [1/8], Loss: 0.0124\n",
      "Epoch: [77/200], Step: [2/8], Loss: 0.0110\n",
      "Epoch: [77/200], Step: [3/8], Loss: 0.0120\n",
      "Epoch: [77/200], Step: [4/8], Loss: 0.0202\n",
      "Epoch: [77/200], Step: [5/8], Loss: 0.0069\n",
      "Epoch: [77/200], Step: [6/8], Loss: 0.0123\n",
      "Epoch: [77/200], Step: [7/8], Loss: 0.0063\n",
      "Epoch: [77/200], Step: [8/8], Loss: 0.0180\n",
      "Epoch: [78/200], Step: [1/8], Loss: 0.0117\n",
      "Epoch: [78/200], Step: [2/8], Loss: 0.0079\n",
      "Epoch: [78/200], Step: [3/8], Loss: 0.0053\n",
      "Epoch: [78/200], Step: [4/8], Loss: 0.0113\n",
      "Epoch: [78/200], Step: [5/8], Loss: 0.0057\n",
      "Epoch: [78/200], Step: [6/8], Loss: 0.0123\n",
      "Epoch: [78/200], Step: [7/8], Loss: 0.0069\n",
      "Epoch: [78/200], Step: [8/8], Loss: 0.0082\n",
      "Epoch: [79/200], Step: [1/8], Loss: 0.0080\n",
      "Epoch: [79/200], Step: [2/8], Loss: 0.0116\n",
      "Epoch: [79/200], Step: [3/8], Loss: 0.0083\n",
      "Epoch: [79/200], Step: [4/8], Loss: 0.0083\n",
      "Epoch: [79/200], Step: [5/8], Loss: 0.0095\n",
      "Epoch: [79/200], Step: [6/8], Loss: 0.0100\n",
      "Epoch: [79/200], Step: [7/8], Loss: 0.0088\n",
      "Epoch: [79/200], Step: [8/8], Loss: 0.0083\n",
      "Epoch: [80/200], Step: [1/8], Loss: 0.0078\n",
      "Epoch: [80/200], Step: [2/8], Loss: 0.0082\n",
      "Epoch: [80/200], Step: [3/8], Loss: 0.0063\n",
      "Epoch: [80/200], Step: [4/8], Loss: 0.0093\n",
      "Epoch: [80/200], Step: [5/8], Loss: 0.0082\n",
      "Epoch: [80/200], Step: [6/8], Loss: 0.0061\n",
      "Epoch: [80/200], Step: [7/8], Loss: 0.0107\n",
      "Epoch: [80/200], Step: [8/8], Loss: 0.0123\n",
      "Epoch: [81/200], Step: [1/8], Loss: 0.0050\n",
      "Epoch: [81/200], Step: [2/8], Loss: 0.0131\n",
      "Epoch: [81/200], Step: [3/8], Loss: 0.0090\n",
      "Epoch: [81/200], Step: [4/8], Loss: 0.0076\n",
      "Epoch: [81/200], Step: [5/8], Loss: 0.0078\n",
      "Epoch: [81/200], Step: [6/8], Loss: 0.0067\n",
      "Epoch: [81/200], Step: [7/8], Loss: 0.0087\n",
      "Epoch: [81/200], Step: [8/8], Loss: 0.0075\n",
      "Epoch: [82/200], Step: [1/8], Loss: 0.0059\n",
      "Epoch: [82/200], Step: [2/8], Loss: 0.0069\n",
      "Epoch: [82/200], Step: [3/8], Loss: 0.0069\n",
      "Epoch: [82/200], Step: [4/8], Loss: 0.0094\n",
      "Epoch: [82/200], Step: [5/8], Loss: 0.0038\n",
      "Epoch: [82/200], Step: [6/8], Loss: 0.0073\n",
      "Epoch: [82/200], Step: [7/8], Loss: 0.0084\n",
      "Epoch: [82/200], Step: [8/8], Loss: 0.0091\n",
      "Epoch: [83/200], Step: [1/8], Loss: 0.0073\n",
      "Epoch: [83/200], Step: [2/8], Loss: 0.0054\n",
      "Epoch: [83/200], Step: [3/8], Loss: 0.0094\n",
      "Epoch: [83/200], Step: [4/8], Loss: 0.0058\n",
      "Epoch: [83/200], Step: [5/8], Loss: 0.0089\n",
      "Epoch: [83/200], Step: [6/8], Loss: 0.0080\n",
      "Epoch: [83/200], Step: [7/8], Loss: 0.0066\n",
      "Epoch: [83/200], Step: [8/8], Loss: 0.0051\n",
      "Epoch: [84/200], Step: [1/8], Loss: 0.0056\n",
      "Epoch: [84/200], Step: [2/8], Loss: 0.0038\n",
      "Epoch: [84/200], Step: [3/8], Loss: 0.0061\n",
      "Epoch: [84/200], Step: [4/8], Loss: 0.0074\n",
      "Epoch: [84/200], Step: [5/8], Loss: 0.0142\n",
      "Epoch: [84/200], Step: [6/8], Loss: 0.0096\n",
      "Epoch: [84/200], Step: [7/8], Loss: 0.0072\n",
      "Epoch: [84/200], Step: [8/8], Loss: 0.0060\n",
      "Epoch: [85/200], Step: [1/8], Loss: 0.0086\n",
      "Epoch: [85/200], Step: [2/8], Loss: 0.0089\n",
      "Epoch: [85/200], Step: [3/8], Loss: 0.0196\n",
      "Epoch: [85/200], Step: [4/8], Loss: 0.0036\n",
      "Epoch: [85/200], Step: [5/8], Loss: 0.0077\n",
      "Epoch: [85/200], Step: [6/8], Loss: 0.0062\n",
      "Epoch: [85/200], Step: [7/8], Loss: 0.0072\n",
      "Epoch: [85/200], Step: [8/8], Loss: 0.0066\n",
      "Epoch: [86/200], Step: [1/8], Loss: 0.0072\n",
      "Epoch: [86/200], Step: [2/8], Loss: 0.0058\n",
      "Epoch: [86/200], Step: [3/8], Loss: 0.0311\n",
      "Epoch: [86/200], Step: [4/8], Loss: 0.0072\n",
      "Epoch: [86/200], Step: [5/8], Loss: 0.0058\n",
      "Epoch: [86/200], Step: [6/8], Loss: 0.0044\n",
      "Epoch: [86/200], Step: [7/8], Loss: 0.0231\n",
      "Epoch: [86/200], Step: [8/8], Loss: 0.0056\n",
      "Epoch: [87/200], Step: [1/8], Loss: 0.0194\n",
      "Epoch: [87/200], Step: [2/8], Loss: 0.0056\n",
      "Epoch: [87/200], Step: [3/8], Loss: 0.0056\n",
      "Epoch: [87/200], Step: [4/8], Loss: 0.0050\n",
      "Epoch: [87/200], Step: [5/8], Loss: 0.0051\n",
      "Epoch: [87/200], Step: [6/8], Loss: 0.0061\n",
      "Epoch: [87/200], Step: [7/8], Loss: 0.0244\n",
      "Epoch: [87/200], Step: [8/8], Loss: 0.0122\n",
      "Epoch: [88/200], Step: [1/8], Loss: 0.0099\n",
      "Epoch: [88/200], Step: [2/8], Loss: 0.0076\n",
      "Epoch: [88/200], Step: [3/8], Loss: 0.0061\n",
      "Epoch: [88/200], Step: [4/8], Loss: 0.0150\n",
      "Epoch: [88/200], Step: [5/8], Loss: 0.0201\n",
      "Epoch: [88/200], Step: [6/8], Loss: 0.0095\n",
      "Epoch: [88/200], Step: [7/8], Loss: 0.0050\n",
      "Epoch: [88/200], Step: [8/8], Loss: 0.0056\n",
      "Epoch: [89/200], Step: [1/8], Loss: 0.0065\n",
      "Epoch: [89/200], Step: [2/8], Loss: 0.0049\n",
      "Epoch: [89/200], Step: [3/8], Loss: 0.0420\n",
      "Epoch: [89/200], Step: [4/8], Loss: 0.0052\n",
      "Epoch: [89/200], Step: [5/8], Loss: 0.0080\n",
      "Epoch: [89/200], Step: [6/8], Loss: 0.0055\n",
      "Epoch: [89/200], Step: [7/8], Loss: 0.0039\n",
      "Epoch: [89/200], Step: [8/8], Loss: 0.0137\n",
      "Epoch: [90/200], Step: [1/8], Loss: 0.0064\n",
      "Epoch: [90/200], Step: [2/8], Loss: 0.0080\n",
      "Epoch: [90/200], Step: [3/8], Loss: 0.0058\n",
      "Epoch: [90/200], Step: [4/8], Loss: 0.0084\n",
      "Epoch: [90/200], Step: [5/8], Loss: 0.0127\n",
      "Epoch: [90/200], Step: [6/8], Loss: 0.0048\n",
      "Epoch: [90/200], Step: [7/8], Loss: 0.0057\n",
      "Epoch: [90/200], Step: [8/8], Loss: 0.0065\n",
      "Epoch: [91/200], Step: [1/8], Loss: 0.0104\n",
      "Epoch: [91/200], Step: [2/8], Loss: 0.0150\n",
      "Epoch: [91/200], Step: [3/8], Loss: 0.0046\n",
      "Epoch: [91/200], Step: [4/8], Loss: 0.0126\n",
      "Epoch: [91/200], Step: [5/8], Loss: 0.0047\n",
      "Epoch: [91/200], Step: [6/8], Loss: 0.0041\n",
      "Epoch: [91/200], Step: [7/8], Loss: 0.0096\n",
      "Epoch: [91/200], Step: [8/8], Loss: 0.0031\n",
      "Epoch: [92/200], Step: [1/8], Loss: 0.0110\n",
      "Epoch: [92/200], Step: [2/8], Loss: 0.0339\n",
      "Epoch: [92/200], Step: [3/8], Loss: 0.0044\n",
      "Epoch: [92/200], Step: [4/8], Loss: 0.0042\n",
      "Epoch: [92/200], Step: [5/8], Loss: 0.0074\n",
      "Epoch: [92/200], Step: [6/8], Loss: 0.0049\n",
      "Epoch: [92/200], Step: [7/8], Loss: 0.0505\n",
      "Epoch: [92/200], Step: [8/8], Loss: 0.0038\n",
      "Epoch: [93/200], Step: [1/8], Loss: 0.0062\n",
      "Epoch: [93/200], Step: [2/8], Loss: 0.0046\n",
      "Epoch: [93/200], Step: [3/8], Loss: 0.0059\n",
      "Epoch: [93/200], Step: [4/8], Loss: 0.0124\n",
      "Epoch: [93/200], Step: [5/8], Loss: 0.0189\n",
      "Epoch: [93/200], Step: [6/8], Loss: 0.0053\n",
      "Epoch: [93/200], Step: [7/8], Loss: 0.0062\n",
      "Epoch: [93/200], Step: [8/8], Loss: 0.0041\n",
      "Epoch: [94/200], Step: [1/8], Loss: 0.0054\n",
      "Epoch: [94/200], Step: [2/8], Loss: 0.0042\n",
      "Epoch: [94/200], Step: [3/8], Loss: 0.0054\n",
      "Epoch: [94/200], Step: [4/8], Loss: 0.0048\n",
      "Epoch: [94/200], Step: [5/8], Loss: 0.0125\n",
      "Epoch: [94/200], Step: [6/8], Loss: 0.0053\n",
      "Epoch: [94/200], Step: [7/8], Loss: 0.0036\n",
      "Epoch: [94/200], Step: [8/8], Loss: 0.0054\n",
      "Epoch: [95/200], Step: [1/8], Loss: 0.0049\n",
      "Epoch: [95/200], Step: [2/8], Loss: 0.0045\n",
      "Epoch: [95/200], Step: [3/8], Loss: 0.0041\n",
      "Epoch: [95/200], Step: [4/8], Loss: 0.0034\n",
      "Epoch: [95/200], Step: [5/8], Loss: 0.0054\n",
      "Epoch: [95/200], Step: [6/8], Loss: 0.0055\n",
      "Epoch: [95/200], Step: [7/8], Loss: 0.0075\n",
      "Epoch: [95/200], Step: [8/8], Loss: 0.0052\n",
      "Epoch: [96/200], Step: [1/8], Loss: 0.0036\n",
      "Epoch: [96/200], Step: [2/8], Loss: 0.0038\n",
      "Epoch: [96/200], Step: [3/8], Loss: 0.0042\n",
      "Epoch: [96/200], Step: [4/8], Loss: 0.0062\n",
      "Epoch: [96/200], Step: [5/8], Loss: 0.0046\n",
      "Epoch: [96/200], Step: [6/8], Loss: 0.0047\n",
      "Epoch: [96/200], Step: [7/8], Loss: 0.0063\n",
      "Epoch: [96/200], Step: [8/8], Loss: 0.0059\n",
      "Epoch: [97/200], Step: [1/8], Loss: 0.0041\n",
      "Epoch: [97/200], Step: [2/8], Loss: 0.0046\n",
      "Epoch: [97/200], Step: [3/8], Loss: 0.0035\n",
      "Epoch: [97/200], Step: [4/8], Loss: 0.0039\n",
      "Epoch: [97/200], Step: [5/8], Loss: 0.0042\n",
      "Epoch: [97/200], Step: [6/8], Loss: 0.0043\n",
      "Epoch: [97/200], Step: [7/8], Loss: 0.0061\n",
      "Epoch: [97/200], Step: [8/8], Loss: 0.0043\n",
      "Epoch: [98/200], Step: [1/8], Loss: 0.0043\n",
      "Epoch: [98/200], Step: [2/8], Loss: 0.0031\n",
      "Epoch: [98/200], Step: [3/8], Loss: 0.0044\n",
      "Epoch: [98/200], Step: [4/8], Loss: 0.0049\n",
      "Epoch: [98/200], Step: [5/8], Loss: 0.0038\n",
      "Epoch: [98/200], Step: [6/8], Loss: 0.0037\n",
      "Epoch: [98/200], Step: [7/8], Loss: 0.0048\n",
      "Epoch: [98/200], Step: [8/8], Loss: 0.0038\n",
      "Epoch: [99/200], Step: [1/8], Loss: 0.0039\n",
      "Epoch: [99/200], Step: [2/8], Loss: 0.0039\n",
      "Epoch: [99/200], Step: [3/8], Loss: 0.0033\n",
      "Epoch: [99/200], Step: [4/8], Loss: 0.0042\n",
      "Epoch: [99/200], Step: [5/8], Loss: 0.0038\n",
      "Epoch: [99/200], Step: [6/8], Loss: 0.0054\n",
      "Epoch: [99/200], Step: [7/8], Loss: 0.0034\n",
      "Epoch: [99/200], Step: [8/8], Loss: 0.0067\n",
      "Epoch: [100/200], Step: [1/8], Loss: 0.0044\n",
      "Epoch: [100/200], Step: [2/8], Loss: 0.0042\n",
      "Epoch: [100/200], Step: [3/8], Loss: 0.0037\n",
      "Epoch: [100/200], Step: [4/8], Loss: 0.0036\n",
      "Epoch: [100/200], Step: [5/8], Loss: 0.0039\n",
      "Epoch: [100/200], Step: [6/8], Loss: 0.0032\n",
      "Epoch: [100/200], Step: [7/8], Loss: 0.0041\n",
      "Epoch: [100/200], Step: [8/8], Loss: 0.0034\n",
      "Epoch: [101/200], Step: [1/8], Loss: 0.0025\n",
      "Epoch: [101/200], Step: [2/8], Loss: 0.0028\n",
      "Epoch: [101/200], Step: [3/8], Loss: 0.0044\n",
      "Epoch: [101/200], Step: [4/8], Loss: 0.0037\n",
      "Epoch: [101/200], Step: [5/8], Loss: 0.0052\n",
      "Epoch: [101/200], Step: [6/8], Loss: 0.0033\n",
      "Epoch: [101/200], Step: [7/8], Loss: 0.0042\n",
      "Epoch: [101/200], Step: [8/8], Loss: 0.0046\n",
      "Epoch: [102/200], Step: [1/8], Loss: 0.0045\n",
      "Epoch: [102/200], Step: [2/8], Loss: 0.0037\n",
      "Epoch: [102/200], Step: [3/8], Loss: 0.0031\n",
      "Epoch: [102/200], Step: [4/8], Loss: 0.0037\n",
      "Epoch: [102/200], Step: [5/8], Loss: 0.0035\n",
      "Epoch: [102/200], Step: [6/8], Loss: 0.0036\n",
      "Epoch: [102/200], Step: [7/8], Loss: 0.0035\n",
      "Epoch: [102/200], Step: [8/8], Loss: 0.0027\n",
      "Epoch: [103/200], Step: [1/8], Loss: 0.0033\n",
      "Epoch: [103/200], Step: [2/8], Loss: 0.0041\n",
      "Epoch: [103/200], Step: [3/8], Loss: 0.0036\n",
      "Epoch: [103/200], Step: [4/8], Loss: 0.0035\n",
      "Epoch: [103/200], Step: [5/8], Loss: 0.0040\n",
      "Epoch: [103/200], Step: [6/8], Loss: 0.0027\n",
      "Epoch: [103/200], Step: [7/8], Loss: 0.0033\n",
      "Epoch: [103/200], Step: [8/8], Loss: 0.0045\n",
      "Epoch: [104/200], Step: [1/8], Loss: 0.0031\n",
      "Epoch: [104/200], Step: [2/8], Loss: 0.0034\n",
      "Epoch: [104/200], Step: [3/8], Loss: 0.0031\n",
      "Epoch: [104/200], Step: [4/8], Loss: 0.0035\n",
      "Epoch: [104/200], Step: [5/8], Loss: 0.0044\n",
      "Epoch: [104/200], Step: [6/8], Loss: 0.0031\n",
      "Epoch: [104/200], Step: [7/8], Loss: 0.0040\n",
      "Epoch: [104/200], Step: [8/8], Loss: 0.0031\n",
      "Epoch: [105/200], Step: [1/8], Loss: 0.0034\n",
      "Epoch: [105/200], Step: [2/8], Loss: 0.0034\n",
      "Epoch: [105/200], Step: [3/8], Loss: 0.0029\n",
      "Epoch: [105/200], Step: [4/8], Loss: 0.0034\n",
      "Epoch: [105/200], Step: [5/8], Loss: 0.0031\n",
      "Epoch: [105/200], Step: [6/8], Loss: 0.0036\n",
      "Epoch: [105/200], Step: [7/8], Loss: 0.0039\n",
      "Epoch: [105/200], Step: [8/8], Loss: 0.0032\n",
      "Epoch: [106/200], Step: [1/8], Loss: 0.0032\n",
      "Epoch: [106/200], Step: [2/8], Loss: 0.0030\n",
      "Epoch: [106/200], Step: [3/8], Loss: 0.0024\n",
      "Epoch: [106/200], Step: [4/8], Loss: 0.0034\n",
      "Epoch: [106/200], Step: [5/8], Loss: 0.0043\n",
      "Epoch: [106/200], Step: [6/8], Loss: 0.0035\n",
      "Epoch: [106/200], Step: [7/8], Loss: 0.0031\n",
      "Epoch: [106/200], Step: [8/8], Loss: 0.0033\n",
      "Epoch: [107/200], Step: [1/8], Loss: 0.0034\n",
      "Epoch: [107/200], Step: [2/8], Loss: 0.0031\n",
      "Epoch: [107/200], Step: [3/8], Loss: 0.0035\n",
      "Epoch: [107/200], Step: [4/8], Loss: 0.0027\n",
      "Epoch: [107/200], Step: [5/8], Loss: 0.0028\n",
      "Epoch: [107/200], Step: [6/8], Loss: 0.0034\n",
      "Epoch: [107/200], Step: [7/8], Loss: 0.0037\n",
      "Epoch: [107/200], Step: [8/8], Loss: 0.0036\n",
      "Epoch: [108/200], Step: [1/8], Loss: 0.0028\n",
      "Epoch: [108/200], Step: [2/8], Loss: 0.0033\n",
      "Epoch: [108/200], Step: [3/8], Loss: 0.0024\n",
      "Epoch: [108/200], Step: [4/8], Loss: 0.0034\n",
      "Epoch: [108/200], Step: [5/8], Loss: 0.0035\n",
      "Epoch: [108/200], Step: [6/8], Loss: 0.0038\n",
      "Epoch: [108/200], Step: [7/8], Loss: 0.0031\n",
      "Epoch: [108/200], Step: [8/8], Loss: 0.0036\n",
      "Epoch: [109/200], Step: [1/8], Loss: 0.0029\n",
      "Epoch: [109/200], Step: [2/8], Loss: 0.0030\n",
      "Epoch: [109/200], Step: [3/8], Loss: 0.0036\n",
      "Epoch: [109/200], Step: [4/8], Loss: 0.0029\n",
      "Epoch: [109/200], Step: [5/8], Loss: 0.0036\n",
      "Epoch: [109/200], Step: [6/8], Loss: 0.0027\n",
      "Epoch: [109/200], Step: [7/8], Loss: 0.0025\n",
      "Epoch: [109/200], Step: [8/8], Loss: 0.0040\n",
      "Epoch: [110/200], Step: [1/8], Loss: 0.0028\n",
      "Epoch: [110/200], Step: [2/8], Loss: 0.0025\n",
      "Epoch: [110/200], Step: [3/8], Loss: 0.0028\n",
      "Epoch: [110/200], Step: [4/8], Loss: 0.0026\n",
      "Epoch: [110/200], Step: [5/8], Loss: 0.0042\n",
      "Epoch: [110/200], Step: [6/8], Loss: 0.0028\n",
      "Epoch: [110/200], Step: [7/8], Loss: 0.0035\n",
      "Epoch: [110/200], Step: [8/8], Loss: 0.0029\n",
      "Epoch: [111/200], Step: [1/8], Loss: 0.0026\n",
      "Epoch: [111/200], Step: [2/8], Loss: 0.0029\n",
      "Epoch: [111/200], Step: [3/8], Loss: 0.0023\n",
      "Epoch: [111/200], Step: [4/8], Loss: 0.0024\n",
      "Epoch: [111/200], Step: [5/8], Loss: 0.0029\n",
      "Epoch: [111/200], Step: [6/8], Loss: 0.0039\n",
      "Epoch: [111/200], Step: [7/8], Loss: 0.0031\n",
      "Epoch: [111/200], Step: [8/8], Loss: 0.0038\n",
      "Epoch: [112/200], Step: [1/8], Loss: 0.0028\n",
      "Epoch: [112/200], Step: [2/8], Loss: 0.0034\n",
      "Epoch: [112/200], Step: [3/8], Loss: 0.0024\n",
      "Epoch: [112/200], Step: [4/8], Loss: 0.0026\n",
      "Epoch: [112/200], Step: [5/8], Loss: 0.0037\n",
      "Epoch: [112/200], Step: [6/8], Loss: 0.0029\n",
      "Epoch: [112/200], Step: [7/8], Loss: 0.0024\n",
      "Epoch: [112/200], Step: [8/8], Loss: 0.0031\n",
      "Epoch: [113/200], Step: [1/8], Loss: 0.0029\n",
      "Epoch: [113/200], Step: [2/8], Loss: 0.0023\n",
      "Epoch: [113/200], Step: [3/8], Loss: 0.0031\n",
      "Epoch: [113/200], Step: [4/8], Loss: 0.0025\n",
      "Epoch: [113/200], Step: [5/8], Loss: 0.0028\n",
      "Epoch: [113/200], Step: [6/8], Loss: 0.0030\n",
      "Epoch: [113/200], Step: [7/8], Loss: 0.0030\n",
      "Epoch: [113/200], Step: [8/8], Loss: 0.0030\n",
      "Epoch: [114/200], Step: [1/8], Loss: 0.0027\n",
      "Epoch: [114/200], Step: [2/8], Loss: 0.0026\n",
      "Epoch: [114/200], Step: [3/8], Loss: 0.0028\n",
      "Epoch: [114/200], Step: [4/8], Loss: 0.0021\n",
      "Epoch: [114/200], Step: [5/8], Loss: 0.0024\n",
      "Epoch: [114/200], Step: [6/8], Loss: 0.0037\n",
      "Epoch: [114/200], Step: [7/8], Loss: 0.0030\n",
      "Epoch: [114/200], Step: [8/8], Loss: 0.0025\n",
      "Epoch: [115/200], Step: [1/8], Loss: 0.0026\n",
      "Epoch: [115/200], Step: [2/8], Loss: 0.0021\n",
      "Epoch: [115/200], Step: [3/8], Loss: 0.0024\n",
      "Epoch: [115/200], Step: [4/8], Loss: 0.0038\n",
      "Epoch: [115/200], Step: [5/8], Loss: 0.0027\n",
      "Epoch: [115/200], Step: [6/8], Loss: 0.0030\n",
      "Epoch: [115/200], Step: [7/8], Loss: 0.0018\n",
      "Epoch: [115/200], Step: [8/8], Loss: 0.0042\n",
      "Epoch: [116/200], Step: [1/8], Loss: 0.0027\n",
      "Epoch: [116/200], Step: [2/8], Loss: 0.0033\n",
      "Epoch: [116/200], Step: [3/8], Loss: 0.0026\n",
      "Epoch: [116/200], Step: [4/8], Loss: 0.0018\n",
      "Epoch: [116/200], Step: [5/8], Loss: 0.0028\n",
      "Epoch: [116/200], Step: [6/8], Loss: 0.0027\n",
      "Epoch: [116/200], Step: [7/8], Loss: 0.0035\n",
      "Epoch: [116/200], Step: [8/8], Loss: 0.0026\n",
      "Epoch: [117/200], Step: [1/8], Loss: 0.0021\n",
      "Epoch: [117/200], Step: [2/8], Loss: 0.0030\n",
      "Epoch: [117/200], Step: [3/8], Loss: 0.0026\n",
      "Epoch: [117/200], Step: [4/8], Loss: 0.0021\n",
      "Epoch: [117/200], Step: [5/8], Loss: 0.0034\n",
      "Epoch: [117/200], Step: [6/8], Loss: 0.0028\n",
      "Epoch: [117/200], Step: [7/8], Loss: 0.0025\n",
      "Epoch: [117/200], Step: [8/8], Loss: 0.0025\n",
      "Epoch: [118/200], Step: [1/8], Loss: 0.0031\n",
      "Epoch: [118/200], Step: [2/8], Loss: 0.0024\n",
      "Epoch: [118/200], Step: [3/8], Loss: 0.0031\n",
      "Epoch: [118/200], Step: [4/8], Loss: 0.0026\n",
      "Epoch: [118/200], Step: [5/8], Loss: 0.0027\n",
      "Epoch: [118/200], Step: [6/8], Loss: 0.0024\n",
      "Epoch: [118/200], Step: [7/8], Loss: 0.0023\n",
      "Epoch: [118/200], Step: [8/8], Loss: 0.0023\n",
      "Epoch: [119/200], Step: [1/8], Loss: 0.0025\n",
      "Epoch: [119/200], Step: [2/8], Loss: 0.0025\n",
      "Epoch: [119/200], Step: [3/8], Loss: 0.0028\n",
      "Epoch: [119/200], Step: [4/8], Loss: 0.0023\n",
      "Epoch: [119/200], Step: [5/8], Loss: 0.0022\n",
      "Epoch: [119/200], Step: [6/8], Loss: 0.0022\n",
      "Epoch: [119/200], Step: [7/8], Loss: 0.0020\n",
      "Epoch: [119/200], Step: [8/8], Loss: 0.0039\n",
      "Epoch: [120/200], Step: [1/8], Loss: 0.0022\n",
      "Epoch: [120/200], Step: [2/8], Loss: 0.0023\n",
      "Epoch: [120/200], Step: [3/8], Loss: 0.0025\n",
      "Epoch: [120/200], Step: [4/8], Loss: 0.0023\n",
      "Epoch: [120/200], Step: [5/8], Loss: 0.0026\n",
      "Epoch: [120/200], Step: [6/8], Loss: 0.0025\n",
      "Epoch: [120/200], Step: [7/8], Loss: 0.0024\n",
      "Epoch: [120/200], Step: [8/8], Loss: 0.0021\n",
      "Epoch: [121/200], Step: [1/8], Loss: 0.0027\n",
      "Epoch: [121/200], Step: [2/8], Loss: 0.0019\n",
      "Epoch: [121/200], Step: [3/8], Loss: 0.0027\n",
      "Epoch: [121/200], Step: [4/8], Loss: 0.0024\n",
      "Epoch: [121/200], Step: [5/8], Loss: 0.0019\n",
      "Epoch: [121/200], Step: [6/8], Loss: 0.0026\n",
      "Epoch: [121/200], Step: [7/8], Loss: 0.0021\n",
      "Epoch: [121/200], Step: [8/8], Loss: 0.0035\n",
      "Epoch: [122/200], Step: [1/8], Loss: 0.0024\n",
      "Epoch: [122/200], Step: [2/8], Loss: 0.0024\n",
      "Epoch: [122/200], Step: [3/8], Loss: 0.0017\n",
      "Epoch: [122/200], Step: [4/8], Loss: 0.0023\n",
      "Epoch: [122/200], Step: [5/8], Loss: 0.0021\n",
      "Epoch: [122/200], Step: [6/8], Loss: 0.0031\n",
      "Epoch: [122/200], Step: [7/8], Loss: 0.0025\n",
      "Epoch: [122/200], Step: [8/8], Loss: 0.0016\n",
      "Epoch: [123/200], Step: [1/8], Loss: 0.0022\n",
      "Epoch: [123/200], Step: [2/8], Loss: 0.0026\n",
      "Epoch: [123/200], Step: [3/8], Loss: 0.0022\n",
      "Epoch: [123/200], Step: [4/8], Loss: 0.0020\n",
      "Epoch: [123/200], Step: [5/8], Loss: 0.0021\n",
      "Epoch: [123/200], Step: [6/8], Loss: 0.0024\n",
      "Epoch: [123/200], Step: [7/8], Loss: 0.0023\n",
      "Epoch: [123/200], Step: [8/8], Loss: 0.0025\n",
      "Epoch: [124/200], Step: [1/8], Loss: 0.0019\n",
      "Epoch: [124/200], Step: [2/8], Loss: 0.0027\n",
      "Epoch: [124/200], Step: [3/8], Loss: 0.0023\n",
      "Epoch: [124/200], Step: [4/8], Loss: 0.0020\n",
      "Epoch: [124/200], Step: [5/8], Loss: 0.0024\n",
      "Epoch: [124/200], Step: [6/8], Loss: 0.0019\n",
      "Epoch: [124/200], Step: [7/8], Loss: 0.0019\n",
      "Epoch: [124/200], Step: [8/8], Loss: 0.0031\n",
      "Epoch: [125/200], Step: [1/8], Loss: 0.0021\n",
      "Epoch: [125/200], Step: [2/8], Loss: 0.0022\n",
      "Epoch: [125/200], Step: [3/8], Loss: 0.0020\n",
      "Epoch: [125/200], Step: [4/8], Loss: 0.0019\n",
      "Epoch: [125/200], Step: [5/8], Loss: 0.0023\n",
      "Epoch: [125/200], Step: [6/8], Loss: 0.0024\n",
      "Epoch: [125/200], Step: [7/8], Loss: 0.0023\n",
      "Epoch: [125/200], Step: [8/8], Loss: 0.0025\n",
      "Epoch: [126/200], Step: [1/8], Loss: 0.0019\n",
      "Epoch: [126/200], Step: [2/8], Loss: 0.0018\n",
      "Epoch: [126/200], Step: [3/8], Loss: 0.0026\n",
      "Epoch: [126/200], Step: [4/8], Loss: 0.0023\n",
      "Epoch: [126/200], Step: [5/8], Loss: 0.0017\n",
      "Epoch: [126/200], Step: [6/8], Loss: 0.0020\n",
      "Epoch: [126/200], Step: [7/8], Loss: 0.0021\n",
      "Epoch: [126/200], Step: [8/8], Loss: 0.0031\n",
      "Epoch: [127/200], Step: [1/8], Loss: 0.0026\n",
      "Epoch: [127/200], Step: [2/8], Loss: 0.0015\n",
      "Epoch: [127/200], Step: [3/8], Loss: 0.0026\n",
      "Epoch: [127/200], Step: [4/8], Loss: 0.0016\n",
      "Epoch: [127/200], Step: [5/8], Loss: 0.0023\n",
      "Epoch: [127/200], Step: [6/8], Loss: 0.0020\n",
      "Epoch: [127/200], Step: [7/8], Loss: 0.0021\n",
      "Epoch: [127/200], Step: [8/8], Loss: 0.0021\n",
      "Epoch: [128/200], Step: [1/8], Loss: 0.0014\n",
      "Epoch: [128/200], Step: [2/8], Loss: 0.0023\n",
      "Epoch: [128/200], Step: [3/8], Loss: 0.0019\n",
      "Epoch: [128/200], Step: [4/8], Loss: 0.0017\n",
      "Epoch: [128/200], Step: [5/8], Loss: 0.0024\n",
      "Epoch: [128/200], Step: [6/8], Loss: 0.0025\n",
      "Epoch: [128/200], Step: [7/8], Loss: 0.0021\n",
      "Epoch: [128/200], Step: [8/8], Loss: 0.0020\n",
      "Epoch: [129/200], Step: [1/8], Loss: 0.0022\n",
      "Epoch: [129/200], Step: [2/8], Loss: 0.0017\n",
      "Epoch: [129/200], Step: [3/8], Loss: 0.0021\n",
      "Epoch: [129/200], Step: [4/8], Loss: 0.0015\n",
      "Epoch: [129/200], Step: [5/8], Loss: 0.0021\n",
      "Epoch: [129/200], Step: [6/8], Loss: 0.0021\n",
      "Epoch: [129/200], Step: [7/8], Loss: 0.0022\n",
      "Epoch: [129/200], Step: [8/8], Loss: 0.0023\n",
      "Epoch: [130/200], Step: [1/8], Loss: 0.0017\n",
      "Epoch: [130/200], Step: [2/8], Loss: 0.0020\n",
      "Epoch: [130/200], Step: [3/8], Loss: 0.0018\n",
      "Epoch: [130/200], Step: [4/8], Loss: 0.0019\n",
      "Epoch: [130/200], Step: [5/8], Loss: 0.0027\n",
      "Epoch: [130/200], Step: [6/8], Loss: 0.0018\n",
      "Epoch: [130/200], Step: [7/8], Loss: 0.0022\n",
      "Epoch: [130/200], Step: [8/8], Loss: 0.0024\n",
      "Epoch: [131/200], Step: [1/8], Loss: 0.0019\n",
      "Epoch: [131/200], Step: [2/8], Loss: 0.0018\n",
      "Epoch: [131/200], Step: [3/8], Loss: 0.0022\n",
      "Epoch: [131/200], Step: [4/8], Loss: 0.0019\n",
      "Epoch: [131/200], Step: [5/8], Loss: 0.0025\n",
      "Epoch: [131/200], Step: [6/8], Loss: 0.0014\n",
      "Epoch: [131/200], Step: [7/8], Loss: 0.0020\n",
      "Epoch: [131/200], Step: [8/8], Loss: 0.0028\n",
      "Epoch: [132/200], Step: [1/8], Loss: 0.0017\n",
      "Epoch: [132/200], Step: [2/8], Loss: 0.0017\n",
      "Epoch: [132/200], Step: [3/8], Loss: 0.0020\n",
      "Epoch: [132/200], Step: [4/8], Loss: 0.0021\n",
      "Epoch: [132/200], Step: [5/8], Loss: 0.0019\n",
      "Epoch: [132/200], Step: [6/8], Loss: 0.0022\n",
      "Epoch: [132/200], Step: [7/8], Loss: 0.0020\n",
      "Epoch: [132/200], Step: [8/8], Loss: 0.0021\n",
      "Epoch: [133/200], Step: [1/8], Loss: 0.0022\n",
      "Epoch: [133/200], Step: [2/8], Loss: 0.0024\n",
      "Epoch: [133/200], Step: [3/8], Loss: 0.0016\n",
      "Epoch: [133/200], Step: [4/8], Loss: 0.0013\n",
      "Epoch: [133/200], Step: [5/8], Loss: 0.0017\n",
      "Epoch: [133/200], Step: [6/8], Loss: 0.0022\n",
      "Epoch: [133/200], Step: [7/8], Loss: 0.0018\n",
      "Epoch: [133/200], Step: [8/8], Loss: 0.0023\n",
      "Epoch: [134/200], Step: [1/8], Loss: 0.0018\n",
      "Epoch: [134/200], Step: [2/8], Loss: 0.0018\n",
      "Epoch: [134/200], Step: [3/8], Loss: 0.0019\n",
      "Epoch: [134/200], Step: [4/8], Loss: 0.0022\n",
      "Epoch: [134/200], Step: [5/8], Loss: 0.0019\n",
      "Epoch: [134/200], Step: [6/8], Loss: 0.0021\n",
      "Epoch: [134/200], Step: [7/8], Loss: 0.0015\n",
      "Epoch: [134/200], Step: [8/8], Loss: 0.0018\n",
      "Epoch: [135/200], Step: [1/8], Loss: 0.0016\n",
      "Epoch: [135/200], Step: [2/8], Loss: 0.0018\n",
      "Epoch: [135/200], Step: [3/8], Loss: 0.0018\n",
      "Epoch: [135/200], Step: [4/8], Loss: 0.0022\n",
      "Epoch: [135/200], Step: [5/8], Loss: 0.0020\n",
      "Epoch: [135/200], Step: [6/8], Loss: 0.0016\n",
      "Epoch: [135/200], Step: [7/8], Loss: 0.0016\n",
      "Epoch: [135/200], Step: [8/8], Loss: 0.0019\n",
      "Epoch: [136/200], Step: [1/8], Loss: 0.0015\n",
      "Epoch: [136/200], Step: [2/8], Loss: 0.0020\n",
      "Epoch: [136/200], Step: [3/8], Loss: 0.0019\n",
      "Epoch: [136/200], Step: [4/8], Loss: 0.0021\n",
      "Epoch: [136/200], Step: [5/8], Loss: 0.0015\n",
      "Epoch: [136/200], Step: [6/8], Loss: 0.0017\n",
      "Epoch: [136/200], Step: [7/8], Loss: 0.0018\n",
      "Epoch: [136/200], Step: [8/8], Loss: 0.0016\n",
      "Epoch: [137/200], Step: [1/8], Loss: 0.0014\n",
      "Epoch: [137/200], Step: [2/8], Loss: 0.0018\n",
      "Epoch: [137/200], Step: [3/8], Loss: 0.0020\n",
      "Epoch: [137/200], Step: [4/8], Loss: 0.0010\n",
      "Epoch: [137/200], Step: [5/8], Loss: 0.0020\n",
      "Epoch: [137/200], Step: [6/8], Loss: 0.0019\n",
      "Epoch: [137/200], Step: [7/8], Loss: 0.0019\n",
      "Epoch: [137/200], Step: [8/8], Loss: 0.0022\n",
      "Epoch: [138/200], Step: [1/8], Loss: 0.0017\n",
      "Epoch: [138/200], Step: [2/8], Loss: 0.0016\n",
      "Epoch: [138/200], Step: [3/8], Loss: 0.0016\n",
      "Epoch: [138/200], Step: [4/8], Loss: 0.0016\n",
      "Epoch: [138/200], Step: [5/8], Loss: 0.0020\n",
      "Epoch: [138/200], Step: [6/8], Loss: 0.0016\n",
      "Epoch: [138/200], Step: [7/8], Loss: 0.0020\n",
      "Epoch: [138/200], Step: [8/8], Loss: 0.0015\n",
      "Epoch: [139/200], Step: [1/8], Loss: 0.0017\n",
      "Epoch: [139/200], Step: [2/8], Loss: 0.0015\n",
      "Epoch: [139/200], Step: [3/8], Loss: 0.0020\n",
      "Epoch: [139/200], Step: [4/8], Loss: 0.0016\n",
      "Epoch: [139/200], Step: [5/8], Loss: 0.0016\n",
      "Epoch: [139/200], Step: [6/8], Loss: 0.0016\n",
      "Epoch: [139/200], Step: [7/8], Loss: 0.0017\n",
      "Epoch: [139/200], Step: [8/8], Loss: 0.0020\n",
      "Epoch: [140/200], Step: [1/8], Loss: 0.0017\n",
      "Epoch: [140/200], Step: [2/8], Loss: 0.0019\n",
      "Epoch: [140/200], Step: [3/8], Loss: 0.0018\n",
      "Epoch: [140/200], Step: [4/8], Loss: 0.0019\n",
      "Epoch: [140/200], Step: [5/8], Loss: 0.0014\n",
      "Epoch: [140/200], Step: [6/8], Loss: 0.0017\n",
      "Epoch: [140/200], Step: [7/8], Loss: 0.0013\n",
      "Epoch: [140/200], Step: [8/8], Loss: 0.0017\n",
      "Epoch: [141/200], Step: [1/8], Loss: 0.0015\n",
      "Epoch: [141/200], Step: [2/8], Loss: 0.0013\n",
      "Epoch: [141/200], Step: [3/8], Loss: 0.0019\n",
      "Epoch: [141/200], Step: [4/8], Loss: 0.0013\n",
      "Epoch: [141/200], Step: [5/8], Loss: 0.0016\n",
      "Epoch: [141/200], Step: [6/8], Loss: 0.0018\n",
      "Epoch: [141/200], Step: [7/8], Loss: 0.0020\n",
      "Epoch: [141/200], Step: [8/8], Loss: 0.0014\n",
      "Epoch: [142/200], Step: [1/8], Loss: 0.0016\n",
      "Epoch: [142/200], Step: [2/8], Loss: 0.0016\n",
      "Epoch: [142/200], Step: [3/8], Loss: 0.0013\n",
      "Epoch: [142/200], Step: [4/8], Loss: 0.0017\n",
      "Epoch: [142/200], Step: [5/8], Loss: 0.0015\n",
      "Epoch: [142/200], Step: [6/8], Loss: 0.0020\n",
      "Epoch: [142/200], Step: [7/8], Loss: 0.0019\n",
      "Epoch: [142/200], Step: [8/8], Loss: 0.0011\n",
      "Epoch: [143/200], Step: [1/8], Loss: 0.0017\n",
      "Epoch: [143/200], Step: [2/8], Loss: 0.0013\n",
      "Epoch: [143/200], Step: [3/8], Loss: 0.0021\n",
      "Epoch: [143/200], Step: [4/8], Loss: 0.0014\n",
      "Epoch: [143/200], Step: [5/8], Loss: 0.0013\n",
      "Epoch: [143/200], Step: [6/8], Loss: 0.0015\n",
      "Epoch: [143/200], Step: [7/8], Loss: 0.0015\n",
      "Epoch: [143/200], Step: [8/8], Loss: 0.0021\n",
      "Epoch: [144/200], Step: [1/8], Loss: 0.0011\n",
      "Epoch: [144/200], Step: [2/8], Loss: 0.0018\n",
      "Epoch: [144/200], Step: [3/8], Loss: 0.0020\n",
      "Epoch: [144/200], Step: [4/8], Loss: 0.0015\n",
      "Epoch: [144/200], Step: [5/8], Loss: 0.0016\n",
      "Epoch: [144/200], Step: [6/8], Loss: 0.0017\n",
      "Epoch: [144/200], Step: [7/8], Loss: 0.0013\n",
      "Epoch: [144/200], Step: [8/8], Loss: 0.0013\n",
      "Epoch: [145/200], Step: [1/8], Loss: 0.0014\n",
      "Epoch: [145/200], Step: [2/8], Loss: 0.0016\n",
      "Epoch: [145/200], Step: [3/8], Loss: 0.0016\n",
      "Epoch: [145/200], Step: [4/8], Loss: 0.0014\n",
      "Epoch: [145/200], Step: [5/8], Loss: 0.0013\n",
      "Epoch: [145/200], Step: [6/8], Loss: 0.0018\n",
      "Epoch: [145/200], Step: [7/8], Loss: 0.0015\n",
      "Epoch: [145/200], Step: [8/8], Loss: 0.0014\n",
      "Epoch: [146/200], Step: [1/8], Loss: 0.0016\n",
      "Epoch: [146/200], Step: [2/8], Loss: 0.0013\n",
      "Epoch: [146/200], Step: [3/8], Loss: 0.0016\n",
      "Epoch: [146/200], Step: [4/8], Loss: 0.0014\n",
      "Epoch: [146/200], Step: [5/8], Loss: 0.0014\n",
      "Epoch: [146/200], Step: [6/8], Loss: 0.0015\n",
      "Epoch: [146/200], Step: [7/8], Loss: 0.0019\n",
      "Epoch: [146/200], Step: [8/8], Loss: 0.0009\n",
      "Epoch: [147/200], Step: [1/8], Loss: 0.0012\n",
      "Epoch: [147/200], Step: [2/8], Loss: 0.0018\n",
      "Epoch: [147/200], Step: [3/8], Loss: 0.0015\n",
      "Epoch: [147/200], Step: [4/8], Loss: 0.0015\n",
      "Epoch: [147/200], Step: [5/8], Loss: 0.0011\n",
      "Epoch: [147/200], Step: [6/8], Loss: 0.0016\n",
      "Epoch: [147/200], Step: [7/8], Loss: 0.0014\n",
      "Epoch: [147/200], Step: [8/8], Loss: 0.0018\n",
      "Epoch: [148/200], Step: [1/8], Loss: 0.0015\n",
      "Epoch: [148/200], Step: [2/8], Loss: 0.0018\n",
      "Epoch: [148/200], Step: [3/8], Loss: 0.0014\n",
      "Epoch: [148/200], Step: [4/8], Loss: 0.0019\n",
      "Epoch: [148/200], Step: [5/8], Loss: 0.0012\n",
      "Epoch: [148/200], Step: [6/8], Loss: 0.0014\n",
      "Epoch: [148/200], Step: [7/8], Loss: 0.0012\n",
      "Epoch: [148/200], Step: [8/8], Loss: 0.0013\n",
      "Epoch: [149/200], Step: [1/8], Loss: 0.0013\n",
      "Epoch: [149/200], Step: [2/8], Loss: 0.0013\n",
      "Epoch: [149/200], Step: [3/8], Loss: 0.0015\n",
      "Epoch: [149/200], Step: [4/8], Loss: 0.0018\n",
      "Epoch: [149/200], Step: [5/8], Loss: 0.0012\n",
      "Epoch: [149/200], Step: [6/8], Loss: 0.0012\n",
      "Epoch: [149/200], Step: [7/8], Loss: 0.0018\n",
      "Epoch: [149/200], Step: [8/8], Loss: 0.0013\n",
      "Epoch: [150/200], Step: [1/8], Loss: 0.0015\n",
      "Epoch: [150/200], Step: [2/8], Loss: 0.0013\n",
      "Epoch: [150/200], Step: [3/8], Loss: 0.0019\n",
      "Epoch: [150/200], Step: [4/8], Loss: 0.0011\n",
      "Epoch: [150/200], Step: [5/8], Loss: 0.0014\n",
      "Epoch: [150/200], Step: [6/8], Loss: 0.0014\n",
      "Epoch: [150/200], Step: [7/8], Loss: 0.0017\n",
      "Epoch: [150/200], Step: [8/8], Loss: 0.0008\n",
      "Epoch: [151/200], Step: [1/8], Loss: 0.0012\n",
      "Epoch: [151/200], Step: [2/8], Loss: 0.0015\n",
      "Epoch: [151/200], Step: [3/8], Loss: 0.0012\n",
      "Epoch: [151/200], Step: [4/8], Loss: 0.0013\n",
      "Epoch: [151/200], Step: [5/8], Loss: 0.0017\n",
      "Epoch: [151/200], Step: [6/8], Loss: 0.0012\n",
      "Epoch: [151/200], Step: [7/8], Loss: 0.0015\n",
      "Epoch: [151/200], Step: [8/8], Loss: 0.0015\n",
      "Epoch: [152/200], Step: [1/8], Loss: 0.0011\n",
      "Epoch: [152/200], Step: [2/8], Loss: 0.0014\n",
      "Epoch: [152/200], Step: [3/8], Loss: 0.0014\n",
      "Epoch: [152/200], Step: [4/8], Loss: 0.0017\n",
      "Epoch: [152/200], Step: [5/8], Loss: 0.0013\n",
      "Epoch: [152/200], Step: [6/8], Loss: 0.0014\n",
      "Epoch: [152/200], Step: [7/8], Loss: 0.0013\n",
      "Epoch: [152/200], Step: [8/8], Loss: 0.0013\n",
      "Epoch: [153/200], Step: [1/8], Loss: 0.0010\n",
      "Epoch: [153/200], Step: [2/8], Loss: 0.0014\n",
      "Epoch: [153/200], Step: [3/8], Loss: 0.0011\n",
      "Epoch: [153/200], Step: [4/8], Loss: 0.0013\n",
      "Epoch: [153/200], Step: [5/8], Loss: 0.0017\n",
      "Epoch: [153/200], Step: [6/8], Loss: 0.0011\n",
      "Epoch: [153/200], Step: [7/8], Loss: 0.0018\n",
      "Epoch: [153/200], Step: [8/8], Loss: 0.0017\n",
      "Epoch: [154/200], Step: [1/8], Loss: 0.0012\n",
      "Epoch: [154/200], Step: [2/8], Loss: 0.0015\n",
      "Epoch: [154/200], Step: [3/8], Loss: 0.0015\n",
      "Epoch: [154/200], Step: [4/8], Loss: 0.0011\n",
      "Epoch: [154/200], Step: [5/8], Loss: 0.0013\n",
      "Epoch: [154/200], Step: [6/8], Loss: 0.0013\n",
      "Epoch: [154/200], Step: [7/8], Loss: 0.0015\n",
      "Epoch: [154/200], Step: [8/8], Loss: 0.0015\n",
      "Epoch: [155/200], Step: [1/8], Loss: 0.0014\n",
      "Epoch: [155/200], Step: [2/8], Loss: 0.0015\n",
      "Epoch: [155/200], Step: [3/8], Loss: 0.0011\n",
      "Epoch: [155/200], Step: [4/8], Loss: 0.0013\n",
      "Epoch: [155/200], Step: [5/8], Loss: 0.0011\n",
      "Epoch: [155/200], Step: [6/8], Loss: 0.0013\n",
      "Epoch: [155/200], Step: [7/8], Loss: 0.0014\n",
      "Epoch: [155/200], Step: [8/8], Loss: 0.0011\n",
      "Epoch: [156/200], Step: [1/8], Loss: 0.0015\n",
      "Epoch: [156/200], Step: [2/8], Loss: 0.0014\n",
      "Epoch: [156/200], Step: [3/8], Loss: 0.0009\n",
      "Epoch: [156/200], Step: [4/8], Loss: 0.0012\n",
      "Epoch: [156/200], Step: [5/8], Loss: 0.0012\n",
      "Epoch: [156/200], Step: [6/8], Loss: 0.0011\n",
      "Epoch: [156/200], Step: [7/8], Loss: 0.0016\n",
      "Epoch: [156/200], Step: [8/8], Loss: 0.0015\n",
      "Epoch: [157/200], Step: [1/8], Loss: 0.0011\n",
      "Epoch: [157/200], Step: [2/8], Loss: 0.0011\n",
      "Epoch: [157/200], Step: [3/8], Loss: 0.0012\n",
      "Epoch: [157/200], Step: [4/8], Loss: 0.0016\n",
      "Epoch: [157/200], Step: [5/8], Loss: 0.0015\n",
      "Epoch: [157/200], Step: [6/8], Loss: 0.0012\n",
      "Epoch: [157/200], Step: [7/8], Loss: 0.0011\n",
      "Epoch: [157/200], Step: [8/8], Loss: 0.0014\n",
      "Epoch: [158/200], Step: [1/8], Loss: 0.0014\n",
      "Epoch: [158/200], Step: [2/8], Loss: 0.0011\n",
      "Epoch: [158/200], Step: [3/8], Loss: 0.0013\n",
      "Epoch: [158/200], Step: [4/8], Loss: 0.0013\n",
      "Epoch: [158/200], Step: [5/8], Loss: 0.0010\n",
      "Epoch: [158/200], Step: [6/8], Loss: 0.0013\n",
      "Epoch: [158/200], Step: [7/8], Loss: 0.0015\n",
      "Epoch: [158/200], Step: [8/8], Loss: 0.0011\n",
      "Epoch: [159/200], Step: [1/8], Loss: 0.0010\n",
      "Epoch: [159/200], Step: [2/8], Loss: 0.0009\n",
      "Epoch: [159/200], Step: [3/8], Loss: 0.0012\n",
      "Epoch: [159/200], Step: [4/8], Loss: 0.0012\n",
      "Epoch: [159/200], Step: [5/8], Loss: 0.0015\n",
      "Epoch: [159/200], Step: [6/8], Loss: 0.0013\n",
      "Epoch: [159/200], Step: [7/8], Loss: 0.0014\n",
      "Epoch: [159/200], Step: [8/8], Loss: 0.0011\n",
      "Epoch: [160/200], Step: [1/8], Loss: 0.0010\n",
      "Epoch: [160/200], Step: [2/8], Loss: 0.0013\n",
      "Epoch: [160/200], Step: [3/8], Loss: 0.0012\n",
      "Epoch: [160/200], Step: [4/8], Loss: 0.0011\n",
      "Epoch: [160/200], Step: [5/8], Loss: 0.0014\n",
      "Epoch: [160/200], Step: [6/8], Loss: 0.0010\n",
      "Epoch: [160/200], Step: [7/8], Loss: 0.0014\n",
      "Epoch: [160/200], Step: [8/8], Loss: 0.0012\n",
      "Epoch: [161/200], Step: [1/8], Loss: 0.0012\n",
      "Epoch: [161/200], Step: [2/8], Loss: 0.0009\n",
      "Epoch: [161/200], Step: [3/8], Loss: 0.0013\n",
      "Epoch: [161/200], Step: [4/8], Loss: 0.0012\n",
      "Epoch: [161/200], Step: [5/8], Loss: 0.0010\n",
      "Epoch: [161/200], Step: [6/8], Loss: 0.0014\n",
      "Epoch: [161/200], Step: [7/8], Loss: 0.0014\n",
      "Epoch: [161/200], Step: [8/8], Loss: 0.0010\n",
      "Epoch: [162/200], Step: [1/8], Loss: 0.0011\n",
      "Epoch: [162/200], Step: [2/8], Loss: 0.0012\n",
      "Epoch: [162/200], Step: [3/8], Loss: 0.0012\n",
      "Epoch: [162/200], Step: [4/8], Loss: 0.0013\n",
      "Epoch: [162/200], Step: [5/8], Loss: 0.0013\n",
      "Epoch: [162/200], Step: [6/8], Loss: 0.0012\n",
      "Epoch: [162/200], Step: [7/8], Loss: 0.0011\n",
      "Epoch: [162/200], Step: [8/8], Loss: 0.0010\n",
      "Epoch: [163/200], Step: [1/8], Loss: 0.0012\n",
      "Epoch: [163/200], Step: [2/8], Loss: 0.0012\n",
      "Epoch: [163/200], Step: [3/8], Loss: 0.0012\n",
      "Epoch: [163/200], Step: [4/8], Loss: 0.0009\n",
      "Epoch: [163/200], Step: [5/8], Loss: 0.0011\n",
      "Epoch: [163/200], Step: [6/8], Loss: 0.0012\n",
      "Epoch: [163/200], Step: [7/8], Loss: 0.0013\n",
      "Epoch: [163/200], Step: [8/8], Loss: 0.0011\n",
      "Epoch: [164/200], Step: [1/8], Loss: 0.0010\n",
      "Epoch: [164/200], Step: [2/8], Loss: 0.0012\n",
      "Epoch: [164/200], Step: [3/8], Loss: 0.0014\n",
      "Epoch: [164/200], Step: [4/8], Loss: 0.0008\n",
      "Epoch: [164/200], Step: [5/8], Loss: 0.0011\n",
      "Epoch: [164/200], Step: [6/8], Loss: 0.0013\n",
      "Epoch: [164/200], Step: [7/8], Loss: 0.0011\n",
      "Epoch: [164/200], Step: [8/8], Loss: 0.0014\n",
      "Epoch: [165/200], Step: [1/8], Loss: 0.0011\n",
      "Epoch: [165/200], Step: [2/8], Loss: 0.0011\n",
      "Epoch: [165/200], Step: [3/8], Loss: 0.0008\n",
      "Epoch: [165/200], Step: [4/8], Loss: 0.0011\n",
      "Epoch: [165/200], Step: [5/8], Loss: 0.0012\n",
      "Epoch: [165/200], Step: [6/8], Loss: 0.0012\n",
      "Epoch: [165/200], Step: [7/8], Loss: 0.0013\n",
      "Epoch: [165/200], Step: [8/8], Loss: 0.0013\n",
      "Epoch: [166/200], Step: [1/8], Loss: 0.0008\n",
      "Epoch: [166/200], Step: [2/8], Loss: 0.0014\n",
      "Epoch: [166/200], Step: [3/8], Loss: 0.0011\n",
      "Epoch: [166/200], Step: [4/8], Loss: 0.0011\n",
      "Epoch: [166/200], Step: [5/8], Loss: 0.0011\n",
      "Epoch: [166/200], Step: [6/8], Loss: 0.0011\n",
      "Epoch: [166/200], Step: [7/8], Loss: 0.0013\n",
      "Epoch: [166/200], Step: [8/8], Loss: 0.0009\n",
      "Epoch: [167/200], Step: [1/8], Loss: 0.0012\n",
      "Epoch: [167/200], Step: [2/8], Loss: 0.0008\n",
      "Epoch: [167/200], Step: [3/8], Loss: 0.0013\n",
      "Epoch: [167/200], Step: [4/8], Loss: 0.0013\n",
      "Epoch: [167/200], Step: [5/8], Loss: 0.0011\n",
      "Epoch: [167/200], Step: [6/8], Loss: 0.0008\n",
      "Epoch: [167/200], Step: [7/8], Loss: 0.0010\n",
      "Epoch: [167/200], Step: [8/8], Loss: 0.0010\n",
      "Epoch: [168/200], Step: [1/8], Loss: 0.0010\n",
      "Epoch: [168/200], Step: [2/8], Loss: 0.0010\n",
      "Epoch: [168/200], Step: [3/8], Loss: 0.0007\n",
      "Epoch: [168/200], Step: [4/8], Loss: 0.0012\n",
      "Epoch: [168/200], Step: [5/8], Loss: 0.0017\n",
      "Epoch: [168/200], Step: [6/8], Loss: 0.0012\n",
      "Epoch: [168/200], Step: [7/8], Loss: 0.0008\n",
      "Epoch: [168/200], Step: [8/8], Loss: 0.0010\n",
      "Epoch: [169/200], Step: [1/8], Loss: 0.0010\n",
      "Epoch: [169/200], Step: [2/8], Loss: 0.0012\n",
      "Epoch: [169/200], Step: [3/8], Loss: 0.0010\n",
      "Epoch: [169/200], Step: [4/8], Loss: 0.0010\n",
      "Epoch: [169/200], Step: [5/8], Loss: 0.0009\n",
      "Epoch: [169/200], Step: [6/8], Loss: 0.0012\n",
      "Epoch: [169/200], Step: [7/8], Loss: 0.0012\n",
      "Epoch: [169/200], Step: [8/8], Loss: 0.0011\n",
      "Epoch: [170/200], Step: [1/8], Loss: 0.0009\n",
      "Epoch: [170/200], Step: [2/8], Loss: 0.0012\n",
      "Epoch: [170/200], Step: [3/8], Loss: 0.0009\n",
      "Epoch: [170/200], Step: [4/8], Loss: 0.0012\n",
      "Epoch: [170/200], Step: [5/8], Loss: 0.0011\n",
      "Epoch: [170/200], Step: [6/8], Loss: 0.0010\n",
      "Epoch: [170/200], Step: [7/8], Loss: 0.0010\n",
      "Epoch: [170/200], Step: [8/8], Loss: 0.0010\n",
      "Epoch: [171/200], Step: [1/8], Loss: 0.0009\n",
      "Epoch: [171/200], Step: [2/8], Loss: 0.0014\n",
      "Epoch: [171/200], Step: [3/8], Loss: 0.0010\n",
      "Epoch: [171/200], Step: [4/8], Loss: 0.0008\n",
      "Epoch: [171/200], Step: [5/8], Loss: 0.0010\n",
      "Epoch: [171/200], Step: [6/8], Loss: 0.0011\n",
      "Epoch: [171/200], Step: [7/8], Loss: 0.0008\n",
      "Epoch: [171/200], Step: [8/8], Loss: 0.0011\n",
      "Epoch: [172/200], Step: [1/8], Loss: 0.0009\n",
      "Epoch: [172/200], Step: [2/8], Loss: 0.0011\n",
      "Epoch: [172/200], Step: [3/8], Loss: 0.0011\n",
      "Epoch: [172/200], Step: [4/8], Loss: 0.0012\n",
      "Epoch: [172/200], Step: [5/8], Loss: 0.0008\n",
      "Epoch: [172/200], Step: [6/8], Loss: 0.0011\n",
      "Epoch: [172/200], Step: [7/8], Loss: 0.0010\n",
      "Epoch: [172/200], Step: [8/8], Loss: 0.0009\n",
      "Epoch: [173/200], Step: [1/8], Loss: 0.0011\n",
      "Epoch: [173/200], Step: [2/8], Loss: 0.0011\n",
      "Epoch: [173/200], Step: [3/8], Loss: 0.0009\n",
      "Epoch: [173/200], Step: [4/8], Loss: 0.0010\n",
      "Epoch: [173/200], Step: [5/8], Loss: 0.0009\n",
      "Epoch: [173/200], Step: [6/8], Loss: 0.0010\n",
      "Epoch: [173/200], Step: [7/8], Loss: 0.0011\n",
      "Epoch: [173/200], Step: [8/8], Loss: 0.0008\n",
      "Epoch: [174/200], Step: [1/8], Loss: 0.0008\n",
      "Epoch: [174/200], Step: [2/8], Loss: 0.0012\n",
      "Epoch: [174/200], Step: [3/8], Loss: 0.0009\n",
      "Epoch: [174/200], Step: [4/8], Loss: 0.0010\n",
      "Epoch: [174/200], Step: [5/8], Loss: 0.0009\n",
      "Epoch: [174/200], Step: [6/8], Loss: 0.0010\n",
      "Epoch: [174/200], Step: [7/8], Loss: 0.0011\n",
      "Epoch: [174/200], Step: [8/8], Loss: 0.0014\n",
      "Epoch: [175/200], Step: [1/8], Loss: 0.0009\n",
      "Epoch: [175/200], Step: [2/8], Loss: 0.0008\n",
      "Epoch: [175/200], Step: [3/8], Loss: 0.0009\n",
      "Epoch: [175/200], Step: [4/8], Loss: 0.0011\n",
      "Epoch: [175/200], Step: [5/8], Loss: 0.0011\n",
      "Epoch: [175/200], Step: [6/8], Loss: 0.0013\n",
      "Epoch: [175/200], Step: [7/8], Loss: 0.0009\n",
      "Epoch: [175/200], Step: [8/8], Loss: 0.0008\n",
      "Epoch: [176/200], Step: [1/8], Loss: 0.0010\n",
      "Epoch: [176/200], Step: [2/8], Loss: 0.0008\n",
      "Epoch: [176/200], Step: [3/8], Loss: 0.0012\n",
      "Epoch: [176/200], Step: [4/8], Loss: 0.0011\n",
      "Epoch: [176/200], Step: [5/8], Loss: 0.0009\n",
      "Epoch: [176/200], Step: [6/8], Loss: 0.0008\n",
      "Epoch: [176/200], Step: [7/8], Loss: 0.0009\n",
      "Epoch: [176/200], Step: [8/8], Loss: 0.0010\n",
      "Epoch: [177/200], Step: [1/8], Loss: 0.0008\n",
      "Epoch: [177/200], Step: [2/8], Loss: 0.0008\n",
      "Epoch: [177/200], Step: [3/8], Loss: 0.0010\n",
      "Epoch: [177/200], Step: [4/8], Loss: 0.0012\n",
      "Epoch: [177/200], Step: [5/8], Loss: 0.0007\n",
      "Epoch: [177/200], Step: [6/8], Loss: 0.0012\n",
      "Epoch: [177/200], Step: [7/8], Loss: 0.0010\n",
      "Epoch: [177/200], Step: [8/8], Loss: 0.0012\n",
      "Epoch: [178/200], Step: [1/8], Loss: 0.0010\n",
      "Epoch: [178/200], Step: [2/8], Loss: 0.0008\n",
      "Epoch: [178/200], Step: [3/8], Loss: 0.0008\n",
      "Epoch: [178/200], Step: [4/8], Loss: 0.0007\n",
      "Epoch: [178/200], Step: [5/8], Loss: 0.0009\n",
      "Epoch: [178/200], Step: [6/8], Loss: 0.0010\n",
      "Epoch: [178/200], Step: [7/8], Loss: 0.0014\n",
      "Epoch: [178/200], Step: [8/8], Loss: 0.0011\n",
      "Epoch: [179/200], Step: [1/8], Loss: 0.0009\n",
      "Epoch: [179/200], Step: [2/8], Loss: 0.0009\n",
      "Epoch: [179/200], Step: [3/8], Loss: 0.0011\n",
      "Epoch: [179/200], Step: [4/8], Loss: 0.0011\n",
      "Epoch: [179/200], Step: [5/8], Loss: 0.0009\n",
      "Epoch: [179/200], Step: [6/8], Loss: 0.0009\n",
      "Epoch: [179/200], Step: [7/8], Loss: 0.0009\n",
      "Epoch: [179/200], Step: [8/8], Loss: 0.0006\n",
      "Epoch: [180/200], Step: [1/8], Loss: 0.0009\n",
      "Epoch: [180/200], Step: [2/8], Loss: 0.0007\n",
      "Epoch: [180/200], Step: [3/8], Loss: 0.0011\n",
      "Epoch: [180/200], Step: [4/8], Loss: 0.0011\n",
      "Epoch: [180/200], Step: [5/8], Loss: 0.0008\n",
      "Epoch: [180/200], Step: [6/8], Loss: 0.0009\n",
      "Epoch: [180/200], Step: [7/8], Loss: 0.0010\n",
      "Epoch: [180/200], Step: [8/8], Loss: 0.0006\n",
      "Epoch: [181/200], Step: [1/8], Loss: 0.0010\n",
      "Epoch: [181/200], Step: [2/8], Loss: 0.0009\n",
      "Epoch: [181/200], Step: [3/8], Loss: 0.0009\n",
      "Epoch: [181/200], Step: [4/8], Loss: 0.0009\n",
      "Epoch: [181/200], Step: [5/8], Loss: 0.0007\n",
      "Epoch: [181/200], Step: [6/8], Loss: 0.0010\n",
      "Epoch: [181/200], Step: [7/8], Loss: 0.0009\n",
      "Epoch: [181/200], Step: [8/8], Loss: 0.0009\n",
      "Epoch: [182/200], Step: [1/8], Loss: 0.0010\n",
      "Epoch: [182/200], Step: [2/8], Loss: 0.0009\n",
      "Epoch: [182/200], Step: [3/8], Loss: 0.0007\n",
      "Epoch: [182/200], Step: [4/8], Loss: 0.0009\n",
      "Epoch: [182/200], Step: [5/8], Loss: 0.0009\n",
      "Epoch: [182/200], Step: [6/8], Loss: 0.0008\n",
      "Epoch: [182/200], Step: [7/8], Loss: 0.0011\n",
      "Epoch: [182/200], Step: [8/8], Loss: 0.0006\n",
      "Epoch: [183/200], Step: [1/8], Loss: 0.0009\n",
      "Epoch: [183/200], Step: [2/8], Loss: 0.0010\n",
      "Epoch: [183/200], Step: [3/8], Loss: 0.0010\n",
      "Epoch: [183/200], Step: [4/8], Loss: 0.0008\n",
      "Epoch: [183/200], Step: [5/8], Loss: 0.0007\n",
      "Epoch: [183/200], Step: [6/8], Loss: 0.0008\n",
      "Epoch: [183/200], Step: [7/8], Loss: 0.0009\n",
      "Epoch: [183/200], Step: [8/8], Loss: 0.0008\n",
      "Epoch: [184/200], Step: [1/8], Loss: 0.0010\n",
      "Epoch: [184/200], Step: [2/8], Loss: 0.0009\n",
      "Epoch: [184/200], Step: [3/8], Loss: 0.0006\n",
      "Epoch: [184/200], Step: [4/8], Loss: 0.0011\n",
      "Epoch: [184/200], Step: [5/8], Loss: 0.0009\n",
      "Epoch: [184/200], Step: [6/8], Loss: 0.0007\n",
      "Epoch: [184/200], Step: [7/8], Loss: 0.0008\n",
      "Epoch: [184/200], Step: [8/8], Loss: 0.0010\n",
      "Epoch: [185/200], Step: [1/8], Loss: 0.0009\n",
      "Epoch: [185/200], Step: [2/8], Loss: 0.0007\n",
      "Epoch: [185/200], Step: [3/8], Loss: 0.0010\n",
      "Epoch: [185/200], Step: [4/8], Loss: 0.0008\n",
      "Epoch: [185/200], Step: [5/8], Loss: 0.0010\n",
      "Epoch: [185/200], Step: [6/8], Loss: 0.0009\n",
      "Epoch: [185/200], Step: [7/8], Loss: 0.0007\n",
      "Epoch: [185/200], Step: [8/8], Loss: 0.0010\n",
      "Epoch: [186/200], Step: [1/8], Loss: 0.0009\n",
      "Epoch: [186/200], Step: [2/8], Loss: 0.0009\n",
      "Epoch: [186/200], Step: [3/8], Loss: 0.0008\n",
      "Epoch: [186/200], Step: [4/8], Loss: 0.0008\n",
      "Epoch: [186/200], Step: [5/8], Loss: 0.0009\n",
      "Epoch: [186/200], Step: [6/8], Loss: 0.0009\n",
      "Epoch: [186/200], Step: [7/8], Loss: 0.0009\n",
      "Epoch: [186/200], Step: [8/8], Loss: 0.0007\n",
      "Epoch: [187/200], Step: [1/8], Loss: 0.0007\n",
      "Epoch: [187/200], Step: [2/8], Loss: 0.0008\n",
      "Epoch: [187/200], Step: [3/8], Loss: 0.0009\n",
      "Epoch: [187/200], Step: [4/8], Loss: 0.0012\n",
      "Epoch: [187/200], Step: [5/8], Loss: 0.0007\n",
      "Epoch: [187/200], Step: [6/8], Loss: 0.0006\n",
      "Epoch: [187/200], Step: [7/8], Loss: 0.0007\n",
      "Epoch: [187/200], Step: [8/8], Loss: 0.0012\n",
      "Epoch: [188/200], Step: [1/8], Loss: 0.0008\n",
      "Epoch: [188/200], Step: [2/8], Loss: 0.0009\n",
      "Epoch: [188/200], Step: [3/8], Loss: 0.0008\n",
      "Epoch: [188/200], Step: [4/8], Loss: 0.0007\n",
      "Epoch: [188/200], Step: [5/8], Loss: 0.0007\n",
      "Epoch: [188/200], Step: [6/8], Loss: 0.0006\n",
      "Epoch: [188/200], Step: [7/8], Loss: 0.0010\n",
      "Epoch: [188/200], Step: [8/8], Loss: 0.0010\n",
      "Epoch: [189/200], Step: [1/8], Loss: 0.0008\n",
      "Epoch: [189/200], Step: [2/8], Loss: 0.0007\n",
      "Epoch: [189/200], Step: [3/8], Loss: 0.0008\n",
      "Epoch: [189/200], Step: [4/8], Loss: 0.0010\n",
      "Epoch: [189/200], Step: [5/8], Loss: 0.0009\n",
      "Epoch: [189/200], Step: [6/8], Loss: 0.0010\n",
      "Epoch: [189/200], Step: [7/8], Loss: 0.0007\n",
      "Epoch: [189/200], Step: [8/8], Loss: 0.0007\n",
      "Epoch: [190/200], Step: [1/8], Loss: 0.0007\n",
      "Epoch: [190/200], Step: [2/8], Loss: 0.0009\n",
      "Epoch: [190/200], Step: [3/8], Loss: 0.0009\n",
      "Epoch: [190/200], Step: [4/8], Loss: 0.0010\n",
      "Epoch: [190/200], Step: [5/8], Loss: 0.0009\n",
      "Epoch: [190/200], Step: [6/8], Loss: 0.0007\n",
      "Epoch: [190/200], Step: [7/8], Loss: 0.0006\n",
      "Epoch: [190/200], Step: [8/8], Loss: 0.0008\n",
      "Epoch: [191/200], Step: [1/8], Loss: 0.0006\n",
      "Epoch: [191/200], Step: [2/8], Loss: 0.0009\n",
      "Epoch: [191/200], Step: [3/8], Loss: 0.0010\n",
      "Epoch: [191/200], Step: [4/8], Loss: 0.0010\n",
      "Epoch: [191/200], Step: [5/8], Loss: 0.0006\n",
      "Epoch: [191/200], Step: [6/8], Loss: 0.0007\n",
      "Epoch: [191/200], Step: [7/8], Loss: 0.0008\n",
      "Epoch: [191/200], Step: [8/8], Loss: 0.0006\n",
      "Epoch: [192/200], Step: [1/8], Loss: 0.0008\n",
      "Epoch: [192/200], Step: [2/8], Loss: 0.0008\n",
      "Epoch: [192/200], Step: [3/8], Loss: 0.0009\n",
      "Epoch: [192/200], Step: [4/8], Loss: 0.0007\n",
      "Epoch: [192/200], Step: [5/8], Loss: 0.0007\n",
      "Epoch: [192/200], Step: [6/8], Loss: 0.0009\n",
      "Epoch: [192/200], Step: [7/8], Loss: 0.0008\n",
      "Epoch: [192/200], Step: [8/8], Loss: 0.0005\n",
      "Epoch: [193/200], Step: [1/8], Loss: 0.0007\n",
      "Epoch: [193/200], Step: [2/8], Loss: 0.0007\n",
      "Epoch: [193/200], Step: [3/8], Loss: 0.0009\n",
      "Epoch: [193/200], Step: [4/8], Loss: 0.0008\n",
      "Epoch: [193/200], Step: [5/8], Loss: 0.0007\n",
      "Epoch: [193/200], Step: [6/8], Loss: 0.0006\n",
      "Epoch: [193/200], Step: [7/8], Loss: 0.0009\n",
      "Epoch: [193/200], Step: [8/8], Loss: 0.0009\n",
      "Epoch: [194/200], Step: [1/8], Loss: 0.0008\n",
      "Epoch: [194/200], Step: [2/8], Loss: 0.0007\n",
      "Epoch: [194/200], Step: [3/8], Loss: 0.0009\n",
      "Epoch: [194/200], Step: [4/8], Loss: 0.0007\n",
      "Epoch: [194/200], Step: [5/8], Loss: 0.0008\n",
      "Epoch: [194/200], Step: [6/8], Loss: 0.0006\n",
      "Epoch: [194/200], Step: [7/8], Loss: 0.0007\n",
      "Epoch: [194/200], Step: [8/8], Loss: 0.0007\n",
      "Epoch: [195/200], Step: [1/8], Loss: 0.0008\n",
      "Epoch: [195/200], Step: [2/8], Loss: 0.0006\n",
      "Epoch: [195/200], Step: [3/8], Loss: 0.0008\n",
      "Epoch: [195/200], Step: [4/8], Loss: 0.0009\n",
      "Epoch: [195/200], Step: [5/8], Loss: 0.0008\n",
      "Epoch: [195/200], Step: [6/8], Loss: 0.0007\n",
      "Epoch: [195/200], Step: [7/8], Loss: 0.0008\n",
      "Epoch: [195/200], Step: [8/8], Loss: 0.0005\n",
      "Epoch: [196/200], Step: [1/8], Loss: 0.0007\n",
      "Epoch: [196/200], Step: [2/8], Loss: 0.0008\n",
      "Epoch: [196/200], Step: [3/8], Loss: 0.0007\n",
      "Epoch: [196/200], Step: [4/8], Loss: 0.0006\n",
      "Epoch: [196/200], Step: [5/8], Loss: 0.0007\n",
      "Epoch: [196/200], Step: [6/8], Loss: 0.0008\n",
      "Epoch: [196/200], Step: [7/8], Loss: 0.0009\n",
      "Epoch: [196/200], Step: [8/8], Loss: 0.0008\n",
      "Epoch: [197/200], Step: [1/8], Loss: 0.0009\n",
      "Epoch: [197/200], Step: [2/8], Loss: 0.0007\n",
      "Epoch: [197/200], Step: [3/8], Loss: 0.0008\n",
      "Epoch: [197/200], Step: [4/8], Loss: 0.0006\n",
      "Epoch: [197/200], Step: [5/8], Loss: 0.0008\n",
      "Epoch: [197/200], Step: [6/8], Loss: 0.0007\n",
      "Epoch: [197/200], Step: [7/8], Loss: 0.0008\n",
      "Epoch: [197/200], Step: [8/8], Loss: 0.0007\n",
      "Epoch: [198/200], Step: [1/8], Loss: 0.0008\n",
      "Epoch: [198/200], Step: [2/8], Loss: 0.0007\n",
      "Epoch: [198/200], Step: [3/8], Loss: 0.0008\n",
      "Epoch: [198/200], Step: [4/8], Loss: 0.0006\n",
      "Epoch: [198/200], Step: [5/8], Loss: 0.0005\n",
      "Epoch: [198/200], Step: [6/8], Loss: 0.0008\n",
      "Epoch: [198/200], Step: [7/8], Loss: 0.0008\n",
      "Epoch: [198/200], Step: [8/8], Loss: 0.0008\n",
      "Epoch: [199/200], Step: [1/8], Loss: 0.0007\n",
      "Epoch: [199/200], Step: [2/8], Loss: 0.0007\n",
      "Epoch: [199/200], Step: [3/8], Loss: 0.0007\n",
      "Epoch: [199/200], Step: [4/8], Loss: 0.0006\n",
      "Epoch: [199/200], Step: [5/8], Loss: 0.0008\n",
      "Epoch: [199/200], Step: [6/8], Loss: 0.0008\n",
      "Epoch: [199/200], Step: [7/8], Loss: 0.0008\n",
      "Epoch: [199/200], Step: [8/8], Loss: 0.0006\n",
      "Epoch: [200/200], Step: [1/8], Loss: 0.0007\n",
      "Epoch: [200/200], Step: [2/8], Loss: 0.0007\n",
      "Epoch: [200/200], Step: [3/8], Loss: 0.0004\n",
      "Epoch: [200/200], Step: [4/8], Loss: 0.0007\n",
      "Epoch: [200/200], Step: [5/8], Loss: 0.0009\n",
      "Epoch: [200/200], Step: [6/8], Loss: 0.0009\n",
      "Epoch: [200/200], Step: [7/8], Loss: 0.0006\n",
      "Epoch: [200/200], Step: [8/8], Loss: 0.0007\n"
     ]
    }
   ],
   "source": [
    "# 训练模型\n",
    "for epoch in range(args.epochs):\n",
    "    for i, (images, labels) in enumerate(train_loader, 0):\n",
    "        # 前向传播\n",
    "        output = model(images)\n",
    "\n",
    "        Loss = loss(output, labels.long())\n",
    "        # 如果采用第二种数据加载思路，请用下面这行代码\n",
    "        # Loss = loss(output, labels)\n",
    "        # 反向传播\n",
    "        optimizer.zero_grad()\n",
    "        Loss.backward()\n",
    "        optimizer.step()\n",
    "        # 输出\n",
    "        print('Epoch: [{}/{}], Step: [{}/{}], Loss: {:.4f}'\n",
    "                  .format(epoch + 1, args.epochs, i + 1, int(len(train_image) / size_batch) + 1, Loss.item()))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 342,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy:：69.2%\n"
     ]
    }
   ],
   "source": [
    "# 模型测试\n",
    "with torch.no_grad():\n",
    "    total_cor = 0\n",
    "    total_num = 0\n",
    "    for images, labels in test_loader:\n",
    "        images = images\n",
    "        labels = labels\n",
    "        output = model(images)\n",
    "        _, predict_result = torch.max(output.data, 1)\n",
    "        total_num += labels.size(0)\n",
    "        total_cor += (predict_result == labels).sum().item()\n",
    "    print(\"Test Accuracy:：{}%\".format(100 * total_cor / total_num))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# 4. 模型分析"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 4.1 调整类别数量"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "num_classes = 200\n",
    "# 加载数据\n",
    "train_image, train_label, test_image, test_label = LoadData(num_classes, args.num_samples_train, args.num_samples_test, args.seed)\n",
    "# 训练用图像应该为二维，即28*28\n",
    "train_images_list = list()\n",
    "for i in range(0, len(train_image)):\n",
    "    temp_image = np.reshape(train_image[i], (1, 28, 28))\n",
    "    train_images_list.append(temp_image)\n",
    "train_images = np.array(train_images_list)\n",
    "print(train_images.shape)\n",
    "\n",
    "# 测试用图像应该为二维，即28*28\n",
    "test_images_list = list()\n",
    "for i in range(0, len(test_image)):\n",
    "    temp_image = np.reshape(test_image[i], (1, 28, 28))\n",
    "    test_images_list.append(temp_image)\n",
    "test_images = np.array(test_images_list)\n",
    "print(test_images.shape)\n",
    "\n",
    "# 转换为pytorch可处理数据集\n",
    "train_dataset = da.TensorDataset(torch.from_numpy(train_images), torch.from_numpy(train_label))\n",
    "test_dataset = da.TensorDataset(torch.from_numpy(test_images), torch.from_numpy(test_label))\n",
    "# 数据分批\n",
    "size_batch = 100\n",
    "train_loader = da.DataLoader(dataset=train_dataset, batch_size=size_batch, shuffle=True)\n",
    "test_loader = da.DataLoader(dataset=test_dataset, batch_size=size_batch, shuffle=False)\n",
    "# 定义超参数, 模型, 损失函数 和 优化器\n",
    "# 定义内部超参数\n",
    "learning_rate = 1e-3\n",
    "\n",
    "\n",
    "# 定义模型\n",
    "class CNN(nn.Module):\n",
    "    def __init__(self, size_out):\n",
    "        super(CNN, self).__init__()\n",
    "        self.layer1 = torch.nn.Sequential(\n",
    "            torch.nn.Conv2d(1, 10, kernel_size=5),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.MaxPool2d(kernel_size=2)\n",
    "        )\n",
    "        self.layer2 = torch.nn.Sequential(\n",
    "            torch.nn.Conv2d(10, 20, kernel_size=5),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.MaxPool2d(kernel_size=2)\n",
    "        )\n",
    "        self.layer3 = torch.nn.Sequential(\n",
    "            torch.nn.Linear(320, 250),\n",
    "            torch.nn.Linear(250, num_classes)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.to(torch.float32)\n",
    "        layer1_out = self.layer1(x)\n",
    "        layer2_out = self.layer2(layer1_out)\n",
    "        layer2_out_solve = layer2_out.view(-1, 320)\n",
    "        layer3_out = self.layer3(layer2_out_solve)\n",
    "        return layer3_out\n",
    "\n",
    "\n",
    "model = CNN(size_out)  # .to(device)\n",
    "# 定义损失函数\n",
    "loss = nn.CrossEntropyLoss()\n",
    "# 定义优化器\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "# optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)\n",
    "# 训练模型\n",
    "for epoch in range(args.epochs):\n",
    "    for i, (images, labels) in enumerate(train_loader, 0):\n",
    "        # 前向传播\n",
    "        output = model(images)\n",
    "\n",
    "        Loss = loss(output, labels.long())\n",
    "        # 如果采用第二种数据加载思路，请用下面这行代码\n",
    "        # Loss = loss(output, labels)\n",
    "        # 反向传播\n",
    "        optimizer.zero_grad()\n",
    "        Loss.backward()\n",
    "        optimizer.step()\n",
    "        # 输出\n",
    "        print('Epoch: [{}/{}], Step: [{}/{}], Loss: {:.4f}'\n",
    "              .format(epoch + 1, args.epochs, i + 1, int(len(train_image) / size_batch) + 1, Loss.item()))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 347,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy:：66.3%\n"
     ]
    }
   ],
   "source": [
    "# 模型测试\n",
    "with torch.no_grad():\n",
    "    total_cor = 0\n",
    "    total_num = 0\n",
    "    for images, labels in test_loader:\n",
    "        images = images\n",
    "        labels = labels\n",
    "        output = model(images)\n",
    "        _, predict_result = torch.max(output.data, 1)\n",
    "        total_num += labels.size(0)\n",
    "        total_cor += (predict_result == labels).sum().item()\n",
    "    print(\"Test Accuracy:：{}%\".format(100 * total_cor / total_num))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 4.2 调整训练/测试样本数量"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "trian_num = 10\n",
    "test_num = 10\n",
    "num_classes = 100\n",
    "# 加载数据\n",
    "train_image, train_label, test_image, test_label = LoadData(num_classes, trian_num,test_num, args.seed)\n",
    "# 训练用图像应该为二维，即28*28\n",
    "train_images_list = list()\n",
    "for i in range(0, len(train_image)):\n",
    "    temp_image = np.reshape(train_image[i], (1, 28, 28))\n",
    "    train_images_list.append(temp_image)\n",
    "train_images = np.array(train_images_list)\n",
    "print(train_images.shape)\n",
    "\n",
    "# 测试用图像应该为二维，即28*28\n",
    "test_images_list = list()\n",
    "for i in range(0, len(test_image)):\n",
    "    temp_image = np.reshape(test_image[i], (1, 28, 28))\n",
    "    test_images_list.append(temp_image)\n",
    "test_images = np.array(test_images_list)\n",
    "print(test_images.shape)\n",
    "\n",
    "# 转换为pytorch可处理数据集\n",
    "train_dataset = da.TensorDataset(torch.from_numpy(train_images), torch.from_numpy(train_label))\n",
    "test_dataset = da.TensorDataset(torch.from_numpy(test_images), torch.from_numpy(test_label))\n",
    "# 数据分批\n",
    "size_batch = 100\n",
    "train_loader = da.DataLoader(dataset=train_dataset, batch_size=size_batch, shuffle=True)\n",
    "test_loader = da.DataLoader(dataset=test_dataset, batch_size=size_batch, shuffle=False)\n",
    "# 定义超参数, 模型, 损失函数 和 优化器\n",
    "# 定义内部超参数\n",
    "learning_rate = 1e-3\n",
    "\n",
    "\n",
    "# 定义模型\n",
    "class CNN(nn.Module):\n",
    "    def __init__(self, size_out):\n",
    "        super(CNN, self).__init__()\n",
    "        self.layer1 = torch.nn.Sequential(\n",
    "            torch.nn.Conv2d(1, 10, kernel_size=5),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.MaxPool2d(kernel_size=2)\n",
    "        )\n",
    "        self.layer2 = torch.nn.Sequential(\n",
    "            torch.nn.Conv2d(10, 20, kernel_size=5),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.MaxPool2d(kernel_size=2)\n",
    "        )\n",
    "        self.layer3 = torch.nn.Sequential(\n",
    "            torch.nn.Linear(320, 200),\n",
    "            torch.nn.Linear(200, num_classes)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.to(torch.float32)\n",
    "        layer1_out = self.layer1(x)\n",
    "        layer2_out = self.layer2(layer1_out)\n",
    "        layer2_out_solve = layer2_out.view(-1, 320)\n",
    "        layer3_out = self.layer3(layer2_out_solve)\n",
    "        return layer3_out\n",
    "\n",
    "\n",
    "model = CNN(size_out)  # .to(device)\n",
    "# 定义损失函数\n",
    "loss = nn.CrossEntropyLoss()\n",
    "# 定义优化器\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "# optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)\n",
    "# 训练模型\n",
    "for epoch in range(args.epochs):\n",
    "    for i, (images, labels) in enumerate(train_loader, 0):\n",
    "        # 前向传播\n",
    "        output = model(images)\n",
    "\n",
    "        Loss = loss(output, labels.long())\n",
    "        # 如果采用第二种数据加载思路，请用下面这行代码\n",
    "        # Loss = loss(output, labels)\n",
    "        # 反向传播\n",
    "        optimizer.zero_grad()\n",
    "        Loss.backward()\n",
    "        optimizer.step()\n",
    "        # 输出\n",
    "        print('Epoch: [{}/{}], Step: [{}/{}], Loss: {:.4f}'\n",
    "              .format(epoch + 1, args.epochs, i + 1, int(len(train_image) / size_batch) + 1, Loss.item()))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 351,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy:：55.5%\n"
     ]
    }
   ],
   "source": [
    "# 模型测试\n",
    "with torch.no_grad():\n",
    "    total_cor = 0\n",
    "    total_num = 0\n",
    "    for images, labels in test_loader:\n",
    "        images = images\n",
    "        labels = labels\n",
    "        output = model(images)\n",
    "        _, predict_result = torch.max(output.data, 1)\n",
    "        total_num += labels.size(0)\n",
    "        total_cor += (predict_result == labels).sum().item()\n",
    "    print(\"Test Accuracy:：{}%\".format(100 * total_cor / total_num))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 4.3不同模型结构"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "trian_num = 18\n",
    "test_num = 2\n",
    "num_classes = 100\n",
    "# 加载数据\n",
    "train_image, train_label, test_image, test_label = LoadData(num_classes, trian_num, test_num, args.seed)\n",
    "# 训练用图像应该为二维，即28*28\n",
    "train_images_list = list()\n",
    "for i in range(0, len(train_image)):\n",
    "    temp_image = np.reshape(train_image[i], (1, 28, 28))\n",
    "    train_images_list.append(temp_image)\n",
    "train_images = np.array(train_images_list)\n",
    "print(train_images.shape)\n",
    "\n",
    "# 测试用图像应该为二维，即28*28\n",
    "test_images_list = list()\n",
    "for i in range(0, len(test_image)):\n",
    "    temp_image = np.reshape(test_image[i], (1, 28, 28))\n",
    "    test_images_list.append(temp_image)\n",
    "test_images = np.array(test_images_list)\n",
    "print(test_images.shape)\n",
    "\n",
    "# 转换为pytorch可处理数据集\n",
    "train_dataset = da.TensorDataset(torch.from_numpy(train_images), torch.from_numpy(train_label))\n",
    "test_dataset = da.TensorDataset(torch.from_numpy(test_images), torch.from_numpy(test_label))\n",
    "# 数据分批\n",
    "size_batch = 100\n",
    "train_loader = da.DataLoader(dataset=train_dataset, batch_size=size_batch, shuffle=True)\n",
    "test_loader = da.DataLoader(dataset=test_dataset, batch_size=size_batch, shuffle=False)\n",
    "# 定义超参数, 模型, 损失函数 和 优化器\n",
    "# 定义内部超参数\n",
    "learning_rate = 1e-3\n",
    "\n",
    "# 定义模型\n",
    "class CNN(nn.Module):\n",
    "    def __init__(self, size_out):\n",
    "        super(CNN, self).__init__()\n",
    "        self.layer1 = torch.nn.Sequential(\n",
    "            torch.nn.Conv2d(1, 20, kernel_size=10),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.MaxPool2d(kernel_size=4)\n",
    "        )\n",
    "        self.layer2 = torch.nn.Sequential(\n",
    "            torch.nn.Linear(320, 150),\n",
    "            torch.nn.Linear(150, 100)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.to(torch.float32)\n",
    "        layer1_out = self.layer1(x)\n",
    "        layer1_out_solve = layer1_out.view(-1, 320)\n",
    "        layer2_out = self.layer2(layer1_out_solve)\n",
    "        return layer2_out\n",
    "\n",
    "\n",
    "model = CNN(size_out)  # .to(device)\n",
    "# 定义损失函数\n",
    "loss = nn.CrossEntropyLoss()\n",
    "# 定义优化器\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "# optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)\n",
    "# 训练模型\n",
    "for epoch in range(args.epochs):\n",
    "    for i, (images, labels) in enumerate(train_loader, 0):\n",
    "        # 前向传播\n",
    "        output = model(images)\n",
    "\n",
    "        Loss = loss(output, labels.long())\n",
    "        # 如果采用第二种数据加载思路，请用下面这行代码\n",
    "        # Loss = loss(output, labels)\n",
    "        # 反向传播\n",
    "        optimizer.zero_grad()\n",
    "        Loss.backward()\n",
    "        optimizer.step()\n",
    "        # 输出\n",
    "        print('Epoch: [{}/{}], Step: [{}/{}], Loss: {:.4f}'\n",
    "              .format(epoch + 1, args.epochs, i + 1, int(len(train_image) / size_batch) + 1, Loss.item()))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 368,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy:：77.5%\n"
     ]
    }
   ],
   "source": [
    "# 模型测试\n",
    "with torch.no_grad():\n",
    "    total_cor = 0\n",
    "    total_num = 0\n",
    "    for images, labels in test_loader:\n",
    "        images = images\n",
    "        labels = labels\n",
    "        output = model(images)\n",
    "        _, predict_result = torch.max(output.data, 1)\n",
    "        total_num += labels.size(0)\n",
    "        total_cor += (predict_result == labels).sum().item()\n",
    "    print(\"Test Accuracy:：{}%\".format(100 * total_cor / total_num))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 4.4不同优化器"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 372,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1800, 1, 28, 28)\n",
      "(200, 1, 28, 28)\n",
      "Epoch: [1/200], Step: [1/19], Loss: 4.6029\n",
      "Epoch: [1/200], Step: [2/19], Loss: 4.6467\n",
      "Epoch: [1/200], Step: [3/19], Loss: 4.6070\n",
      "Epoch: [1/200], Step: [4/19], Loss: 4.6175\n",
      "Epoch: [1/200], Step: [5/19], Loss: 4.5904\n",
      "Epoch: [1/200], Step: [6/19], Loss: 4.5934\n",
      "Epoch: [1/200], Step: [7/19], Loss: 4.6088\n",
      "Epoch: [1/200], Step: [8/19], Loss: 4.6199\n",
      "Epoch: [1/200], Step: [9/19], Loss: 4.6157\n",
      "Epoch: [1/200], Step: [10/19], Loss: 4.6027\n",
      "Epoch: [1/200], Step: [11/19], Loss: 4.6124\n",
      "Epoch: [1/200], Step: [12/19], Loss: 4.6397\n",
      "Epoch: [1/200], Step: [13/19], Loss: 4.6148\n",
      "Epoch: [1/200], Step: [14/19], Loss: 4.5782\n",
      "Epoch: [1/200], Step: [15/19], Loss: 4.6109\n",
      "Epoch: [1/200], Step: [16/19], Loss: 4.6260\n",
      "Epoch: [1/200], Step: [17/19], Loss: 4.6111\n",
      "Epoch: [1/200], Step: [18/19], Loss: 4.6126\n",
      "Epoch: [2/200], Step: [1/19], Loss: 4.6261\n",
      "Epoch: [2/200], Step: [2/19], Loss: 4.6010\n",
      "Epoch: [2/200], Step: [3/19], Loss: 4.6359\n",
      "Epoch: [2/200], Step: [4/19], Loss: 4.5926\n",
      "Epoch: [2/200], Step: [5/19], Loss: 4.6054\n",
      "Epoch: [2/200], Step: [6/19], Loss: 4.6254\n",
      "Epoch: [2/200], Step: [7/19], Loss: 4.5856\n",
      "Epoch: [2/200], Step: [8/19], Loss: 4.6148\n",
      "Epoch: [2/200], Step: [9/19], Loss: 4.6153\n",
      "Epoch: [2/200], Step: [10/19], Loss: 4.5991\n",
      "Epoch: [2/200], Step: [11/19], Loss: 4.6141\n",
      "Epoch: [2/200], Step: [12/19], Loss: 4.6244\n",
      "Epoch: [2/200], Step: [13/19], Loss: 4.6214\n",
      "Epoch: [2/200], Step: [14/19], Loss: 4.5988\n",
      "Epoch: [2/200], Step: [15/19], Loss: 4.6323\n",
      "Epoch: [2/200], Step: [16/19], Loss: 4.6182\n",
      "Epoch: [2/200], Step: [17/19], Loss: 4.5915\n",
      "Epoch: [2/200], Step: [18/19], Loss: 4.6038\n",
      "Epoch: [3/200], Step: [1/19], Loss: 4.6087\n",
      "Epoch: [3/200], Step: [2/19], Loss: 4.6110\n",
      "Epoch: [3/200], Step: [3/19], Loss: 4.6032\n",
      "Epoch: [3/200], Step: [4/19], Loss: 4.5967\n",
      "Epoch: [3/200], Step: [5/19], Loss: 4.6151\n",
      "Epoch: [3/200], Step: [6/19], Loss: 4.6085\n",
      "Epoch: [3/200], Step: [7/19], Loss: 4.6070\n",
      "Epoch: [3/200], Step: [8/19], Loss: 4.6084\n",
      "Epoch: [3/200], Step: [9/19], Loss: 4.5857\n",
      "Epoch: [3/200], Step: [10/19], Loss: 4.6307\n",
      "Epoch: [3/200], Step: [11/19], Loss: 4.6207\n",
      "Epoch: [3/200], Step: [12/19], Loss: 4.6246\n",
      "Epoch: [3/200], Step: [13/19], Loss: 4.6052\n",
      "Epoch: [3/200], Step: [14/19], Loss: 4.6141\n",
      "Epoch: [3/200], Step: [15/19], Loss: 4.6054\n",
      "Epoch: [3/200], Step: [16/19], Loss: 4.6265\n",
      "Epoch: [3/200], Step: [17/19], Loss: 4.6174\n",
      "Epoch: [3/200], Step: [18/19], Loss: 4.6120\n",
      "Epoch: [4/200], Step: [1/19], Loss: 4.5822\n",
      "Epoch: [4/200], Step: [2/19], Loss: 4.6069\n",
      "Epoch: [4/200], Step: [3/19], Loss: 4.6230\n",
      "Epoch: [4/200], Step: [4/19], Loss: 4.6250\n",
      "Epoch: [4/200], Step: [5/19], Loss: 4.6004\n",
      "Epoch: [4/200], Step: [6/19], Loss: 4.6307\n",
      "Epoch: [4/200], Step: [7/19], Loss: 4.6296\n",
      "Epoch: [4/200], Step: [8/19], Loss: 4.5947\n",
      "Epoch: [4/200], Step: [9/19], Loss: 4.6012\n",
      "Epoch: [4/200], Step: [10/19], Loss: 4.6148\n",
      "Epoch: [4/200], Step: [11/19], Loss: 4.6090\n",
      "Epoch: [4/200], Step: [12/19], Loss: 4.6028\n",
      "Epoch: [4/200], Step: [13/19], Loss: 4.6041\n",
      "Epoch: [4/200], Step: [14/19], Loss: 4.6119\n",
      "Epoch: [4/200], Step: [15/19], Loss: 4.6321\n",
      "Epoch: [4/200], Step: [16/19], Loss: 4.6195\n",
      "Epoch: [4/200], Step: [17/19], Loss: 4.6059\n",
      "Epoch: [4/200], Step: [18/19], Loss: 4.6021\n",
      "Epoch: [5/200], Step: [1/19], Loss: 4.5912\n",
      "Epoch: [5/200], Step: [2/19], Loss: 4.5981\n",
      "Epoch: [5/200], Step: [3/19], Loss: 4.6093\n",
      "Epoch: [5/200], Step: [4/19], Loss: 4.6050\n",
      "Epoch: [5/200], Step: [5/19], Loss: 4.6389\n",
      "Epoch: [5/200], Step: [6/19], Loss: 4.6088\n",
      "Epoch: [5/200], Step: [7/19], Loss: 4.6484\n",
      "Epoch: [5/200], Step: [8/19], Loss: 4.5889\n",
      "Epoch: [5/200], Step: [9/19], Loss: 4.5945\n",
      "Epoch: [5/200], Step: [10/19], Loss: 4.6240\n",
      "Epoch: [5/200], Step: [11/19], Loss: 4.6007\n",
      "Epoch: [5/200], Step: [12/19], Loss: 4.6142\n",
      "Epoch: [5/200], Step: [13/19], Loss: 4.6069\n",
      "Epoch: [5/200], Step: [14/19], Loss: 4.6079\n",
      "Epoch: [5/200], Step: [15/19], Loss: 4.6215\n",
      "Epoch: [5/200], Step: [16/19], Loss: 4.5910\n",
      "Epoch: [5/200], Step: [17/19], Loss: 4.6177\n",
      "Epoch: [5/200], Step: [18/19], Loss: 4.6242\n",
      "Epoch: [6/200], Step: [1/19], Loss: 4.6070\n",
      "Epoch: [6/200], Step: [2/19], Loss: 4.5945\n",
      "Epoch: [6/200], Step: [3/19], Loss: 4.6017\n",
      "Epoch: [6/200], Step: [4/19], Loss: 4.6044\n",
      "Epoch: [6/200], Step: [5/19], Loss: 4.5984\n",
      "Epoch: [6/200], Step: [6/19], Loss: 4.6379\n",
      "Epoch: [6/200], Step: [7/19], Loss: 4.6221\n",
      "Epoch: [6/200], Step: [8/19], Loss: 4.6071\n",
      "Epoch: [6/200], Step: [9/19], Loss: 4.6102\n",
      "Epoch: [6/200], Step: [10/19], Loss: 4.6264\n",
      "Epoch: [6/200], Step: [11/19], Loss: 4.6131\n",
      "Epoch: [6/200], Step: [12/19], Loss: 4.6178\n",
      "Epoch: [6/200], Step: [13/19], Loss: 4.6097\n",
      "Epoch: [6/200], Step: [14/19], Loss: 4.6024\n",
      "Epoch: [6/200], Step: [15/19], Loss: 4.6024\n",
      "Epoch: [6/200], Step: [16/19], Loss: 4.6018\n",
      "Epoch: [6/200], Step: [17/19], Loss: 4.6101\n",
      "Epoch: [6/200], Step: [18/19], Loss: 4.6190\n",
      "Epoch: [7/200], Step: [1/19], Loss: 4.6286\n",
      "Epoch: [7/200], Step: [2/19], Loss: 4.6017\n",
      "Epoch: [7/200], Step: [3/19], Loss: 4.6068\n",
      "Epoch: [7/200], Step: [4/19], Loss: 4.5837\n",
      "Epoch: [7/200], Step: [5/19], Loss: 4.6065\n",
      "Epoch: [7/200], Step: [6/19], Loss: 4.6243\n",
      "Epoch: [7/200], Step: [7/19], Loss: 4.6037\n",
      "Epoch: [7/200], Step: [8/19], Loss: 4.5957\n",
      "Epoch: [7/200], Step: [9/19], Loss: 4.6053\n",
      "Epoch: [7/200], Step: [10/19], Loss: 4.5980\n",
      "Epoch: [7/200], Step: [11/19], Loss: 4.6237\n",
      "Epoch: [7/200], Step: [12/19], Loss: 4.5893\n",
      "Epoch: [7/200], Step: [13/19], Loss: 4.6360\n",
      "Epoch: [7/200], Step: [14/19], Loss: 4.6093\n",
      "Epoch: [7/200], Step: [15/19], Loss: 4.5969\n",
      "Epoch: [7/200], Step: [16/19], Loss: 4.6074\n",
      "Epoch: [7/200], Step: [17/19], Loss: 4.6307\n",
      "Epoch: [7/200], Step: [18/19], Loss: 4.6335\n",
      "Epoch: [8/200], Step: [1/19], Loss: 4.5991\n",
      "Epoch: [8/200], Step: [2/19], Loss: 4.6101\n",
      "Epoch: [8/200], Step: [3/19], Loss: 4.6046\n",
      "Epoch: [8/200], Step: [4/19], Loss: 4.6138\n",
      "Epoch: [8/200], Step: [5/19], Loss: 4.6144\n",
      "Epoch: [8/200], Step: [6/19], Loss: 4.5976\n",
      "Epoch: [8/200], Step: [7/19], Loss: 4.6027\n",
      "Epoch: [8/200], Step: [8/19], Loss: 4.6111\n",
      "Epoch: [8/200], Step: [9/19], Loss: 4.5960\n",
      "Epoch: [8/200], Step: [10/19], Loss: 4.6144\n",
      "Epoch: [8/200], Step: [11/19], Loss: 4.6210\n",
      "Epoch: [8/200], Step: [12/19], Loss: 4.6357\n",
      "Epoch: [8/200], Step: [13/19], Loss: 4.5971\n",
      "Epoch: [8/200], Step: [14/19], Loss: 4.5962\n",
      "Epoch: [8/200], Step: [15/19], Loss: 4.5972\n",
      "Epoch: [8/200], Step: [16/19], Loss: 4.6267\n",
      "Epoch: [8/200], Step: [17/19], Loss: 4.6220\n",
      "Epoch: [8/200], Step: [18/19], Loss: 4.6165\n",
      "Epoch: [9/200], Step: [1/19], Loss: 4.6158\n",
      "Epoch: [9/200], Step: [2/19], Loss: 4.6156\n",
      "Epoch: [9/200], Step: [3/19], Loss: 4.5967\n",
      "Epoch: [9/200], Step: [4/19], Loss: 4.6049\n",
      "Epoch: [9/200], Step: [5/19], Loss: 4.5906\n",
      "Epoch: [9/200], Step: [6/19], Loss: 4.6136\n",
      "Epoch: [9/200], Step: [7/19], Loss: 4.6028\n",
      "Epoch: [9/200], Step: [8/19], Loss: 4.6181\n",
      "Epoch: [9/200], Step: [9/19], Loss: 4.5919\n",
      "Epoch: [9/200], Step: [10/19], Loss: 4.5857\n",
      "Epoch: [9/200], Step: [11/19], Loss: 4.6088\n",
      "Epoch: [9/200], Step: [12/19], Loss: 4.6022\n",
      "Epoch: [9/200], Step: [13/19], Loss: 4.6424\n",
      "Epoch: [9/200], Step: [14/19], Loss: 4.5988\n",
      "Epoch: [9/200], Step: [15/19], Loss: 4.6138\n",
      "Epoch: [9/200], Step: [16/19], Loss: 4.6139\n",
      "Epoch: [9/200], Step: [17/19], Loss: 4.6446\n",
      "Epoch: [9/200], Step: [18/19], Loss: 4.6114\n",
      "Epoch: [10/200], Step: [1/19], Loss: 4.5997\n",
      "Epoch: [10/200], Step: [2/19], Loss: 4.6230\n",
      "Epoch: [10/200], Step: [3/19], Loss: 4.6002\n",
      "Epoch: [10/200], Step: [4/19], Loss: 4.6184\n",
      "Epoch: [10/200], Step: [5/19], Loss: 4.5885\n",
      "Epoch: [10/200], Step: [6/19], Loss: 4.6161\n",
      "Epoch: [10/200], Step: [7/19], Loss: 4.6232\n",
      "Epoch: [10/200], Step: [8/19], Loss: 4.6071\n",
      "Epoch: [10/200], Step: [9/19], Loss: 4.5938\n",
      "Epoch: [10/200], Step: [10/19], Loss: 4.6239\n",
      "Epoch: [10/200], Step: [11/19], Loss: 4.6216\n",
      "Epoch: [10/200], Step: [12/19], Loss: 4.6069\n",
      "Epoch: [10/200], Step: [13/19], Loss: 4.6077\n",
      "Epoch: [10/200], Step: [14/19], Loss: 4.6183\n",
      "Epoch: [10/200], Step: [15/19], Loss: 4.6165\n",
      "Epoch: [10/200], Step: [16/19], Loss: 4.6172\n",
      "Epoch: [10/200], Step: [17/19], Loss: 4.5983\n",
      "Epoch: [10/200], Step: [18/19], Loss: 4.5863\n",
      "Epoch: [11/200], Step: [1/19], Loss: 4.5909\n",
      "Epoch: [11/200], Step: [2/19], Loss: 4.6173\n",
      "Epoch: [11/200], Step: [3/19], Loss: 4.6006\n",
      "Epoch: [11/200], Step: [4/19], Loss: 4.6058\n",
      "Epoch: [11/200], Step: [5/19], Loss: 4.6224\n",
      "Epoch: [11/200], Step: [6/19], Loss: 4.6143\n",
      "Epoch: [11/200], Step: [7/19], Loss: 4.6050\n",
      "Epoch: [11/200], Step: [8/19], Loss: 4.6077\n",
      "Epoch: [11/200], Step: [9/19], Loss: 4.6139\n",
      "Epoch: [11/200], Step: [10/19], Loss: 4.6070\n",
      "Epoch: [11/200], Step: [11/19], Loss: 4.6136\n",
      "Epoch: [11/200], Step: [12/19], Loss: 4.5977\n",
      "Epoch: [11/200], Step: [13/19], Loss: 4.6185\n",
      "Epoch: [11/200], Step: [14/19], Loss: 4.6300\n",
      "Epoch: [11/200], Step: [15/19], Loss: 4.6068\n",
      "Epoch: [11/200], Step: [16/19], Loss: 4.6133\n",
      "Epoch: [11/200], Step: [17/19], Loss: 4.5977\n",
      "Epoch: [11/200], Step: [18/19], Loss: 4.5999\n",
      "Epoch: [12/200], Step: [1/19], Loss: 4.6067\n",
      "Epoch: [12/200], Step: [2/19], Loss: 4.6015\n",
      "Epoch: [12/200], Step: [3/19], Loss: 4.6047\n",
      "Epoch: [12/200], Step: [4/19], Loss: 4.6178\n",
      "Epoch: [12/200], Step: [5/19], Loss: 4.6047\n",
      "Epoch: [12/200], Step: [6/19], Loss: 4.6059\n",
      "Epoch: [12/200], Step: [7/19], Loss: 4.6097\n",
      "Epoch: [12/200], Step: [8/19], Loss: 4.6090\n",
      "Epoch: [12/200], Step: [9/19], Loss: 4.5953\n",
      "Epoch: [12/200], Step: [10/19], Loss: 4.6062\n",
      "Epoch: [12/200], Step: [11/19], Loss: 4.6096\n",
      "Epoch: [12/200], Step: [12/19], Loss: 4.6156\n",
      "Epoch: [12/200], Step: [13/19], Loss: 4.6257\n",
      "Epoch: [12/200], Step: [14/19], Loss: 4.6087\n",
      "Epoch: [12/200], Step: [15/19], Loss: 4.6149\n",
      "Epoch: [12/200], Step: [16/19], Loss: 4.5846\n",
      "Epoch: [12/200], Step: [17/19], Loss: 4.6065\n",
      "Epoch: [12/200], Step: [18/19], Loss: 4.6304\n",
      "Epoch: [13/200], Step: [1/19], Loss: 4.6081\n",
      "Epoch: [13/200], Step: [2/19], Loss: 4.6025\n",
      "Epoch: [13/200], Step: [3/19], Loss: 4.6183\n",
      "Epoch: [13/200], Step: [4/19], Loss: 4.6010\n",
      "Epoch: [13/200], Step: [5/19], Loss: 4.5938\n",
      "Epoch: [13/200], Step: [6/19], Loss: 4.5846\n",
      "Epoch: [13/200], Step: [7/19], Loss: 4.6006\n",
      "Epoch: [13/200], Step: [8/19], Loss: 4.6091\n",
      "Epoch: [13/200], Step: [9/19], Loss: 4.6033\n",
      "Epoch: [13/200], Step: [10/19], Loss: 4.6108\n",
      "Epoch: [13/200], Step: [11/19], Loss: 4.6389\n",
      "Epoch: [13/200], Step: [12/19], Loss: 4.6271\n",
      "Epoch: [13/200], Step: [13/19], Loss: 4.6010\n",
      "Epoch: [13/200], Step: [14/19], Loss: 4.6176\n",
      "Epoch: [13/200], Step: [15/19], Loss: 4.5871\n",
      "Epoch: [13/200], Step: [16/19], Loss: 4.6286\n",
      "Epoch: [13/200], Step: [17/19], Loss: 4.6037\n",
      "Epoch: [13/200], Step: [18/19], Loss: 4.6166\n",
      "Epoch: [14/200], Step: [1/19], Loss: 4.5997\n",
      "Epoch: [14/200], Step: [2/19], Loss: 4.6066\n",
      "Epoch: [14/200], Step: [3/19], Loss: 4.6235\n",
      "Epoch: [14/200], Step: [4/19], Loss: 4.6027\n",
      "Epoch: [14/200], Step: [5/19], Loss: 4.6060\n",
      "Epoch: [14/200], Step: [6/19], Loss: 4.6152\n",
      "Epoch: [14/200], Step: [7/19], Loss: 4.6147\n",
      "Epoch: [14/200], Step: [8/19], Loss: 4.5912\n",
      "Epoch: [14/200], Step: [9/19], Loss: 4.6203\n",
      "Epoch: [14/200], Step: [10/19], Loss: 4.6117\n",
      "Epoch: [14/200], Step: [11/19], Loss: 4.6063\n",
      "Epoch: [14/200], Step: [12/19], Loss: 4.5933\n",
      "Epoch: [14/200], Step: [13/19], Loss: 4.6433\n",
      "Epoch: [14/200], Step: [14/19], Loss: 4.6092\n",
      "Epoch: [14/200], Step: [15/19], Loss: 4.6092\n",
      "Epoch: [14/200], Step: [16/19], Loss: 4.6025\n",
      "Epoch: [14/200], Step: [17/19], Loss: 4.5992\n",
      "Epoch: [14/200], Step: [18/19], Loss: 4.5932\n",
      "Epoch: [15/200], Step: [1/19], Loss: 4.6438\n",
      "Epoch: [15/200], Step: [2/19], Loss: 4.6265\n",
      "Epoch: [15/200], Step: [3/19], Loss: 4.5997\n",
      "Epoch: [15/200], Step: [4/19], Loss: 4.6030\n",
      "Epoch: [15/200], Step: [5/19], Loss: 4.6178\n",
      "Epoch: [15/200], Step: [6/19], Loss: 4.5895\n",
      "Epoch: [15/200], Step: [7/19], Loss: 4.6063\n",
      "Epoch: [15/200], Step: [8/19], Loss: 4.5958\n",
      "Epoch: [15/200], Step: [9/19], Loss: 4.5962\n",
      "Epoch: [15/200], Step: [10/19], Loss: 4.6032\n",
      "Epoch: [15/200], Step: [11/19], Loss: 4.5917\n",
      "Epoch: [15/200], Step: [12/19], Loss: 4.6123\n",
      "Epoch: [15/200], Step: [13/19], Loss: 4.6124\n",
      "Epoch: [15/200], Step: [14/19], Loss: 4.5932\n",
      "Epoch: [15/200], Step: [15/19], Loss: 4.6110\n",
      "Epoch: [15/200], Step: [16/19], Loss: 4.6140\n",
      "Epoch: [15/200], Step: [17/19], Loss: 4.6141\n",
      "Epoch: [15/200], Step: [18/19], Loss: 4.6127\n",
      "Epoch: [16/200], Step: [1/19], Loss: 4.6095\n",
      "Epoch: [16/200], Step: [2/19], Loss: 4.6253\n",
      "Epoch: [16/200], Step: [3/19], Loss: 4.5773\n",
      "Epoch: [16/200], Step: [4/19], Loss: 4.5987\n",
      "Epoch: [16/200], Step: [5/19], Loss: 4.6154\n",
      "Epoch: [16/200], Step: [6/19], Loss: 4.6114\n",
      "Epoch: [16/200], Step: [7/19], Loss: 4.6071\n",
      "Epoch: [16/200], Step: [8/19], Loss: 4.5984\n",
      "Epoch: [16/200], Step: [9/19], Loss: 4.6100\n",
      "Epoch: [16/200], Step: [10/19], Loss: 4.6016\n",
      "Epoch: [16/200], Step: [11/19], Loss: 4.5959\n",
      "Epoch: [16/200], Step: [12/19], Loss: 4.6170\n",
      "Epoch: [16/200], Step: [13/19], Loss: 4.6167\n",
      "Epoch: [16/200], Step: [14/19], Loss: 4.6127\n",
      "Epoch: [16/200], Step: [15/19], Loss: 4.6108\n",
      "Epoch: [16/200], Step: [16/19], Loss: 4.6168\n",
      "Epoch: [16/200], Step: [17/19], Loss: 4.6135\n",
      "Epoch: [16/200], Step: [18/19], Loss: 4.6004\n",
      "Epoch: [17/200], Step: [1/19], Loss: 4.5858\n",
      "Epoch: [17/200], Step: [2/19], Loss: 4.6000\n",
      "Epoch: [17/200], Step: [3/19], Loss: 4.6023\n",
      "Epoch: [17/200], Step: [4/19], Loss: 4.6168\n",
      "Epoch: [17/200], Step: [5/19], Loss: 4.6212\n",
      "Epoch: [17/200], Step: [6/19], Loss: 4.6211\n",
      "Epoch: [17/200], Step: [7/19], Loss: 4.6104\n",
      "Epoch: [17/200], Step: [8/19], Loss: 4.6090\n",
      "Epoch: [17/200], Step: [9/19], Loss: 4.6137\n",
      "Epoch: [17/200], Step: [10/19], Loss: 4.6212\n",
      "Epoch: [17/200], Step: [11/19], Loss: 4.6125\n",
      "Epoch: [17/200], Step: [12/19], Loss: 4.5926\n",
      "Epoch: [17/200], Step: [13/19], Loss: 4.6234\n",
      "Epoch: [17/200], Step: [14/19], Loss: 4.6001\n",
      "Epoch: [17/200], Step: [15/19], Loss: 4.5924\n",
      "Epoch: [17/200], Step: [16/19], Loss: 4.5879\n",
      "Epoch: [17/200], Step: [17/19], Loss: 4.5978\n",
      "Epoch: [17/200], Step: [18/19], Loss: 4.6255\n",
      "Epoch: [18/200], Step: [1/19], Loss: 4.6063\n",
      "Epoch: [18/200], Step: [2/19], Loss: 4.6123\n",
      "Epoch: [18/200], Step: [3/19], Loss: 4.6168\n",
      "Epoch: [18/200], Step: [4/19], Loss: 4.6126\n",
      "Epoch: [18/200], Step: [5/19], Loss: 4.6136\n",
      "Epoch: [18/200], Step: [6/19], Loss: 4.6114\n",
      "Epoch: [18/200], Step: [7/19], Loss: 4.6071\n",
      "Epoch: [18/200], Step: [8/19], Loss: 4.5906\n",
      "Epoch: [18/200], Step: [9/19], Loss: 4.6111\n",
      "Epoch: [18/200], Step: [10/19], Loss: 4.5946\n",
      "Epoch: [18/200], Step: [11/19], Loss: 4.5965\n",
      "Epoch: [18/200], Step: [12/19], Loss: 4.6085\n",
      "Epoch: [18/200], Step: [13/19], Loss: 4.6150\n",
      "Epoch: [18/200], Step: [14/19], Loss: 4.5995\n",
      "Epoch: [18/200], Step: [15/19], Loss: 4.5976\n",
      "Epoch: [18/200], Step: [16/19], Loss: 4.6072\n",
      "Epoch: [18/200], Step: [17/19], Loss: 4.6268\n",
      "Epoch: [18/200], Step: [18/19], Loss: 4.6019\n",
      "Epoch: [19/200], Step: [1/19], Loss: 4.6002\n",
      "Epoch: [19/200], Step: [2/19], Loss: 4.5978\n",
      "Epoch: [19/200], Step: [3/19], Loss: 4.6187\n",
      "Epoch: [19/200], Step: [4/19], Loss: 4.6213\n",
      "Epoch: [19/200], Step: [5/19], Loss: 4.6025\n",
      "Epoch: [19/200], Step: [6/19], Loss: 4.6154\n",
      "Epoch: [19/200], Step: [7/19], Loss: 4.5925\n",
      "Epoch: [19/200], Step: [8/19], Loss: 4.5986\n",
      "Epoch: [19/200], Step: [9/19], Loss: 4.6040\n",
      "Epoch: [19/200], Step: [10/19], Loss: 4.5949\n",
      "Epoch: [19/200], Step: [11/19], Loss: 4.6094\n",
      "Epoch: [19/200], Step: [12/19], Loss: 4.6001\n",
      "Epoch: [19/200], Step: [13/19], Loss: 4.6193\n",
      "Epoch: [19/200], Step: [14/19], Loss: 4.6041\n",
      "Epoch: [19/200], Step: [15/19], Loss: 4.6165\n",
      "Epoch: [19/200], Step: [16/19], Loss: 4.5997\n",
      "Epoch: [19/200], Step: [17/19], Loss: 4.6222\n",
      "Epoch: [19/200], Step: [18/19], Loss: 4.6074\n",
      "Epoch: [20/200], Step: [1/19], Loss: 4.6179\n",
      "Epoch: [20/200], Step: [2/19], Loss: 4.6053\n",
      "Epoch: [20/200], Step: [3/19], Loss: 4.6080\n",
      "Epoch: [20/200], Step: [4/19], Loss: 4.5905\n",
      "Epoch: [20/200], Step: [5/19], Loss: 4.6070\n",
      "Epoch: [20/200], Step: [6/19], Loss: 4.6043\n",
      "Epoch: [20/200], Step: [7/19], Loss: 4.5991\n",
      "Epoch: [20/200], Step: [8/19], Loss: 4.6110\n",
      "Epoch: [20/200], Step: [9/19], Loss: 4.6088\n",
      "Epoch: [20/200], Step: [10/19], Loss: 4.5921\n",
      "Epoch: [20/200], Step: [11/19], Loss: 4.6200\n",
      "Epoch: [20/200], Step: [12/19], Loss: 4.6192\n",
      "Epoch: [20/200], Step: [13/19], Loss: 4.6134\n",
      "Epoch: [20/200], Step: [14/19], Loss: 4.6072\n",
      "Epoch: [20/200], Step: [15/19], Loss: 4.6123\n",
      "Epoch: [20/200], Step: [16/19], Loss: 4.5960\n",
      "Epoch: [20/200], Step: [17/19], Loss: 4.6122\n",
      "Epoch: [20/200], Step: [18/19], Loss: 4.5960\n",
      "Epoch: [21/200], Step: [1/19], Loss: 4.6116\n",
      "Epoch: [21/200], Step: [2/19], Loss: 4.6199\n",
      "Epoch: [21/200], Step: [3/19], Loss: 4.5842\n",
      "Epoch: [21/200], Step: [4/19], Loss: 4.5829\n",
      "Epoch: [21/200], Step: [5/19], Loss: 4.6231\n",
      "Epoch: [21/200], Step: [6/19], Loss: 4.6115\n",
      "Epoch: [21/200], Step: [7/19], Loss: 4.6119\n",
      "Epoch: [21/200], Step: [8/19], Loss: 4.6143\n",
      "Epoch: [21/200], Step: [9/19], Loss: 4.5912\n",
      "Epoch: [21/200], Step: [10/19], Loss: 4.5902\n",
      "Epoch: [21/200], Step: [11/19], Loss: 4.6187\n",
      "Epoch: [21/200], Step: [12/19], Loss: 4.6022\n",
      "Epoch: [21/200], Step: [13/19], Loss: 4.6091\n",
      "Epoch: [21/200], Step: [14/19], Loss: 4.6065\n",
      "Epoch: [21/200], Step: [15/19], Loss: 4.6215\n",
      "Epoch: [21/200], Step: [16/19], Loss: 4.5915\n",
      "Epoch: [21/200], Step: [17/19], Loss: 4.6203\n",
      "Epoch: [21/200], Step: [18/19], Loss: 4.6048\n",
      "Epoch: [22/200], Step: [1/19], Loss: 4.6048\n",
      "Epoch: [22/200], Step: [2/19], Loss: 4.6223\n",
      "Epoch: [22/200], Step: [3/19], Loss: 4.6048\n",
      "Epoch: [22/200], Step: [4/19], Loss: 4.5970\n",
      "Epoch: [22/200], Step: [5/19], Loss: 4.6149\n",
      "Epoch: [22/200], Step: [6/19], Loss: 4.5893\n",
      "Epoch: [22/200], Step: [7/19], Loss: 4.6160\n",
      "Epoch: [22/200], Step: [8/19], Loss: 4.6222\n",
      "Epoch: [22/200], Step: [9/19], Loss: 4.6251\n",
      "Epoch: [22/200], Step: [10/19], Loss: 4.5953\n",
      "Epoch: [22/200], Step: [11/19], Loss: 4.6147\n",
      "Epoch: [22/200], Step: [12/19], Loss: 4.5965\n",
      "Epoch: [22/200], Step: [13/19], Loss: 4.5924\n",
      "Epoch: [22/200], Step: [14/19], Loss: 4.5922\n",
      "Epoch: [22/200], Step: [15/19], Loss: 4.6126\n",
      "Epoch: [22/200], Step: [16/19], Loss: 4.6069\n",
      "Epoch: [22/200], Step: [17/19], Loss: 4.6085\n",
      "Epoch: [22/200], Step: [18/19], Loss: 4.5956\n",
      "Epoch: [23/200], Step: [1/19], Loss: 4.6280\n",
      "Epoch: [23/200], Step: [2/19], Loss: 4.5961\n",
      "Epoch: [23/200], Step: [3/19], Loss: 4.5954\n",
      "Epoch: [23/200], Step: [4/19], Loss: 4.6165\n",
      "Epoch: [23/200], Step: [5/19], Loss: 4.6049\n",
      "Epoch: [23/200], Step: [6/19], Loss: 4.5872\n",
      "Epoch: [23/200], Step: [7/19], Loss: 4.6155\n",
      "Epoch: [23/200], Step: [8/19], Loss: 4.6019\n",
      "Epoch: [23/200], Step: [9/19], Loss: 4.6283\n",
      "Epoch: [23/200], Step: [10/19], Loss: 4.5970\n",
      "Epoch: [23/200], Step: [11/19], Loss: 4.6071\n",
      "Epoch: [23/200], Step: [12/19], Loss: 4.6097\n",
      "Epoch: [23/200], Step: [13/19], Loss: 4.6024\n",
      "Epoch: [23/200], Step: [14/19], Loss: 4.5983\n",
      "Epoch: [23/200], Step: [15/19], Loss: 4.6023\n",
      "Epoch: [23/200], Step: [16/19], Loss: 4.6131\n",
      "Epoch: [23/200], Step: [17/19], Loss: 4.5957\n",
      "Epoch: [23/200], Step: [18/19], Loss: 4.6072\n",
      "Epoch: [24/200], Step: [1/19], Loss: 4.6152\n",
      "Epoch: [24/200], Step: [2/19], Loss: 4.5911\n",
      "Epoch: [24/200], Step: [3/19], Loss: 4.6051\n",
      "Epoch: [24/200], Step: [4/19], Loss: 4.5995\n",
      "Epoch: [24/200], Step: [5/19], Loss: 4.6002\n",
      "Epoch: [24/200], Step: [6/19], Loss: 4.6120\n",
      "Epoch: [24/200], Step: [7/19], Loss: 4.6039\n",
      "Epoch: [24/200], Step: [8/19], Loss: 4.6199\n",
      "Epoch: [24/200], Step: [9/19], Loss: 4.5961\n",
      "Epoch: [24/200], Step: [10/19], Loss: 4.6219\n",
      "Epoch: [24/200], Step: [11/19], Loss: 4.6011\n",
      "Epoch: [24/200], Step: [12/19], Loss: 4.6048\n",
      "Epoch: [24/200], Step: [13/19], Loss: 4.6040\n",
      "Epoch: [24/200], Step: [14/19], Loss: 4.6050\n",
      "Epoch: [24/200], Step: [15/19], Loss: 4.5919\n",
      "Epoch: [24/200], Step: [16/19], Loss: 4.6119\n",
      "Epoch: [24/200], Step: [17/19], Loss: 4.6060\n",
      "Epoch: [24/200], Step: [18/19], Loss: 4.6124\n",
      "Epoch: [25/200], Step: [1/19], Loss: 4.6029\n",
      "Epoch: [25/200], Step: [2/19], Loss: 4.6017\n",
      "Epoch: [25/200], Step: [3/19], Loss: 4.6196\n",
      "Epoch: [25/200], Step: [4/19], Loss: 4.6103\n",
      "Epoch: [25/200], Step: [5/19], Loss: 4.6168\n",
      "Epoch: [25/200], Step: [6/19], Loss: 4.6225\n",
      "Epoch: [25/200], Step: [7/19], Loss: 4.6333\n",
      "Epoch: [25/200], Step: [8/19], Loss: 4.5969\n",
      "Epoch: [25/200], Step: [9/19], Loss: 4.6026\n",
      "Epoch: [25/200], Step: [10/19], Loss: 4.6002\n",
      "Epoch: [25/200], Step: [11/19], Loss: 4.6002\n",
      "Epoch: [25/200], Step: [12/19], Loss: 4.5994\n",
      "Epoch: [25/200], Step: [13/19], Loss: 4.6018\n",
      "Epoch: [25/200], Step: [14/19], Loss: 4.5998\n",
      "Epoch: [25/200], Step: [15/19], Loss: 4.5983\n",
      "Epoch: [25/200], Step: [16/19], Loss: 4.5951\n",
      "Epoch: [25/200], Step: [17/19], Loss: 4.6079\n",
      "Epoch: [25/200], Step: [18/19], Loss: 4.5880\n",
      "Epoch: [26/200], Step: [1/19], Loss: 4.6061\n",
      "Epoch: [26/200], Step: [2/19], Loss: 4.5979\n",
      "Epoch: [26/200], Step: [3/19], Loss: 4.5887\n",
      "Epoch: [26/200], Step: [4/19], Loss: 4.6024\n",
      "Epoch: [26/200], Step: [5/19], Loss: 4.6143\n",
      "Epoch: [26/200], Step: [6/19], Loss: 4.6163\n",
      "Epoch: [26/200], Step: [7/19], Loss: 4.6217\n",
      "Epoch: [26/200], Step: [8/19], Loss: 4.5975\n",
      "Epoch: [26/200], Step: [9/19], Loss: 4.6128\n",
      "Epoch: [26/200], Step: [10/19], Loss: 4.6091\n",
      "Epoch: [26/200], Step: [11/19], Loss: 4.6101\n",
      "Epoch: [26/200], Step: [12/19], Loss: 4.6216\n",
      "Epoch: [26/200], Step: [13/19], Loss: 4.5971\n",
      "Epoch: [26/200], Step: [14/19], Loss: 4.6003\n",
      "Epoch: [26/200], Step: [15/19], Loss: 4.6026\n",
      "Epoch: [26/200], Step: [16/19], Loss: 4.5797\n",
      "Epoch: [26/200], Step: [17/19], Loss: 4.6079\n",
      "Epoch: [26/200], Step: [18/19], Loss: 4.6069\n",
      "Epoch: [27/200], Step: [1/19], Loss: 4.5868\n",
      "Epoch: [27/200], Step: [2/19], Loss: 4.5910\n",
      "Epoch: [27/200], Step: [3/19], Loss: 4.6011\n",
      "Epoch: [27/200], Step: [4/19], Loss: 4.5945\n",
      "Epoch: [27/200], Step: [5/19], Loss: 4.6280\n",
      "Epoch: [27/200], Step: [6/19], Loss: 4.6089\n",
      "Epoch: [27/200], Step: [7/19], Loss: 4.6283\n",
      "Epoch: [27/200], Step: [8/19], Loss: 4.6082\n",
      "Epoch: [27/200], Step: [9/19], Loss: 4.6017\n",
      "Epoch: [27/200], Step: [10/19], Loss: 4.5757\n",
      "Epoch: [27/200], Step: [11/19], Loss: 4.5924\n",
      "Epoch: [27/200], Step: [12/19], Loss: 4.5905\n",
      "Epoch: [27/200], Step: [13/19], Loss: 4.6091\n",
      "Epoch: [27/200], Step: [14/19], Loss: 4.6104\n",
      "Epoch: [27/200], Step: [15/19], Loss: 4.6130\n",
      "Epoch: [27/200], Step: [16/19], Loss: 4.6335\n",
      "Epoch: [27/200], Step: [17/19], Loss: 4.6002\n",
      "Epoch: [27/200], Step: [18/19], Loss: 4.6152\n",
      "Epoch: [28/200], Step: [1/19], Loss: 4.5942\n",
      "Epoch: [28/200], Step: [2/19], Loss: 4.6078\n",
      "Epoch: [28/200], Step: [3/19], Loss: 4.5995\n",
      "Epoch: [28/200], Step: [4/19], Loss: 4.6155\n",
      "Epoch: [28/200], Step: [5/19], Loss: 4.6183\n",
      "Epoch: [28/200], Step: [6/19], Loss: 4.6139\n",
      "Epoch: [28/200], Step: [7/19], Loss: 4.6225\n",
      "Epoch: [28/200], Step: [8/19], Loss: 4.5960\n",
      "Epoch: [28/200], Step: [9/19], Loss: 4.6100\n",
      "Epoch: [28/200], Step: [10/19], Loss: 4.6162\n",
      "Epoch: [28/200], Step: [11/19], Loss: 4.5855\n",
      "Epoch: [28/200], Step: [12/19], Loss: 4.6081\n",
      "Epoch: [28/200], Step: [13/19], Loss: 4.5918\n",
      "Epoch: [28/200], Step: [14/19], Loss: 4.5917\n",
      "Epoch: [28/200], Step: [15/19], Loss: 4.5927\n",
      "Epoch: [28/200], Step: [16/19], Loss: 4.6107\n",
      "Epoch: [28/200], Step: [17/19], Loss: 4.6033\n",
      "Epoch: [28/200], Step: [18/19], Loss: 4.6063\n",
      "Epoch: [29/200], Step: [1/19], Loss: 4.6186\n",
      "Epoch: [29/200], Step: [2/19], Loss: 4.6099\n",
      "Epoch: [29/200], Step: [3/19], Loss: 4.6236\n",
      "Epoch: [29/200], Step: [4/19], Loss: 4.5847\n",
      "Epoch: [29/200], Step: [5/19], Loss: 4.6137\n",
      "Epoch: [29/200], Step: [6/19], Loss: 4.6182\n",
      "Epoch: [29/200], Step: [7/19], Loss: 4.5991\n",
      "Epoch: [29/200], Step: [8/19], Loss: 4.6081\n",
      "Epoch: [29/200], Step: [9/19], Loss: 4.5942\n",
      "Epoch: [29/200], Step: [10/19], Loss: 4.6010\n",
      "Epoch: [29/200], Step: [11/19], Loss: 4.5878\n",
      "Epoch: [29/200], Step: [12/19], Loss: 4.5981\n",
      "Epoch: [29/200], Step: [13/19], Loss: 4.5924\n",
      "Epoch: [29/200], Step: [14/19], Loss: 4.6120\n",
      "Epoch: [29/200], Step: [15/19], Loss: 4.6077\n",
      "Epoch: [29/200], Step: [16/19], Loss: 4.6204\n",
      "Epoch: [29/200], Step: [17/19], Loss: 4.6010\n",
      "Epoch: [29/200], Step: [18/19], Loss: 4.5891\n",
      "Epoch: [30/200], Step: [1/19], Loss: 4.6138\n",
      "Epoch: [30/200], Step: [2/19], Loss: 4.5854\n",
      "Epoch: [30/200], Step: [3/19], Loss: 4.5851\n",
      "Epoch: [30/200], Step: [4/19], Loss: 4.6050\n",
      "Epoch: [30/200], Step: [5/19], Loss: 4.5955\n",
      "Epoch: [30/200], Step: [6/19], Loss: 4.5908\n",
      "Epoch: [30/200], Step: [7/19], Loss: 4.5934\n",
      "Epoch: [30/200], Step: [8/19], Loss: 4.5990\n",
      "Epoch: [30/200], Step: [9/19], Loss: 4.6194\n",
      "Epoch: [30/200], Step: [10/19], Loss: 4.6113\n",
      "Epoch: [30/200], Step: [11/19], Loss: 4.6103\n",
      "Epoch: [30/200], Step: [12/19], Loss: 4.6068\n",
      "Epoch: [30/200], Step: [13/19], Loss: 4.6100\n",
      "Epoch: [30/200], Step: [14/19], Loss: 4.6197\n",
      "Epoch: [30/200], Step: [15/19], Loss: 4.6030\n",
      "Epoch: [30/200], Step: [16/19], Loss: 4.6097\n",
      "Epoch: [30/200], Step: [17/19], Loss: 4.6095\n",
      "Epoch: [30/200], Step: [18/19], Loss: 4.6072\n",
      "Epoch: [31/200], Step: [1/19], Loss: 4.6158\n",
      "Epoch: [31/200], Step: [2/19], Loss: 4.6041\n",
      "Epoch: [31/200], Step: [3/19], Loss: 4.6140\n",
      "Epoch: [31/200], Step: [4/19], Loss: 4.6069\n",
      "Epoch: [31/200], Step: [5/19], Loss: 4.5993\n",
      "Epoch: [31/200], Step: [6/19], Loss: 4.6022\n",
      "Epoch: [31/200], Step: [7/19], Loss: 4.5901\n",
      "Epoch: [31/200], Step: [8/19], Loss: 4.5895\n",
      "Epoch: [31/200], Step: [9/19], Loss: 4.6014\n",
      "Epoch: [31/200], Step: [10/19], Loss: 4.5916\n",
      "Epoch: [31/200], Step: [11/19], Loss: 4.6085\n",
      "Epoch: [31/200], Step: [12/19], Loss: 4.6140\n",
      "Epoch: [31/200], Step: [13/19], Loss: 4.6141\n",
      "Epoch: [31/200], Step: [14/19], Loss: 4.5878\n",
      "Epoch: [31/200], Step: [15/19], Loss: 4.6000\n",
      "Epoch: [31/200], Step: [16/19], Loss: 4.6062\n",
      "Epoch: [31/200], Step: [17/19], Loss: 4.5986\n",
      "Epoch: [31/200], Step: [18/19], Loss: 4.6266\n",
      "Epoch: [32/200], Step: [1/19], Loss: 4.5962\n",
      "Epoch: [32/200], Step: [2/19], Loss: 4.6224\n",
      "Epoch: [32/200], Step: [3/19], Loss: 4.5947\n",
      "Epoch: [32/200], Step: [4/19], Loss: 4.5929\n",
      "Epoch: [32/200], Step: [5/19], Loss: 4.6293\n",
      "Epoch: [32/200], Step: [6/19], Loss: 4.6189\n",
      "Epoch: [32/200], Step: [7/19], Loss: 4.5864\n",
      "Epoch: [32/200], Step: [8/19], Loss: 4.5976\n",
      "Epoch: [32/200], Step: [9/19], Loss: 4.6080\n",
      "Epoch: [32/200], Step: [10/19], Loss: 4.5832\n",
      "Epoch: [32/200], Step: [11/19], Loss: 4.6048\n",
      "Epoch: [32/200], Step: [12/19], Loss: 4.6140\n",
      "Epoch: [32/200], Step: [13/19], Loss: 4.5881\n",
      "Epoch: [32/200], Step: [14/19], Loss: 4.5915\n",
      "Epoch: [32/200], Step: [15/19], Loss: 4.6113\n",
      "Epoch: [32/200], Step: [16/19], Loss: 4.5991\n",
      "Epoch: [32/200], Step: [17/19], Loss: 4.6126\n",
      "Epoch: [32/200], Step: [18/19], Loss: 4.6152\n",
      "Epoch: [33/200], Step: [1/19], Loss: 4.5999\n",
      "Epoch: [33/200], Step: [2/19], Loss: 4.5950\n",
      "Epoch: [33/200], Step: [3/19], Loss: 4.6165\n",
      "Epoch: [33/200], Step: [4/19], Loss: 4.6094\n",
      "Epoch: [33/200], Step: [5/19], Loss: 4.6097\n",
      "Epoch: [33/200], Step: [6/19], Loss: 4.6235\n",
      "Epoch: [33/200], Step: [7/19], Loss: 4.6084\n",
      "Epoch: [33/200], Step: [8/19], Loss: 4.5854\n",
      "Epoch: [33/200], Step: [9/19], Loss: 4.6088\n",
      "Epoch: [33/200], Step: [10/19], Loss: 4.6028\n",
      "Epoch: [33/200], Step: [11/19], Loss: 4.6118\n",
      "Epoch: [33/200], Step: [12/19], Loss: 4.5819\n",
      "Epoch: [33/200], Step: [13/19], Loss: 4.5824\n",
      "Epoch: [33/200], Step: [14/19], Loss: 4.6059\n",
      "Epoch: [33/200], Step: [15/19], Loss: 4.6059\n",
      "Epoch: [33/200], Step: [16/19], Loss: 4.5990\n",
      "Epoch: [33/200], Step: [17/19], Loss: 4.5965\n",
      "Epoch: [33/200], Step: [18/19], Loss: 4.6190\n",
      "Epoch: [34/200], Step: [1/19], Loss: 4.6008\n",
      "Epoch: [34/200], Step: [2/19], Loss: 4.5874\n",
      "Epoch: [34/200], Step: [3/19], Loss: 4.6069\n",
      "Epoch: [34/200], Step: [4/19], Loss: 4.6084\n",
      "Epoch: [34/200], Step: [5/19], Loss: 4.6133\n",
      "Epoch: [34/200], Step: [6/19], Loss: 4.5768\n",
      "Epoch: [34/200], Step: [7/19], Loss: 4.6051\n",
      "Epoch: [34/200], Step: [8/19], Loss: 4.6075\n",
      "Epoch: [34/200], Step: [9/19], Loss: 4.5983\n",
      "Epoch: [34/200], Step: [10/19], Loss: 4.6030\n",
      "Epoch: [34/200], Step: [11/19], Loss: 4.6048\n",
      "Epoch: [34/200], Step: [12/19], Loss: 4.6132\n",
      "Epoch: [34/200], Step: [13/19], Loss: 4.6068\n",
      "Epoch: [34/200], Step: [14/19], Loss: 4.6005\n",
      "Epoch: [34/200], Step: [15/19], Loss: 4.6103\n",
      "Epoch: [34/200], Step: [16/19], Loss: 4.5951\n",
      "Epoch: [34/200], Step: [17/19], Loss: 4.6114\n",
      "Epoch: [34/200], Step: [18/19], Loss: 4.6077\n",
      "Epoch: [35/200], Step: [1/19], Loss: 4.6058\n",
      "Epoch: [35/200], Step: [2/19], Loss: 4.6170\n",
      "Epoch: [35/200], Step: [3/19], Loss: 4.5858\n",
      "Epoch: [35/200], Step: [4/19], Loss: 4.5929\n",
      "Epoch: [35/200], Step: [5/19], Loss: 4.5979\n",
      "Epoch: [35/200], Step: [6/19], Loss: 4.6109\n",
      "Epoch: [35/200], Step: [7/19], Loss: 4.6081\n",
      "Epoch: [35/200], Step: [8/19], Loss: 4.6175\n",
      "Epoch: [35/200], Step: [9/19], Loss: 4.6141\n",
      "Epoch: [35/200], Step: [10/19], Loss: 4.6073\n",
      "Epoch: [35/200], Step: [11/19], Loss: 4.6081\n",
      "Epoch: [35/200], Step: [12/19], Loss: 4.5911\n",
      "Epoch: [35/200], Step: [13/19], Loss: 4.5866\n",
      "Epoch: [35/200], Step: [14/19], Loss: 4.6164\n",
      "Epoch: [35/200], Step: [15/19], Loss: 4.6148\n",
      "Epoch: [35/200], Step: [16/19], Loss: 4.5984\n",
      "Epoch: [35/200], Step: [17/19], Loss: 4.6033\n",
      "Epoch: [35/200], Step: [18/19], Loss: 4.5773\n",
      "Epoch: [36/200], Step: [1/19], Loss: 4.6007\n",
      "Epoch: [36/200], Step: [2/19], Loss: 4.5990\n",
      "Epoch: [36/200], Step: [3/19], Loss: 4.5964\n",
      "Epoch: [36/200], Step: [4/19], Loss: 4.5887\n",
      "Epoch: [36/200], Step: [5/19], Loss: 4.6134\n",
      "Epoch: [36/200], Step: [6/19], Loss: 4.5975\n",
      "Epoch: [36/200], Step: [7/19], Loss: 4.6058\n",
      "Epoch: [36/200], Step: [8/19], Loss: 4.6185\n",
      "Epoch: [36/200], Step: [9/19], Loss: 4.6142\n",
      "Epoch: [36/200], Step: [10/19], Loss: 4.6124\n",
      "Epoch: [36/200], Step: [11/19], Loss: 4.5929\n",
      "Epoch: [36/200], Step: [12/19], Loss: 4.5975\n",
      "Epoch: [36/200], Step: [13/19], Loss: 4.5979\n",
      "Epoch: [36/200], Step: [14/19], Loss: 4.6022\n",
      "Epoch: [36/200], Step: [15/19], Loss: 4.5978\n",
      "Epoch: [36/200], Step: [16/19], Loss: 4.5911\n",
      "Epoch: [36/200], Step: [17/19], Loss: 4.5935\n",
      "Epoch: [36/200], Step: [18/19], Loss: 4.6294\n",
      "Epoch: [37/200], Step: [1/19], Loss: 4.6069\n",
      "Epoch: [37/200], Step: [2/19], Loss: 4.6034\n",
      "Epoch: [37/200], Step: [3/19], Loss: 4.6071\n",
      "Epoch: [37/200], Step: [4/19], Loss: 4.5992\n",
      "Epoch: [37/200], Step: [5/19], Loss: 4.5784\n",
      "Epoch: [37/200], Step: [6/19], Loss: 4.6008\n",
      "Epoch: [37/200], Step: [7/19], Loss: 4.6055\n",
      "Epoch: [37/200], Step: [8/19], Loss: 4.6301\n",
      "Epoch: [37/200], Step: [9/19], Loss: 4.6159\n",
      "Epoch: [37/200], Step: [10/19], Loss: 4.6056\n",
      "Epoch: [37/200], Step: [11/19], Loss: 4.5965\n",
      "Epoch: [37/200], Step: [12/19], Loss: 4.6105\n",
      "Epoch: [37/200], Step: [13/19], Loss: 4.5836\n",
      "Epoch: [37/200], Step: [14/19], Loss: 4.6104\n",
      "Epoch: [37/200], Step: [15/19], Loss: 4.6109\n",
      "Epoch: [37/200], Step: [16/19], Loss: 4.6075\n",
      "Epoch: [37/200], Step: [17/19], Loss: 4.5906\n",
      "Epoch: [37/200], Step: [18/19], Loss: 4.5815\n",
      "Epoch: [38/200], Step: [1/19], Loss: 4.5938\n",
      "Epoch: [38/200], Step: [2/19], Loss: 4.5837\n",
      "Epoch: [38/200], Step: [3/19], Loss: 4.6340\n",
      "Epoch: [38/200], Step: [4/19], Loss: 4.5980\n",
      "Epoch: [38/200], Step: [5/19], Loss: 4.6235\n",
      "Epoch: [38/200], Step: [6/19], Loss: 4.5949\n",
      "Epoch: [38/200], Step: [7/19], Loss: 4.6045\n",
      "Epoch: [38/200], Step: [8/19], Loss: 4.6045\n",
      "Epoch: [38/200], Step: [9/19], Loss: 4.6176\n",
      "Epoch: [38/200], Step: [10/19], Loss: 4.5959\n",
      "Epoch: [38/200], Step: [11/19], Loss: 4.6144\n",
      "Epoch: [38/200], Step: [12/19], Loss: 4.6032\n",
      "Epoch: [38/200], Step: [13/19], Loss: 4.5795\n",
      "Epoch: [38/200], Step: [14/19], Loss: 4.6165\n",
      "Epoch: [38/200], Step: [15/19], Loss: 4.5890\n",
      "Epoch: [38/200], Step: [16/19], Loss: 4.5815\n",
      "Epoch: [38/200], Step: [17/19], Loss: 4.5884\n",
      "Epoch: [38/200], Step: [18/19], Loss: 4.6171\n",
      "Epoch: [39/200], Step: [1/19], Loss: 4.5869\n",
      "Epoch: [39/200], Step: [2/19], Loss: 4.5867\n",
      "Epoch: [39/200], Step: [3/19], Loss: 4.6291\n",
      "Epoch: [39/200], Step: [4/19], Loss: 4.5750\n",
      "Epoch: [39/200], Step: [5/19], Loss: 4.5836\n",
      "Epoch: [39/200], Step: [6/19], Loss: 4.5947\n",
      "Epoch: [39/200], Step: [7/19], Loss: 4.6107\n",
      "Epoch: [39/200], Step: [8/19], Loss: 4.6239\n",
      "Epoch: [39/200], Step: [9/19], Loss: 4.6051\n",
      "Epoch: [39/200], Step: [10/19], Loss: 4.5947\n",
      "Epoch: [39/200], Step: [11/19], Loss: 4.6041\n",
      "Epoch: [39/200], Step: [12/19], Loss: 4.6182\n",
      "Epoch: [39/200], Step: [13/19], Loss: 4.6010\n",
      "Epoch: [39/200], Step: [14/19], Loss: 4.6163\n",
      "Epoch: [39/200], Step: [15/19], Loss: 4.6088\n",
      "Epoch: [39/200], Step: [16/19], Loss: 4.5895\n",
      "Epoch: [39/200], Step: [17/19], Loss: 4.6032\n",
      "Epoch: [39/200], Step: [18/19], Loss: 4.6043\n",
      "Epoch: [40/200], Step: [1/19], Loss: 4.5918\n",
      "Epoch: [40/200], Step: [2/19], Loss: 4.5840\n",
      "Epoch: [40/200], Step: [3/19], Loss: 4.6019\n",
      "Epoch: [40/200], Step: [4/19], Loss: 4.6175\n",
      "Epoch: [40/200], Step: [5/19], Loss: 4.5953\n",
      "Epoch: [40/200], Step: [6/19], Loss: 4.6111\n",
      "Epoch: [40/200], Step: [7/19], Loss: 4.5905\n",
      "Epoch: [40/200], Step: [8/19], Loss: 4.5982\n",
      "Epoch: [40/200], Step: [9/19], Loss: 4.5970\n",
      "Epoch: [40/200], Step: [10/19], Loss: 4.5948\n",
      "Epoch: [40/200], Step: [11/19], Loss: 4.6069\n",
      "Epoch: [40/200], Step: [12/19], Loss: 4.6083\n",
      "Epoch: [40/200], Step: [13/19], Loss: 4.5999\n",
      "Epoch: [40/200], Step: [14/19], Loss: 4.6062\n",
      "Epoch: [40/200], Step: [15/19], Loss: 4.5994\n",
      "Epoch: [40/200], Step: [16/19], Loss: 4.6063\n",
      "Epoch: [40/200], Step: [17/19], Loss: 4.6135\n",
      "Epoch: [40/200], Step: [18/19], Loss: 4.6088\n",
      "Epoch: [41/200], Step: [1/19], Loss: 4.5850\n",
      "Epoch: [41/200], Step: [2/19], Loss: 4.5810\n",
      "Epoch: [41/200], Step: [3/19], Loss: 4.5912\n",
      "Epoch: [41/200], Step: [4/19], Loss: 4.6100\n",
      "Epoch: [41/200], Step: [5/19], Loss: 4.5991\n",
      "Epoch: [41/200], Step: [6/19], Loss: 4.6004\n",
      "Epoch: [41/200], Step: [7/19], Loss: 4.6176\n",
      "Epoch: [41/200], Step: [8/19], Loss: 4.6056\n",
      "Epoch: [41/200], Step: [9/19], Loss: 4.5979\n",
      "Epoch: [41/200], Step: [10/19], Loss: 4.6165\n",
      "Epoch: [41/200], Step: [11/19], Loss: 4.5915\n",
      "Epoch: [41/200], Step: [12/19], Loss: 4.5972\n",
      "Epoch: [41/200], Step: [13/19], Loss: 4.6022\n",
      "Epoch: [41/200], Step: [14/19], Loss: 4.5881\n",
      "Epoch: [41/200], Step: [15/19], Loss: 4.6225\n",
      "Epoch: [41/200], Step: [16/19], Loss: 4.6051\n",
      "Epoch: [41/200], Step: [17/19], Loss: 4.6084\n",
      "Epoch: [41/200], Step: [18/19], Loss: 4.6077\n",
      "Epoch: [42/200], Step: [1/19], Loss: 4.6029\n",
      "Epoch: [42/200], Step: [2/19], Loss: 4.6027\n",
      "Epoch: [42/200], Step: [3/19], Loss: 4.5862\n",
      "Epoch: [42/200], Step: [4/19], Loss: 4.6042\n",
      "Epoch: [42/200], Step: [5/19], Loss: 4.6240\n",
      "Epoch: [42/200], Step: [6/19], Loss: 4.6003\n",
      "Epoch: [42/200], Step: [7/19], Loss: 4.5875\n",
      "Epoch: [42/200], Step: [8/19], Loss: 4.5911\n",
      "Epoch: [42/200], Step: [9/19], Loss: 4.6135\n",
      "Epoch: [42/200], Step: [10/19], Loss: 4.6070\n",
      "Epoch: [42/200], Step: [11/19], Loss: 4.5994\n",
      "Epoch: [42/200], Step: [12/19], Loss: 4.5961\n",
      "Epoch: [42/200], Step: [13/19], Loss: 4.6067\n",
      "Epoch: [42/200], Step: [14/19], Loss: 4.5985\n",
      "Epoch: [42/200], Step: [15/19], Loss: 4.6076\n",
      "Epoch: [42/200], Step: [16/19], Loss: 4.6050\n",
      "Epoch: [42/200], Step: [17/19], Loss: 4.5934\n",
      "Epoch: [42/200], Step: [18/19], Loss: 4.5968\n",
      "Epoch: [43/200], Step: [1/19], Loss: 4.5937\n",
      "Epoch: [43/200], Step: [2/19], Loss: 4.6096\n",
      "Epoch: [43/200], Step: [3/19], Loss: 4.6014\n",
      "Epoch: [43/200], Step: [4/19], Loss: 4.6031\n",
      "Epoch: [43/200], Step: [5/19], Loss: 4.6119\n",
      "Epoch: [43/200], Step: [6/19], Loss: 4.5938\n",
      "Epoch: [43/200], Step: [7/19], Loss: 4.5964\n",
      "Epoch: [43/200], Step: [8/19], Loss: 4.6093\n",
      "Epoch: [43/200], Step: [9/19], Loss: 4.6163\n",
      "Epoch: [43/200], Step: [10/19], Loss: 4.5975\n",
      "Epoch: [43/200], Step: [11/19], Loss: 4.6027\n",
      "Epoch: [43/200], Step: [12/19], Loss: 4.6052\n",
      "Epoch: [43/200], Step: [13/19], Loss: 4.5803\n",
      "Epoch: [43/200], Step: [14/19], Loss: 4.5818\n",
      "Epoch: [43/200], Step: [15/19], Loss: 4.6146\n",
      "Epoch: [43/200], Step: [16/19], Loss: 4.5928\n",
      "Epoch: [43/200], Step: [17/19], Loss: 4.6039\n",
      "Epoch: [43/200], Step: [18/19], Loss: 4.6043\n",
      "Epoch: [44/200], Step: [1/19], Loss: 4.6106\n",
      "Epoch: [44/200], Step: [2/19], Loss: 4.5913\n",
      "Epoch: [44/200], Step: [3/19], Loss: 4.6025\n",
      "Epoch: [44/200], Step: [4/19], Loss: 4.5999\n",
      "Epoch: [44/200], Step: [5/19], Loss: 4.5989\n",
      "Epoch: [44/200], Step: [6/19], Loss: 4.5804\n",
      "Epoch: [44/200], Step: [7/19], Loss: 4.5903\n",
      "Epoch: [44/200], Step: [8/19], Loss: 4.6011\n",
      "Epoch: [44/200], Step: [9/19], Loss: 4.6129\n",
      "Epoch: [44/200], Step: [10/19], Loss: 4.6127\n",
      "Epoch: [44/200], Step: [11/19], Loss: 4.5949\n",
      "Epoch: [44/200], Step: [12/19], Loss: 4.5947\n",
      "Epoch: [44/200], Step: [13/19], Loss: 4.5949\n",
      "Epoch: [44/200], Step: [14/19], Loss: 4.6236\n",
      "Epoch: [44/200], Step: [15/19], Loss: 4.6025\n",
      "Epoch: [44/200], Step: [16/19], Loss: 4.5901\n",
      "Epoch: [44/200], Step: [17/19], Loss: 4.6085\n",
      "Epoch: [44/200], Step: [18/19], Loss: 4.6047\n",
      "Epoch: [45/200], Step: [1/19], Loss: 4.6166\n",
      "Epoch: [45/200], Step: [2/19], Loss: 4.5935\n",
      "Epoch: [45/200], Step: [3/19], Loss: 4.5845\n",
      "Epoch: [45/200], Step: [4/19], Loss: 4.5961\n",
      "Epoch: [45/200], Step: [5/19], Loss: 4.5970\n",
      "Epoch: [45/200], Step: [6/19], Loss: 4.6025\n",
      "Epoch: [45/200], Step: [7/19], Loss: 4.6148\n",
      "Epoch: [45/200], Step: [8/19], Loss: 4.6243\n",
      "Epoch: [45/200], Step: [9/19], Loss: 4.5906\n",
      "Epoch: [45/200], Step: [10/19], Loss: 4.5862\n",
      "Epoch: [45/200], Step: [11/19], Loss: 4.5950\n",
      "Epoch: [45/200], Step: [12/19], Loss: 4.6012\n",
      "Epoch: [45/200], Step: [13/19], Loss: 4.6185\n",
      "Epoch: [45/200], Step: [14/19], Loss: 4.5813\n",
      "Epoch: [45/200], Step: [15/19], Loss: 4.6008\n",
      "Epoch: [45/200], Step: [16/19], Loss: 4.6126\n",
      "Epoch: [45/200], Step: [17/19], Loss: 4.5882\n",
      "Epoch: [45/200], Step: [18/19], Loss: 4.6064\n",
      "Epoch: [46/200], Step: [1/19], Loss: 4.5966\n",
      "Epoch: [46/200], Step: [2/19], Loss: 4.5750\n",
      "Epoch: [46/200], Step: [3/19], Loss: 4.5979\n",
      "Epoch: [46/200], Step: [4/19], Loss: 4.6049\n",
      "Epoch: [46/200], Step: [5/19], Loss: 4.5939\n",
      "Epoch: [46/200], Step: [6/19], Loss: 4.6025\n",
      "Epoch: [46/200], Step: [7/19], Loss: 4.5889\n",
      "Epoch: [46/200], Step: [8/19], Loss: 4.5977\n",
      "Epoch: [46/200], Step: [9/19], Loss: 4.6068\n",
      "Epoch: [46/200], Step: [10/19], Loss: 4.6271\n",
      "Epoch: [46/200], Step: [11/19], Loss: 4.6055\n",
      "Epoch: [46/200], Step: [12/19], Loss: 4.5958\n",
      "Epoch: [46/200], Step: [13/19], Loss: 4.5772\n",
      "Epoch: [46/200], Step: [14/19], Loss: 4.6053\n",
      "Epoch: [46/200], Step: [15/19], Loss: 4.5982\n",
      "Epoch: [46/200], Step: [16/19], Loss: 4.6063\n",
      "Epoch: [46/200], Step: [17/19], Loss: 4.6219\n",
      "Epoch: [46/200], Step: [18/19], Loss: 4.6042\n",
      "Epoch: [47/200], Step: [1/19], Loss: 4.5955\n",
      "Epoch: [47/200], Step: [2/19], Loss: 4.5952\n",
      "Epoch: [47/200], Step: [3/19], Loss: 4.5868\n",
      "Epoch: [47/200], Step: [4/19], Loss: 4.5925\n",
      "Epoch: [47/200], Step: [5/19], Loss: 4.6162\n",
      "Epoch: [47/200], Step: [6/19], Loss: 4.5902\n",
      "Epoch: [47/200], Step: [7/19], Loss: 4.5954\n",
      "Epoch: [47/200], Step: [8/19], Loss: 4.6149\n",
      "Epoch: [47/200], Step: [9/19], Loss: 4.6154\n",
      "Epoch: [47/200], Step: [10/19], Loss: 4.5917\n",
      "Epoch: [47/200], Step: [11/19], Loss: 4.5971\n",
      "Epoch: [47/200], Step: [12/19], Loss: 4.5912\n",
      "Epoch: [47/200], Step: [13/19], Loss: 4.6088\n",
      "Epoch: [47/200], Step: [14/19], Loss: 4.6041\n",
      "Epoch: [47/200], Step: [15/19], Loss: 4.6048\n",
      "Epoch: [47/200], Step: [16/19], Loss: 4.6001\n",
      "Epoch: [47/200], Step: [17/19], Loss: 4.6026\n",
      "Epoch: [47/200], Step: [18/19], Loss: 4.5991\n",
      "Epoch: [48/200], Step: [1/19], Loss: 4.6009\n",
      "Epoch: [48/200], Step: [2/19], Loss: 4.6124\n",
      "Epoch: [48/200], Step: [3/19], Loss: 4.5956\n",
      "Epoch: [48/200], Step: [4/19], Loss: 4.5975\n",
      "Epoch: [48/200], Step: [5/19], Loss: 4.5906\n",
      "Epoch: [48/200], Step: [6/19], Loss: 4.5904\n",
      "Epoch: [48/200], Step: [7/19], Loss: 4.6020\n",
      "Epoch: [48/200], Step: [8/19], Loss: 4.6008\n",
      "Epoch: [48/200], Step: [9/19], Loss: 4.6185\n",
      "Epoch: [48/200], Step: [10/19], Loss: 4.5914\n",
      "Epoch: [48/200], Step: [11/19], Loss: 4.5945\n",
      "Epoch: [48/200], Step: [12/19], Loss: 4.5834\n",
      "Epoch: [48/200], Step: [13/19], Loss: 4.5996\n",
      "Epoch: [48/200], Step: [14/19], Loss: 4.5936\n",
      "Epoch: [48/200], Step: [15/19], Loss: 4.6011\n",
      "Epoch: [48/200], Step: [16/19], Loss: 4.6106\n",
      "Epoch: [48/200], Step: [17/19], Loss: 4.5906\n",
      "Epoch: [48/200], Step: [18/19], Loss: 4.6238\n",
      "Epoch: [49/200], Step: [1/19], Loss: 4.6091\n",
      "Epoch: [49/200], Step: [2/19], Loss: 4.5891\n",
      "Epoch: [49/200], Step: [3/19], Loss: 4.5788\n",
      "Epoch: [49/200], Step: [4/19], Loss: 4.5900\n",
      "Epoch: [49/200], Step: [5/19], Loss: 4.6093\n",
      "Epoch: [49/200], Step: [6/19], Loss: 4.5918\n",
      "Epoch: [49/200], Step: [7/19], Loss: 4.6065\n",
      "Epoch: [49/200], Step: [8/19], Loss: 4.6007\n",
      "Epoch: [49/200], Step: [9/19], Loss: 4.6097\n",
      "Epoch: [49/200], Step: [10/19], Loss: 4.5968\n",
      "Epoch: [49/200], Step: [11/19], Loss: 4.6019\n",
      "Epoch: [49/200], Step: [12/19], Loss: 4.6031\n",
      "Epoch: [49/200], Step: [13/19], Loss: 4.6020\n",
      "Epoch: [49/200], Step: [14/19], Loss: 4.6180\n",
      "Epoch: [49/200], Step: [15/19], Loss: 4.6068\n",
      "Epoch: [49/200], Step: [16/19], Loss: 4.5972\n",
      "Epoch: [49/200], Step: [17/19], Loss: 4.6062\n",
      "Epoch: [49/200], Step: [18/19], Loss: 4.5762\n",
      "Epoch: [50/200], Step: [1/19], Loss: 4.5876\n",
      "Epoch: [50/200], Step: [2/19], Loss: 4.5761\n",
      "Epoch: [50/200], Step: [3/19], Loss: 4.6014\n",
      "Epoch: [50/200], Step: [4/19], Loss: 4.6196\n",
      "Epoch: [50/200], Step: [5/19], Loss: 4.6052\n",
      "Epoch: [50/200], Step: [6/19], Loss: 4.6154\n",
      "Epoch: [50/200], Step: [7/19], Loss: 4.5999\n",
      "Epoch: [50/200], Step: [8/19], Loss: 4.5921\n",
      "Epoch: [50/200], Step: [9/19], Loss: 4.5975\n",
      "Epoch: [50/200], Step: [10/19], Loss: 4.5958\n",
      "Epoch: [50/200], Step: [11/19], Loss: 4.6107\n",
      "Epoch: [50/200], Step: [12/19], Loss: 4.6138\n",
      "Epoch: [50/200], Step: [13/19], Loss: 4.5844\n",
      "Epoch: [50/200], Step: [14/19], Loss: 4.5802\n",
      "Epoch: [50/200], Step: [15/19], Loss: 4.6009\n",
      "Epoch: [50/200], Step: [16/19], Loss: 4.6077\n",
      "Epoch: [50/200], Step: [17/19], Loss: 4.5970\n",
      "Epoch: [50/200], Step: [18/19], Loss: 4.6039\n",
      "Epoch: [51/200], Step: [1/19], Loss: 4.6099\n",
      "Epoch: [51/200], Step: [2/19], Loss: 4.5884\n",
      "Epoch: [51/200], Step: [3/19], Loss: 4.6074\n",
      "Epoch: [51/200], Step: [4/19], Loss: 4.6072\n",
      "Epoch: [51/200], Step: [5/19], Loss: 4.5988\n",
      "Epoch: [51/200], Step: [6/19], Loss: 4.5894\n",
      "Epoch: [51/200], Step: [7/19], Loss: 4.5945\n",
      "Epoch: [51/200], Step: [8/19], Loss: 4.5813\n",
      "Epoch: [51/200], Step: [9/19], Loss: 4.5946\n",
      "Epoch: [51/200], Step: [10/19], Loss: 4.5984\n",
      "Epoch: [51/200], Step: [11/19], Loss: 4.6206\n",
      "Epoch: [51/200], Step: [12/19], Loss: 4.5979\n",
      "Epoch: [51/200], Step: [13/19], Loss: 4.5993\n",
      "Epoch: [51/200], Step: [14/19], Loss: 4.5983\n",
      "Epoch: [51/200], Step: [15/19], Loss: 4.5955\n",
      "Epoch: [51/200], Step: [16/19], Loss: 4.6038\n",
      "Epoch: [51/200], Step: [17/19], Loss: 4.6047\n",
      "Epoch: [51/200], Step: [18/19], Loss: 4.5949\n",
      "Epoch: [52/200], Step: [1/19], Loss: 4.6166\n",
      "Epoch: [52/200], Step: [2/19], Loss: 4.6160\n",
      "Epoch: [52/200], Step: [3/19], Loss: 4.5971\n",
      "Epoch: [52/200], Step: [4/19], Loss: 4.5896\n",
      "Epoch: [52/200], Step: [5/19], Loss: 4.5912\n",
      "Epoch: [52/200], Step: [6/19], Loss: 4.5821\n",
      "Epoch: [52/200], Step: [7/19], Loss: 4.5861\n",
      "Epoch: [52/200], Step: [8/19], Loss: 4.6083\n",
      "Epoch: [52/200], Step: [9/19], Loss: 4.5863\n",
      "Epoch: [52/200], Step: [10/19], Loss: 4.6001\n",
      "Epoch: [52/200], Step: [11/19], Loss: 4.6118\n",
      "Epoch: [52/200], Step: [12/19], Loss: 4.5998\n",
      "Epoch: [52/200], Step: [13/19], Loss: 4.6054\n",
      "Epoch: [52/200], Step: [14/19], Loss: 4.5991\n",
      "Epoch: [52/200], Step: [15/19], Loss: 4.6000\n",
      "Epoch: [52/200], Step: [16/19], Loss: 4.6150\n",
      "Epoch: [52/200], Step: [17/19], Loss: 4.5979\n",
      "Epoch: [52/200], Step: [18/19], Loss: 4.5780\n",
      "Epoch: [53/200], Step: [1/19], Loss: 4.6033\n",
      "Epoch: [53/200], Step: [2/19], Loss: 4.6054\n",
      "Epoch: [53/200], Step: [3/19], Loss: 4.6032\n",
      "Epoch: [53/200], Step: [4/19], Loss: 4.5906\n",
      "Epoch: [53/200], Step: [5/19], Loss: 4.6063\n",
      "Epoch: [53/200], Step: [6/19], Loss: 4.6020\n",
      "Epoch: [53/200], Step: [7/19], Loss: 4.6189\n",
      "Epoch: [53/200], Step: [8/19], Loss: 4.6012\n",
      "Epoch: [53/200], Step: [9/19], Loss: 4.6066\n",
      "Epoch: [53/200], Step: [10/19], Loss: 4.5969\n",
      "Epoch: [53/200], Step: [11/19], Loss: 4.5986\n",
      "Epoch: [53/200], Step: [12/19], Loss: 4.5854\n",
      "Epoch: [53/200], Step: [13/19], Loss: 4.5931\n",
      "Epoch: [53/200], Step: [14/19], Loss: 4.6024\n",
      "Epoch: [53/200], Step: [15/19], Loss: 4.5655\n",
      "Epoch: [53/200], Step: [16/19], Loss: 4.5986\n",
      "Epoch: [53/200], Step: [17/19], Loss: 4.6062\n",
      "Epoch: [53/200], Step: [18/19], Loss: 4.5920\n",
      "Epoch: [54/200], Step: [1/19], Loss: 4.6033\n",
      "Epoch: [54/200], Step: [2/19], Loss: 4.5706\n",
      "Epoch: [54/200], Step: [3/19], Loss: 4.5923\n",
      "Epoch: [54/200], Step: [4/19], Loss: 4.5832\n",
      "Epoch: [54/200], Step: [5/19], Loss: 4.5995\n",
      "Epoch: [54/200], Step: [6/19], Loss: 4.5946\n",
      "Epoch: [54/200], Step: [7/19], Loss: 4.6043\n",
      "Epoch: [54/200], Step: [8/19], Loss: 4.5956\n",
      "Epoch: [54/200], Step: [9/19], Loss: 4.6024\n",
      "Epoch: [54/200], Step: [10/19], Loss: 4.6219\n",
      "Epoch: [54/200], Step: [11/19], Loss: 4.6017\n",
      "Epoch: [54/200], Step: [12/19], Loss: 4.6088\n",
      "Epoch: [54/200], Step: [13/19], Loss: 4.5937\n",
      "Epoch: [54/200], Step: [14/19], Loss: 4.6048\n",
      "Epoch: [54/200], Step: [15/19], Loss: 4.5935\n",
      "Epoch: [54/200], Step: [16/19], Loss: 4.5938\n",
      "Epoch: [54/200], Step: [17/19], Loss: 4.6089\n",
      "Epoch: [54/200], Step: [18/19], Loss: 4.5992\n",
      "Epoch: [55/200], Step: [1/19], Loss: 4.6076\n",
      "Epoch: [55/200], Step: [2/19], Loss: 4.6184\n",
      "Epoch: [55/200], Step: [3/19], Loss: 4.6044\n",
      "Epoch: [55/200], Step: [4/19], Loss: 4.5979\n",
      "Epoch: [55/200], Step: [5/19], Loss: 4.5979\n",
      "Epoch: [55/200], Step: [6/19], Loss: 4.5884\n",
      "Epoch: [55/200], Step: [7/19], Loss: 4.5776\n",
      "Epoch: [55/200], Step: [8/19], Loss: 4.5886\n",
      "Epoch: [55/200], Step: [9/19], Loss: 4.6113\n",
      "Epoch: [55/200], Step: [10/19], Loss: 4.6128\n",
      "Epoch: [55/200], Step: [11/19], Loss: 4.5958\n",
      "Epoch: [55/200], Step: [12/19], Loss: 4.6049\n",
      "Epoch: [55/200], Step: [13/19], Loss: 4.5887\n",
      "Epoch: [55/200], Step: [14/19], Loss: 4.6059\n",
      "Epoch: [55/200], Step: [15/19], Loss: 4.5924\n",
      "Epoch: [55/200], Step: [16/19], Loss: 4.5798\n",
      "Epoch: [55/200], Step: [17/19], Loss: 4.5962\n",
      "Epoch: [55/200], Step: [18/19], Loss: 4.5993\n",
      "Epoch: [56/200], Step: [1/19], Loss: 4.5928\n",
      "Epoch: [56/200], Step: [2/19], Loss: 4.6130\n",
      "Epoch: [56/200], Step: [3/19], Loss: 4.5919\n",
      "Epoch: [56/200], Step: [4/19], Loss: 4.5927\n",
      "Epoch: [56/200], Step: [5/19], Loss: 4.5979\n",
      "Epoch: [56/200], Step: [6/19], Loss: 4.6053\n",
      "Epoch: [56/200], Step: [7/19], Loss: 4.6164\n",
      "Epoch: [56/200], Step: [8/19], Loss: 4.5841\n",
      "Epoch: [56/200], Step: [9/19], Loss: 4.5997\n",
      "Epoch: [56/200], Step: [10/19], Loss: 4.6034\n",
      "Epoch: [56/200], Step: [11/19], Loss: 4.5819\n",
      "Epoch: [56/200], Step: [12/19], Loss: 4.6020\n",
      "Epoch: [56/200], Step: [13/19], Loss: 4.6134\n",
      "Epoch: [56/200], Step: [14/19], Loss: 4.5913\n",
      "Epoch: [56/200], Step: [15/19], Loss: 4.5731\n",
      "Epoch: [56/200], Step: [16/19], Loss: 4.6080\n",
      "Epoch: [56/200], Step: [17/19], Loss: 4.5891\n",
      "Epoch: [56/200], Step: [18/19], Loss: 4.6076\n",
      "Epoch: [57/200], Step: [1/19], Loss: 4.6067\n",
      "Epoch: [57/200], Step: [2/19], Loss: 4.6073\n",
      "Epoch: [57/200], Step: [3/19], Loss: 4.5784\n",
      "Epoch: [57/200], Step: [4/19], Loss: 4.5852\n",
      "Epoch: [57/200], Step: [5/19], Loss: 4.6067\n",
      "Epoch: [57/200], Step: [6/19], Loss: 4.6003\n",
      "Epoch: [57/200], Step: [7/19], Loss: 4.6193\n",
      "Epoch: [57/200], Step: [8/19], Loss: 4.5889\n",
      "Epoch: [57/200], Step: [9/19], Loss: 4.6000\n",
      "Epoch: [57/200], Step: [10/19], Loss: 4.5956\n",
      "Epoch: [57/200], Step: [11/19], Loss: 4.6027\n",
      "Epoch: [57/200], Step: [12/19], Loss: 4.6051\n",
      "Epoch: [57/200], Step: [13/19], Loss: 4.6035\n",
      "Epoch: [57/200], Step: [14/19], Loss: 4.5886\n",
      "Epoch: [57/200], Step: [15/19], Loss: 4.5973\n",
      "Epoch: [57/200], Step: [16/19], Loss: 4.5848\n",
      "Epoch: [57/200], Step: [17/19], Loss: 4.5845\n",
      "Epoch: [57/200], Step: [18/19], Loss: 4.6047\n",
      "Epoch: [58/200], Step: [1/19], Loss: 4.6039\n",
      "Epoch: [58/200], Step: [2/19], Loss: 4.6073\n",
      "Epoch: [58/200], Step: [3/19], Loss: 4.5886\n",
      "Epoch: [58/200], Step: [4/19], Loss: 4.5972\n",
      "Epoch: [58/200], Step: [5/19], Loss: 4.5893\n",
      "Epoch: [58/200], Step: [6/19], Loss: 4.5921\n",
      "Epoch: [58/200], Step: [7/19], Loss: 4.6117\n",
      "Epoch: [58/200], Step: [8/19], Loss: 4.5933\n",
      "Epoch: [58/200], Step: [9/19], Loss: 4.5965\n",
      "Epoch: [58/200], Step: [10/19], Loss: 4.5847\n",
      "Epoch: [58/200], Step: [11/19], Loss: 4.6092\n",
      "Epoch: [58/200], Step: [12/19], Loss: 4.6040\n",
      "Epoch: [58/200], Step: [13/19], Loss: 4.5918\n",
      "Epoch: [58/200], Step: [14/19], Loss: 4.6009\n",
      "Epoch: [58/200], Step: [15/19], Loss: 4.5916\n",
      "Epoch: [58/200], Step: [16/19], Loss: 4.6019\n",
      "Epoch: [58/200], Step: [17/19], Loss: 4.5947\n",
      "Epoch: [58/200], Step: [18/19], Loss: 4.5966\n",
      "Epoch: [59/200], Step: [1/19], Loss: 4.5982\n",
      "Epoch: [59/200], Step: [2/19], Loss: 4.6228\n",
      "Epoch: [59/200], Step: [3/19], Loss: 4.6167\n",
      "Epoch: [59/200], Step: [4/19], Loss: 4.5971\n",
      "Epoch: [59/200], Step: [5/19], Loss: 4.5942\n",
      "Epoch: [59/200], Step: [6/19], Loss: 4.5922\n",
      "Epoch: [59/200], Step: [7/19], Loss: 4.5732\n",
      "Epoch: [59/200], Step: [8/19], Loss: 4.6175\n",
      "Epoch: [59/200], Step: [9/19], Loss: 4.5814\n",
      "Epoch: [59/200], Step: [10/19], Loss: 4.5843\n",
      "Epoch: [59/200], Step: [11/19], Loss: 4.5952\n",
      "Epoch: [59/200], Step: [12/19], Loss: 4.5961\n",
      "Epoch: [59/200], Step: [13/19], Loss: 4.5824\n",
      "Epoch: [59/200], Step: [14/19], Loss: 4.5909\n",
      "Epoch: [59/200], Step: [15/19], Loss: 4.6033\n",
      "Epoch: [59/200], Step: [16/19], Loss: 4.5972\n",
      "Epoch: [59/200], Step: [17/19], Loss: 4.5997\n",
      "Epoch: [59/200], Step: [18/19], Loss: 4.6086\n",
      "Epoch: [60/200], Step: [1/19], Loss: 4.6027\n",
      "Epoch: [60/200], Step: [2/19], Loss: 4.5964\n",
      "Epoch: [60/200], Step: [3/19], Loss: 4.5990\n",
      "Epoch: [60/200], Step: [4/19], Loss: 4.5833\n",
      "Epoch: [60/200], Step: [5/19], Loss: 4.5908\n",
      "Epoch: [60/200], Step: [6/19], Loss: 4.6104\n",
      "Epoch: [60/200], Step: [7/19], Loss: 4.5851\n",
      "Epoch: [60/200], Step: [8/19], Loss: 4.5930\n",
      "Epoch: [60/200], Step: [9/19], Loss: 4.5867\n",
      "Epoch: [60/200], Step: [10/19], Loss: 4.5759\n",
      "Epoch: [60/200], Step: [11/19], Loss: 4.6144\n",
      "Epoch: [60/200], Step: [12/19], Loss: 4.5985\n",
      "Epoch: [60/200], Step: [13/19], Loss: 4.6012\n",
      "Epoch: [60/200], Step: [14/19], Loss: 4.5970\n",
      "Epoch: [60/200], Step: [15/19], Loss: 4.5990\n",
      "Epoch: [60/200], Step: [16/19], Loss: 4.5918\n",
      "Epoch: [60/200], Step: [17/19], Loss: 4.6097\n",
      "Epoch: [60/200], Step: [18/19], Loss: 4.6122\n",
      "Epoch: [61/200], Step: [1/19], Loss: 4.5820\n",
      "Epoch: [61/200], Step: [2/19], Loss: 4.6054\n",
      "Epoch: [61/200], Step: [3/19], Loss: 4.6082\n",
      "Epoch: [61/200], Step: [4/19], Loss: 4.6060\n",
      "Epoch: [61/200], Step: [5/19], Loss: 4.5806\n",
      "Epoch: [61/200], Step: [6/19], Loss: 4.6045\n",
      "Epoch: [61/200], Step: [7/19], Loss: 4.5973\n",
      "Epoch: [61/200], Step: [8/19], Loss: 4.5877\n",
      "Epoch: [61/200], Step: [9/19], Loss: 4.5921\n",
      "Epoch: [61/200], Step: [10/19], Loss: 4.6070\n",
      "Epoch: [61/200], Step: [11/19], Loss: 4.5883\n",
      "Epoch: [61/200], Step: [12/19], Loss: 4.5947\n",
      "Epoch: [61/200], Step: [13/19], Loss: 4.5787\n",
      "Epoch: [61/200], Step: [14/19], Loss: 4.6100\n",
      "Epoch: [61/200], Step: [15/19], Loss: 4.5993\n",
      "Epoch: [61/200], Step: [16/19], Loss: 4.5963\n",
      "Epoch: [61/200], Step: [17/19], Loss: 4.5848\n",
      "Epoch: [61/200], Step: [18/19], Loss: 4.6200\n",
      "Epoch: [62/200], Step: [1/19], Loss: 4.6023\n",
      "Epoch: [62/200], Step: [2/19], Loss: 4.6000\n",
      "Epoch: [62/200], Step: [3/19], Loss: 4.5898\n",
      "Epoch: [62/200], Step: [4/19], Loss: 4.5972\n",
      "Epoch: [62/200], Step: [5/19], Loss: 4.5949\n",
      "Epoch: [62/200], Step: [6/19], Loss: 4.5914\n",
      "Epoch: [62/200], Step: [7/19], Loss: 4.6022\n",
      "Epoch: [62/200], Step: [8/19], Loss: 4.6064\n",
      "Epoch: [62/200], Step: [9/19], Loss: 4.5891\n",
      "Epoch: [62/200], Step: [10/19], Loss: 4.5972\n",
      "Epoch: [62/200], Step: [11/19], Loss: 4.5936\n",
      "Epoch: [62/200], Step: [12/19], Loss: 4.5942\n",
      "Epoch: [62/200], Step: [13/19], Loss: 4.5881\n",
      "Epoch: [62/200], Step: [14/19], Loss: 4.5991\n",
      "Epoch: [62/200], Step: [15/19], Loss: 4.5982\n",
      "Epoch: [62/200], Step: [16/19], Loss: 4.5841\n",
      "Epoch: [62/200], Step: [17/19], Loss: 4.6053\n",
      "Epoch: [62/200], Step: [18/19], Loss: 4.6057\n",
      "Epoch: [63/200], Step: [1/19], Loss: 4.5858\n",
      "Epoch: [63/200], Step: [2/19], Loss: 4.5975\n",
      "Epoch: [63/200], Step: [3/19], Loss: 4.5826\n",
      "Epoch: [63/200], Step: [4/19], Loss: 4.6076\n",
      "Epoch: [63/200], Step: [5/19], Loss: 4.6058\n",
      "Epoch: [63/200], Step: [6/19], Loss: 4.5978\n",
      "Epoch: [63/200], Step: [7/19], Loss: 4.5870\n",
      "Epoch: [63/200], Step: [8/19], Loss: 4.5984\n",
      "Epoch: [63/200], Step: [9/19], Loss: 4.6127\n",
      "Epoch: [63/200], Step: [10/19], Loss: 4.6105\n",
      "Epoch: [63/200], Step: [11/19], Loss: 4.5994\n",
      "Epoch: [63/200], Step: [12/19], Loss: 4.5909\n",
      "Epoch: [63/200], Step: [13/19], Loss: 4.6018\n",
      "Epoch: [63/200], Step: [14/19], Loss: 4.5993\n",
      "Epoch: [63/200], Step: [15/19], Loss: 4.5936\n",
      "Epoch: [63/200], Step: [16/19], Loss: 4.6005\n",
      "Epoch: [63/200], Step: [17/19], Loss: 4.5913\n",
      "Epoch: [63/200], Step: [18/19], Loss: 4.5721\n",
      "Epoch: [64/200], Step: [1/19], Loss: 4.5937\n",
      "Epoch: [64/200], Step: [2/19], Loss: 4.5914\n",
      "Epoch: [64/200], Step: [3/19], Loss: 4.6087\n",
      "Epoch: [64/200], Step: [4/19], Loss: 4.6149\n",
      "Epoch: [64/200], Step: [5/19], Loss: 4.6018\n",
      "Epoch: [64/200], Step: [6/19], Loss: 4.5842\n",
      "Epoch: [64/200], Step: [7/19], Loss: 4.5813\n",
      "Epoch: [64/200], Step: [8/19], Loss: 4.5948\n",
      "Epoch: [64/200], Step: [9/19], Loss: 4.6041\n",
      "Epoch: [64/200], Step: [10/19], Loss: 4.6094\n",
      "Epoch: [64/200], Step: [11/19], Loss: 4.5952\n",
      "Epoch: [64/200], Step: [12/19], Loss: 4.6080\n",
      "Epoch: [64/200], Step: [13/19], Loss: 4.5952\n",
      "Epoch: [64/200], Step: [14/19], Loss: 4.5800\n",
      "Epoch: [64/200], Step: [15/19], Loss: 4.5759\n",
      "Epoch: [64/200], Step: [16/19], Loss: 4.5918\n",
      "Epoch: [64/200], Step: [17/19], Loss: 4.6068\n",
      "Epoch: [64/200], Step: [18/19], Loss: 4.5933\n",
      "Epoch: [65/200], Step: [1/19], Loss: 4.5957\n",
      "Epoch: [65/200], Step: [2/19], Loss: 4.6014\n",
      "Epoch: [65/200], Step: [3/19], Loss: 4.5989\n",
      "Epoch: [65/200], Step: [4/19], Loss: 4.5922\n",
      "Epoch: [65/200], Step: [5/19], Loss: 4.5853\n",
      "Epoch: [65/200], Step: [6/19], Loss: 4.5976\n",
      "Epoch: [65/200], Step: [7/19], Loss: 4.5896\n",
      "Epoch: [65/200], Step: [8/19], Loss: 4.5900\n",
      "Epoch: [65/200], Step: [9/19], Loss: 4.5959\n",
      "Epoch: [65/200], Step: [10/19], Loss: 4.6219\n",
      "Epoch: [65/200], Step: [11/19], Loss: 4.6017\n",
      "Epoch: [65/200], Step: [12/19], Loss: 4.5777\n",
      "Epoch: [65/200], Step: [13/19], Loss: 4.6033\n",
      "Epoch: [65/200], Step: [14/19], Loss: 4.5974\n",
      "Epoch: [65/200], Step: [15/19], Loss: 4.6122\n",
      "Epoch: [65/200], Step: [16/19], Loss: 4.5724\n",
      "Epoch: [65/200], Step: [17/19], Loss: 4.5952\n",
      "Epoch: [65/200], Step: [18/19], Loss: 4.5980\n",
      "Epoch: [66/200], Step: [1/19], Loss: 4.5974\n",
      "Epoch: [66/200], Step: [2/19], Loss: 4.6007\n",
      "Epoch: [66/200], Step: [3/19], Loss: 4.5773\n",
      "Epoch: [66/200], Step: [4/19], Loss: 4.6088\n",
      "Epoch: [66/200], Step: [5/19], Loss: 4.6008\n",
      "Epoch: [66/200], Step: [6/19], Loss: 4.5967\n",
      "Epoch: [66/200], Step: [7/19], Loss: 4.5835\n",
      "Epoch: [66/200], Step: [8/19], Loss: 4.5892\n",
      "Epoch: [66/200], Step: [9/19], Loss: 4.5917\n",
      "Epoch: [66/200], Step: [10/19], Loss: 4.5946\n",
      "Epoch: [66/200], Step: [11/19], Loss: 4.5885\n",
      "Epoch: [66/200], Step: [12/19], Loss: 4.5954\n",
      "Epoch: [66/200], Step: [13/19], Loss: 4.5910\n",
      "Epoch: [66/200], Step: [14/19], Loss: 4.6056\n",
      "Epoch: [66/200], Step: [15/19], Loss: 4.6046\n",
      "Epoch: [66/200], Step: [16/19], Loss: 4.6112\n",
      "Epoch: [66/200], Step: [17/19], Loss: 4.6016\n",
      "Epoch: [66/200], Step: [18/19], Loss: 4.5836\n",
      "Epoch: [67/200], Step: [1/19], Loss: 4.5993\n",
      "Epoch: [67/200], Step: [2/19], Loss: 4.6194\n",
      "Epoch: [67/200], Step: [3/19], Loss: 4.5841\n",
      "Epoch: [67/200], Step: [4/19], Loss: 4.5856\n",
      "Epoch: [67/200], Step: [5/19], Loss: 4.6104\n",
      "Epoch: [67/200], Step: [6/19], Loss: 4.6002\n",
      "Epoch: [67/200], Step: [7/19], Loss: 4.6003\n",
      "Epoch: [67/200], Step: [8/19], Loss: 4.6015\n",
      "Epoch: [67/200], Step: [9/19], Loss: 4.5926\n",
      "Epoch: [67/200], Step: [10/19], Loss: 4.5955\n",
      "Epoch: [67/200], Step: [11/19], Loss: 4.5954\n",
      "Epoch: [67/200], Step: [12/19], Loss: 4.6024\n",
      "Epoch: [67/200], Step: [13/19], Loss: 4.5909\n",
      "Epoch: [67/200], Step: [14/19], Loss: 4.5801\n",
      "Epoch: [67/200], Step: [15/19], Loss: 4.5948\n",
      "Epoch: [67/200], Step: [16/19], Loss: 4.5792\n",
      "Epoch: [67/200], Step: [17/19], Loss: 4.5956\n",
      "Epoch: [67/200], Step: [18/19], Loss: 4.5907\n",
      "Epoch: [68/200], Step: [1/19], Loss: 4.6082\n",
      "Epoch: [68/200], Step: [2/19], Loss: 4.5982\n",
      "Epoch: [68/200], Step: [3/19], Loss: 4.5924\n",
      "Epoch: [68/200], Step: [4/19], Loss: 4.6059\n",
      "Epoch: [68/200], Step: [5/19], Loss: 4.5864\n",
      "Epoch: [68/200], Step: [6/19], Loss: 4.5952\n",
      "Epoch: [68/200], Step: [7/19], Loss: 4.5973\n",
      "Epoch: [68/200], Step: [8/19], Loss: 4.5939\n",
      "Epoch: [68/200], Step: [9/19], Loss: 4.5840\n",
      "Epoch: [68/200], Step: [10/19], Loss: 4.5840\n",
      "Epoch: [68/200], Step: [11/19], Loss: 4.6086\n",
      "Epoch: [68/200], Step: [12/19], Loss: 4.5977\n",
      "Epoch: [68/200], Step: [13/19], Loss: 4.6124\n",
      "Epoch: [68/200], Step: [14/19], Loss: 4.5986\n",
      "Epoch: [68/200], Step: [15/19], Loss: 4.5802\n",
      "Epoch: [68/200], Step: [16/19], Loss: 4.5914\n",
      "Epoch: [68/200], Step: [17/19], Loss: 4.5885\n",
      "Epoch: [68/200], Step: [18/19], Loss: 4.5910\n",
      "Epoch: [69/200], Step: [1/19], Loss: 4.6043\n",
      "Epoch: [69/200], Step: [2/19], Loss: 4.5879\n",
      "Epoch: [69/200], Step: [3/19], Loss: 4.5998\n",
      "Epoch: [69/200], Step: [4/19], Loss: 4.5990\n",
      "Epoch: [69/200], Step: [5/19], Loss: 4.5961\n",
      "Epoch: [69/200], Step: [6/19], Loss: 4.5729\n",
      "Epoch: [69/200], Step: [7/19], Loss: 4.6062\n",
      "Epoch: [69/200], Step: [8/19], Loss: 4.6028\n",
      "Epoch: [69/200], Step: [9/19], Loss: 4.6095\n",
      "Epoch: [69/200], Step: [10/19], Loss: 4.5821\n",
      "Epoch: [69/200], Step: [11/19], Loss: 4.5818\n",
      "Epoch: [69/200], Step: [12/19], Loss: 4.5749\n",
      "Epoch: [69/200], Step: [13/19], Loss: 4.6008\n",
      "Epoch: [69/200], Step: [14/19], Loss: 4.5900\n",
      "Epoch: [69/200], Step: [15/19], Loss: 4.6093\n",
      "Epoch: [69/200], Step: [16/19], Loss: 4.5932\n",
      "Epoch: [69/200], Step: [17/19], Loss: 4.6020\n",
      "Epoch: [69/200], Step: [18/19], Loss: 4.5972\n",
      "Epoch: [70/200], Step: [1/19], Loss: 4.6044\n",
      "Epoch: [70/200], Step: [2/19], Loss: 4.6059\n",
      "Epoch: [70/200], Step: [3/19], Loss: 4.6052\n",
      "Epoch: [70/200], Step: [4/19], Loss: 4.5851\n",
      "Epoch: [70/200], Step: [5/19], Loss: 4.6083\n",
      "Epoch: [70/200], Step: [6/19], Loss: 4.5985\n",
      "Epoch: [70/200], Step: [7/19], Loss: 4.5987\n",
      "Epoch: [70/200], Step: [8/19], Loss: 4.5903\n",
      "Epoch: [70/200], Step: [9/19], Loss: 4.5856\n",
      "Epoch: [70/200], Step: [10/19], Loss: 4.5822\n",
      "Epoch: [70/200], Step: [11/19], Loss: 4.6026\n",
      "Epoch: [70/200], Step: [12/19], Loss: 4.5989\n",
      "Epoch: [70/200], Step: [13/19], Loss: 4.5973\n",
      "Epoch: [70/200], Step: [14/19], Loss: 4.5874\n",
      "Epoch: [70/200], Step: [15/19], Loss: 4.5862\n",
      "Epoch: [70/200], Step: [16/19], Loss: 4.5881\n",
      "Epoch: [70/200], Step: [17/19], Loss: 4.5923\n",
      "Epoch: [70/200], Step: [18/19], Loss: 4.5888\n",
      "Epoch: [71/200], Step: [1/19], Loss: 4.6077\n",
      "Epoch: [71/200], Step: [2/19], Loss: 4.5842\n",
      "Epoch: [71/200], Step: [3/19], Loss: 4.6048\n",
      "Epoch: [71/200], Step: [4/19], Loss: 4.5722\n",
      "Epoch: [71/200], Step: [5/19], Loss: 4.5888\n",
      "Epoch: [71/200], Step: [6/19], Loss: 4.6042\n",
      "Epoch: [71/200], Step: [7/19], Loss: 4.5692\n",
      "Epoch: [71/200], Step: [8/19], Loss: 4.5980\n",
      "Epoch: [71/200], Step: [9/19], Loss: 4.6046\n",
      "Epoch: [71/200], Step: [10/19], Loss: 4.5877\n",
      "Epoch: [71/200], Step: [11/19], Loss: 4.5857\n",
      "Epoch: [71/200], Step: [12/19], Loss: 4.6040\n",
      "Epoch: [71/200], Step: [13/19], Loss: 4.5948\n",
      "Epoch: [71/200], Step: [14/19], Loss: 4.5960\n",
      "Epoch: [71/200], Step: [15/19], Loss: 4.5884\n",
      "Epoch: [71/200], Step: [16/19], Loss: 4.6105\n",
      "Epoch: [71/200], Step: [17/19], Loss: 4.5940\n",
      "Epoch: [71/200], Step: [18/19], Loss: 4.6068\n",
      "Epoch: [72/200], Step: [1/19], Loss: 4.6081\n",
      "Epoch: [72/200], Step: [2/19], Loss: 4.6046\n",
      "Epoch: [72/200], Step: [3/19], Loss: 4.6150\n",
      "Epoch: [72/200], Step: [4/19], Loss: 4.6004\n",
      "Epoch: [72/200], Step: [5/19], Loss: 4.5815\n",
      "Epoch: [72/200], Step: [6/19], Loss: 4.5731\n",
      "Epoch: [72/200], Step: [7/19], Loss: 4.5891\n",
      "Epoch: [72/200], Step: [8/19], Loss: 4.5850\n",
      "Epoch: [72/200], Step: [9/19], Loss: 4.5913\n",
      "Epoch: [72/200], Step: [10/19], Loss: 4.5871\n",
      "Epoch: [72/200], Step: [11/19], Loss: 4.5915\n",
      "Epoch: [72/200], Step: [12/19], Loss: 4.6082\n",
      "Epoch: [72/200], Step: [13/19], Loss: 4.5861\n",
      "Epoch: [72/200], Step: [14/19], Loss: 4.5883\n",
      "Epoch: [72/200], Step: [15/19], Loss: 4.5972\n",
      "Epoch: [72/200], Step: [16/19], Loss: 4.5990\n",
      "Epoch: [72/200], Step: [17/19], Loss: 4.6001\n",
      "Epoch: [72/200], Step: [18/19], Loss: 4.5922\n",
      "Epoch: [73/200], Step: [1/19], Loss: 4.5906\n",
      "Epoch: [73/200], Step: [2/19], Loss: 4.5961\n",
      "Epoch: [73/200], Step: [3/19], Loss: 4.5815\n",
      "Epoch: [73/200], Step: [4/19], Loss: 4.5880\n",
      "Epoch: [73/200], Step: [5/19], Loss: 4.5729\n",
      "Epoch: [73/200], Step: [6/19], Loss: 4.6000\n",
      "Epoch: [73/200], Step: [7/19], Loss: 4.5772\n",
      "Epoch: [73/200], Step: [8/19], Loss: 4.5938\n",
      "Epoch: [73/200], Step: [9/19], Loss: 4.5858\n",
      "Epoch: [73/200], Step: [10/19], Loss: 4.5872\n",
      "Epoch: [73/200], Step: [11/19], Loss: 4.6108\n",
      "Epoch: [73/200], Step: [12/19], Loss: 4.5868\n",
      "Epoch: [73/200], Step: [13/19], Loss: 4.5979\n",
      "Epoch: [73/200], Step: [14/19], Loss: 4.6000\n",
      "Epoch: [73/200], Step: [15/19], Loss: 4.6053\n",
      "Epoch: [73/200], Step: [16/19], Loss: 4.6118\n",
      "Epoch: [73/200], Step: [17/19], Loss: 4.6008\n",
      "Epoch: [73/200], Step: [18/19], Loss: 4.6069\n",
      "Epoch: [74/200], Step: [1/19], Loss: 4.5951\n",
      "Epoch: [74/200], Step: [2/19], Loss: 4.5997\n",
      "Epoch: [74/200], Step: [3/19], Loss: 4.5819\n",
      "Epoch: [74/200], Step: [4/19], Loss: 4.6030\n",
      "Epoch: [74/200], Step: [5/19], Loss: 4.6177\n",
      "Epoch: [74/200], Step: [6/19], Loss: 4.5877\n",
      "Epoch: [74/200], Step: [7/19], Loss: 4.5878\n",
      "Epoch: [74/200], Step: [8/19], Loss: 4.5892\n",
      "Epoch: [74/200], Step: [9/19], Loss: 4.5932\n",
      "Epoch: [74/200], Step: [10/19], Loss: 4.5928\n",
      "Epoch: [74/200], Step: [11/19], Loss: 4.5770\n",
      "Epoch: [74/200], Step: [12/19], Loss: 4.5809\n",
      "Epoch: [74/200], Step: [13/19], Loss: 4.6051\n",
      "Epoch: [74/200], Step: [14/19], Loss: 4.5986\n",
      "Epoch: [74/200], Step: [15/19], Loss: 4.5836\n",
      "Epoch: [74/200], Step: [16/19], Loss: 4.6094\n",
      "Epoch: [74/200], Step: [17/19], Loss: 4.6042\n",
      "Epoch: [74/200], Step: [18/19], Loss: 4.5826\n",
      "Epoch: [75/200], Step: [1/19], Loss: 4.5958\n",
      "Epoch: [75/200], Step: [2/19], Loss: 4.5626\n",
      "Epoch: [75/200], Step: [3/19], Loss: 4.5967\n",
      "Epoch: [75/200], Step: [4/19], Loss: 4.6088\n",
      "Epoch: [75/200], Step: [5/19], Loss: 4.6005\n",
      "Epoch: [75/200], Step: [6/19], Loss: 4.5793\n",
      "Epoch: [75/200], Step: [7/19], Loss: 4.5804\n",
      "Epoch: [75/200], Step: [8/19], Loss: 4.5846\n",
      "Epoch: [75/200], Step: [9/19], Loss: 4.6003\n",
      "Epoch: [75/200], Step: [10/19], Loss: 4.5959\n",
      "Epoch: [75/200], Step: [11/19], Loss: 4.5734\n",
      "Epoch: [75/200], Step: [12/19], Loss: 4.6232\n",
      "Epoch: [75/200], Step: [13/19], Loss: 4.5828\n",
      "Epoch: [75/200], Step: [14/19], Loss: 4.5824\n",
      "Epoch: [75/200], Step: [15/19], Loss: 4.5988\n",
      "Epoch: [75/200], Step: [16/19], Loss: 4.5991\n",
      "Epoch: [75/200], Step: [17/19], Loss: 4.6041\n",
      "Epoch: [75/200], Step: [18/19], Loss: 4.6166\n",
      "Epoch: [76/200], Step: [1/19], Loss: 4.6014\n",
      "Epoch: [76/200], Step: [2/19], Loss: 4.5935\n",
      "Epoch: [76/200], Step: [3/19], Loss: 4.5764\n",
      "Epoch: [76/200], Step: [4/19], Loss: 4.5953\n",
      "Epoch: [76/200], Step: [5/19], Loss: 4.5874\n",
      "Epoch: [76/200], Step: [6/19], Loss: 4.5903\n",
      "Epoch: [76/200], Step: [7/19], Loss: 4.6006\n",
      "Epoch: [76/200], Step: [8/19], Loss: 4.5840\n",
      "Epoch: [76/200], Step: [9/19], Loss: 4.5843\n",
      "Epoch: [76/200], Step: [10/19], Loss: 4.5875\n",
      "Epoch: [76/200], Step: [11/19], Loss: 4.5971\n",
      "Epoch: [76/200], Step: [12/19], Loss: 4.5964\n",
      "Epoch: [76/200], Step: [13/19], Loss: 4.6004\n",
      "Epoch: [76/200], Step: [14/19], Loss: 4.5718\n",
      "Epoch: [76/200], Step: [15/19], Loss: 4.5975\n",
      "Epoch: [76/200], Step: [16/19], Loss: 4.6129\n",
      "Epoch: [76/200], Step: [17/19], Loss: 4.5952\n",
      "Epoch: [76/200], Step: [18/19], Loss: 4.6092\n",
      "Epoch: [77/200], Step: [1/19], Loss: 4.5872\n",
      "Epoch: [77/200], Step: [2/19], Loss: 4.5919\n",
      "Epoch: [77/200], Step: [3/19], Loss: 4.5797\n",
      "Epoch: [77/200], Step: [4/19], Loss: 4.5908\n",
      "Epoch: [77/200], Step: [5/19], Loss: 4.5934\n",
      "Epoch: [77/200], Step: [6/19], Loss: 4.5910\n",
      "Epoch: [77/200], Step: [7/19], Loss: 4.5793\n",
      "Epoch: [77/200], Step: [8/19], Loss: 4.5809\n",
      "Epoch: [77/200], Step: [9/19], Loss: 4.5921\n",
      "Epoch: [77/200], Step: [10/19], Loss: 4.6067\n",
      "Epoch: [77/200], Step: [11/19], Loss: 4.5927\n",
      "Epoch: [77/200], Step: [12/19], Loss: 4.5942\n",
      "Epoch: [77/200], Step: [13/19], Loss: 4.6032\n",
      "Epoch: [77/200], Step: [14/19], Loss: 4.5951\n",
      "Epoch: [77/200], Step: [15/19], Loss: 4.5928\n",
      "Epoch: [77/200], Step: [16/19], Loss: 4.6100\n",
      "Epoch: [77/200], Step: [17/19], Loss: 4.6005\n",
      "Epoch: [77/200], Step: [18/19], Loss: 4.5957\n",
      "Epoch: [78/200], Step: [1/19], Loss: 4.5920\n",
      "Epoch: [78/200], Step: [2/19], Loss: 4.5892\n",
      "Epoch: [78/200], Step: [3/19], Loss: 4.5816\n",
      "Epoch: [78/200], Step: [4/19], Loss: 4.5967\n",
      "Epoch: [78/200], Step: [5/19], Loss: 4.6034\n",
      "Epoch: [78/200], Step: [6/19], Loss: 4.5865\n",
      "Epoch: [78/200], Step: [7/19], Loss: 4.5820\n",
      "Epoch: [78/200], Step: [8/19], Loss: 4.5930\n",
      "Epoch: [78/200], Step: [9/19], Loss: 4.5932\n",
      "Epoch: [78/200], Step: [10/19], Loss: 4.5993\n",
      "Epoch: [78/200], Step: [11/19], Loss: 4.5947\n",
      "Epoch: [78/200], Step: [12/19], Loss: 4.6064\n",
      "Epoch: [78/200], Step: [13/19], Loss: 4.5847\n",
      "Epoch: [78/200], Step: [14/19], Loss: 4.5940\n",
      "Epoch: [78/200], Step: [15/19], Loss: 4.5901\n",
      "Epoch: [78/200], Step: [16/19], Loss: 4.5901\n",
      "Epoch: [78/200], Step: [17/19], Loss: 4.6055\n",
      "Epoch: [78/200], Step: [18/19], Loss: 4.5906\n",
      "Epoch: [79/200], Step: [1/19], Loss: 4.5881\n",
      "Epoch: [79/200], Step: [2/19], Loss: 4.6070\n",
      "Epoch: [79/200], Step: [3/19], Loss: 4.5734\n",
      "Epoch: [79/200], Step: [4/19], Loss: 4.5903\n",
      "Epoch: [79/200], Step: [5/19], Loss: 4.5954\n",
      "Epoch: [79/200], Step: [6/19], Loss: 4.5828\n",
      "Epoch: [79/200], Step: [7/19], Loss: 4.6017\n",
      "Epoch: [79/200], Step: [8/19], Loss: 4.5857\n",
      "Epoch: [79/200], Step: [9/19], Loss: 4.5802\n",
      "Epoch: [79/200], Step: [10/19], Loss: 4.5879\n",
      "Epoch: [79/200], Step: [11/19], Loss: 4.6079\n",
      "Epoch: [79/200], Step: [12/19], Loss: 4.6093\n",
      "Epoch: [79/200], Step: [13/19], Loss: 4.5942\n",
      "Epoch: [79/200], Step: [14/19], Loss: 4.5941\n",
      "Epoch: [79/200], Step: [15/19], Loss: 4.6000\n",
      "Epoch: [79/200], Step: [16/19], Loss: 4.5768\n",
      "Epoch: [79/200], Step: [17/19], Loss: 4.5961\n",
      "Epoch: [79/200], Step: [18/19], Loss: 4.5983\n",
      "Epoch: [80/200], Step: [1/19], Loss: 4.5995\n",
      "Epoch: [80/200], Step: [2/19], Loss: 4.5863\n",
      "Epoch: [80/200], Step: [3/19], Loss: 4.5868\n",
      "Epoch: [80/200], Step: [4/19], Loss: 4.6011\n",
      "Epoch: [80/200], Step: [5/19], Loss: 4.6012\n",
      "Epoch: [80/200], Step: [6/19], Loss: 4.5796\n",
      "Epoch: [80/200], Step: [7/19], Loss: 4.5872\n",
      "Epoch: [80/200], Step: [8/19], Loss: 4.5889\n",
      "Epoch: [80/200], Step: [9/19], Loss: 4.5948\n",
      "Epoch: [80/200], Step: [10/19], Loss: 4.5887\n",
      "Epoch: [80/200], Step: [11/19], Loss: 4.6082\n",
      "Epoch: [80/200], Step: [12/19], Loss: 4.5686\n",
      "Epoch: [80/200], Step: [13/19], Loss: 4.5955\n",
      "Epoch: [80/200], Step: [14/19], Loss: 4.5987\n",
      "Epoch: [80/200], Step: [15/19], Loss: 4.5985\n",
      "Epoch: [80/200], Step: [16/19], Loss: 4.5867\n",
      "Epoch: [80/200], Step: [17/19], Loss: 4.5883\n",
      "Epoch: [80/200], Step: [18/19], Loss: 4.6064\n",
      "Epoch: [81/200], Step: [1/19], Loss: 4.5983\n",
      "Epoch: [81/200], Step: [2/19], Loss: 4.5794\n",
      "Epoch: [81/200], Step: [3/19], Loss: 4.5914\n",
      "Epoch: [81/200], Step: [4/19], Loss: 4.5876\n",
      "Epoch: [81/200], Step: [5/19], Loss: 4.6058\n",
      "Epoch: [81/200], Step: [6/19], Loss: 4.6008\n",
      "Epoch: [81/200], Step: [7/19], Loss: 4.5901\n",
      "Epoch: [81/200], Step: [8/19], Loss: 4.5984\n",
      "Epoch: [81/200], Step: [9/19], Loss: 4.5863\n",
      "Epoch: [81/200], Step: [10/19], Loss: 4.5702\n",
      "Epoch: [81/200], Step: [11/19], Loss: 4.5892\n",
      "Epoch: [81/200], Step: [12/19], Loss: 4.6070\n",
      "Epoch: [81/200], Step: [13/19], Loss: 4.5793\n",
      "Epoch: [81/200], Step: [14/19], Loss: 4.6051\n",
      "Epoch: [81/200], Step: [15/19], Loss: 4.5788\n",
      "Epoch: [81/200], Step: [16/19], Loss: 4.6012\n",
      "Epoch: [81/200], Step: [17/19], Loss: 4.5944\n",
      "Epoch: [81/200], Step: [18/19], Loss: 4.5976\n",
      "Epoch: [82/200], Step: [1/19], Loss: 4.5829\n",
      "Epoch: [82/200], Step: [2/19], Loss: 4.5926\n",
      "Epoch: [82/200], Step: [3/19], Loss: 4.5935\n",
      "Epoch: [82/200], Step: [4/19], Loss: 4.6009\n",
      "Epoch: [82/200], Step: [5/19], Loss: 4.5775\n",
      "Epoch: [82/200], Step: [6/19], Loss: 4.5856\n",
      "Epoch: [82/200], Step: [7/19], Loss: 4.5935\n",
      "Epoch: [82/200], Step: [8/19], Loss: 4.5999\n",
      "Epoch: [82/200], Step: [9/19], Loss: 4.6100\n",
      "Epoch: [82/200], Step: [10/19], Loss: 4.5863\n",
      "Epoch: [82/200], Step: [11/19], Loss: 4.5829\n",
      "Epoch: [82/200], Step: [12/19], Loss: 4.6005\n",
      "Epoch: [82/200], Step: [13/19], Loss: 4.6032\n",
      "Epoch: [82/200], Step: [14/19], Loss: 4.6039\n",
      "Epoch: [82/200], Step: [15/19], Loss: 4.5946\n",
      "Epoch: [82/200], Step: [16/19], Loss: 4.5824\n",
      "Epoch: [82/200], Step: [17/19], Loss: 4.5904\n",
      "Epoch: [82/200], Step: [18/19], Loss: 4.5763\n",
      "Epoch: [83/200], Step: [1/19], Loss: 4.5995\n",
      "Epoch: [83/200], Step: [2/19], Loss: 4.5964\n",
      "Epoch: [83/200], Step: [3/19], Loss: 4.6071\n",
      "Epoch: [83/200], Step: [4/19], Loss: 4.5911\n",
      "Epoch: [83/200], Step: [5/19], Loss: 4.6008\n",
      "Epoch: [83/200], Step: [6/19], Loss: 4.5755\n",
      "Epoch: [83/200], Step: [7/19], Loss: 4.5933\n",
      "Epoch: [83/200], Step: [8/19], Loss: 4.5960\n",
      "Epoch: [83/200], Step: [9/19], Loss: 4.5901\n",
      "Epoch: [83/200], Step: [10/19], Loss: 4.5801\n",
      "Epoch: [83/200], Step: [11/19], Loss: 4.5897\n",
      "Epoch: [83/200], Step: [12/19], Loss: 4.5856\n",
      "Epoch: [83/200], Step: [13/19], Loss: 4.6022\n",
      "Epoch: [83/200], Step: [14/19], Loss: 4.5869\n",
      "Epoch: [83/200], Step: [15/19], Loss: 4.6042\n",
      "Epoch: [83/200], Step: [16/19], Loss: 4.5868\n",
      "Epoch: [83/200], Step: [17/19], Loss: 4.5849\n",
      "Epoch: [83/200], Step: [18/19], Loss: 4.5826\n",
      "Epoch: [84/200], Step: [1/19], Loss: 4.6179\n",
      "Epoch: [84/200], Step: [2/19], Loss: 4.5757\n",
      "Epoch: [84/200], Step: [3/19], Loss: 4.5961\n",
      "Epoch: [84/200], Step: [4/19], Loss: 4.5808\n",
      "Epoch: [84/200], Step: [5/19], Loss: 4.5983\n",
      "Epoch: [84/200], Step: [6/19], Loss: 4.5717\n",
      "Epoch: [84/200], Step: [7/19], Loss: 4.5969\n",
      "Epoch: [84/200], Step: [8/19], Loss: 4.5835\n",
      "Epoch: [84/200], Step: [9/19], Loss: 4.5692\n",
      "Epoch: [84/200], Step: [10/19], Loss: 4.5908\n",
      "Epoch: [84/200], Step: [11/19], Loss: 4.6008\n",
      "Epoch: [84/200], Step: [12/19], Loss: 4.6034\n",
      "Epoch: [84/200], Step: [13/19], Loss: 4.6100\n",
      "Epoch: [84/200], Step: [14/19], Loss: 4.5778\n",
      "Epoch: [84/200], Step: [15/19], Loss: 4.6000\n",
      "Epoch: [84/200], Step: [16/19], Loss: 4.5962\n",
      "Epoch: [84/200], Step: [17/19], Loss: 4.6019\n",
      "Epoch: [84/200], Step: [18/19], Loss: 4.5776\n",
      "Epoch: [85/200], Step: [1/19], Loss: 4.5977\n",
      "Epoch: [85/200], Step: [2/19], Loss: 4.5741\n",
      "Epoch: [85/200], Step: [3/19], Loss: 4.5978\n",
      "Epoch: [85/200], Step: [4/19], Loss: 4.6027\n",
      "Epoch: [85/200], Step: [5/19], Loss: 4.5904\n",
      "Epoch: [85/200], Step: [6/19], Loss: 4.5856\n",
      "Epoch: [85/200], Step: [7/19], Loss: 4.5927\n",
      "Epoch: [85/200], Step: [8/19], Loss: 4.5709\n",
      "Epoch: [85/200], Step: [9/19], Loss: 4.5997\n",
      "Epoch: [85/200], Step: [10/19], Loss: 4.5857\n",
      "Epoch: [85/200], Step: [11/19], Loss: 4.5794\n",
      "Epoch: [85/200], Step: [12/19], Loss: 4.5829\n",
      "Epoch: [85/200], Step: [13/19], Loss: 4.5932\n",
      "Epoch: [85/200], Step: [14/19], Loss: 4.5963\n",
      "Epoch: [85/200], Step: [15/19], Loss: 4.5946\n",
      "Epoch: [85/200], Step: [16/19], Loss: 4.5976\n",
      "Epoch: [85/200], Step: [17/19], Loss: 4.6135\n",
      "Epoch: [85/200], Step: [18/19], Loss: 4.5897\n",
      "Epoch: [86/200], Step: [1/19], Loss: 4.5963\n",
      "Epoch: [86/200], Step: [2/19], Loss: 4.5950\n",
      "Epoch: [86/200], Step: [3/19], Loss: 4.5921\n",
      "Epoch: [86/200], Step: [4/19], Loss: 4.5947\n",
      "Epoch: [86/200], Step: [5/19], Loss: 4.5821\n",
      "Epoch: [86/200], Step: [6/19], Loss: 4.5826\n",
      "Epoch: [86/200], Step: [7/19], Loss: 4.5789\n",
      "Epoch: [86/200], Step: [8/19], Loss: 4.5926\n",
      "Epoch: [86/200], Step: [9/19], Loss: 4.5841\n",
      "Epoch: [86/200], Step: [10/19], Loss: 4.5791\n",
      "Epoch: [86/200], Step: [11/19], Loss: 4.5902\n",
      "Epoch: [86/200], Step: [12/19], Loss: 4.5811\n",
      "Epoch: [86/200], Step: [13/19], Loss: 4.5986\n",
      "Epoch: [86/200], Step: [14/19], Loss: 4.6036\n",
      "Epoch: [86/200], Step: [15/19], Loss: 4.6077\n",
      "Epoch: [86/200], Step: [16/19], Loss: 4.5904\n",
      "Epoch: [86/200], Step: [17/19], Loss: 4.5979\n",
      "Epoch: [86/200], Step: [18/19], Loss: 4.5936\n",
      "Epoch: [87/200], Step: [1/19], Loss: 4.6049\n",
      "Epoch: [87/200], Step: [2/19], Loss: 4.5973\n",
      "Epoch: [87/200], Step: [3/19], Loss: 4.5840\n",
      "Epoch: [87/200], Step: [4/19], Loss: 4.6025\n",
      "Epoch: [87/200], Step: [5/19], Loss: 4.5814\n",
      "Epoch: [87/200], Step: [6/19], Loss: 4.5952\n",
      "Epoch: [87/200], Step: [7/19], Loss: 4.5971\n",
      "Epoch: [87/200], Step: [8/19], Loss: 4.5943\n",
      "Epoch: [87/200], Step: [9/19], Loss: 4.5953\n",
      "Epoch: [87/200], Step: [10/19], Loss: 4.5875\n",
      "Epoch: [87/200], Step: [11/19], Loss: 4.6003\n",
      "Epoch: [87/200], Step: [12/19], Loss: 4.5840\n",
      "Epoch: [87/200], Step: [13/19], Loss: 4.5783\n",
      "Epoch: [87/200], Step: [14/19], Loss: 4.5824\n",
      "Epoch: [87/200], Step: [15/19], Loss: 4.5903\n",
      "Epoch: [87/200], Step: [16/19], Loss: 4.5713\n",
      "Epoch: [87/200], Step: [17/19], Loss: 4.5965\n",
      "Epoch: [87/200], Step: [18/19], Loss: 4.5938\n",
      "Epoch: [88/200], Step: [1/19], Loss: 4.5965\n",
      "Epoch: [88/200], Step: [2/19], Loss: 4.5823\n",
      "Epoch: [88/200], Step: [3/19], Loss: 4.5876\n",
      "Epoch: [88/200], Step: [4/19], Loss: 4.5896\n",
      "Epoch: [88/200], Step: [5/19], Loss: 4.5748\n",
      "Epoch: [88/200], Step: [6/19], Loss: 4.6017\n",
      "Epoch: [88/200], Step: [7/19], Loss: 4.5824\n",
      "Epoch: [88/200], Step: [8/19], Loss: 4.5986\n",
      "Epoch: [88/200], Step: [9/19], Loss: 4.5927\n",
      "Epoch: [88/200], Step: [10/19], Loss: 4.6078\n",
      "Epoch: [88/200], Step: [11/19], Loss: 4.6024\n",
      "Epoch: [88/200], Step: [12/19], Loss: 4.5876\n",
      "Epoch: [88/200], Step: [13/19], Loss: 4.5939\n",
      "Epoch: [88/200], Step: [14/19], Loss: 4.5813\n",
      "Epoch: [88/200], Step: [15/19], Loss: 4.5891\n",
      "Epoch: [88/200], Step: [16/19], Loss: 4.5855\n",
      "Epoch: [88/200], Step: [17/19], Loss: 4.5935\n",
      "Epoch: [88/200], Step: [18/19], Loss: 4.5848\n",
      "Epoch: [89/200], Step: [1/19], Loss: 4.5926\n",
      "Epoch: [89/200], Step: [2/19], Loss: 4.5807\n",
      "Epoch: [89/200], Step: [3/19], Loss: 4.5904\n",
      "Epoch: [89/200], Step: [4/19], Loss: 4.6078\n",
      "Epoch: [89/200], Step: [5/19], Loss: 4.5860\n",
      "Epoch: [89/200], Step: [6/19], Loss: 4.6032\n",
      "Epoch: [89/200], Step: [7/19], Loss: 4.5972\n",
      "Epoch: [89/200], Step: [8/19], Loss: 4.5811\n",
      "Epoch: [89/200], Step: [9/19], Loss: 4.5906\n",
      "Epoch: [89/200], Step: [10/19], Loss: 4.5709\n",
      "Epoch: [89/200], Step: [11/19], Loss: 4.5871\n",
      "Epoch: [89/200], Step: [12/19], Loss: 4.5911\n",
      "Epoch: [89/200], Step: [13/19], Loss: 4.6048\n",
      "Epoch: [89/200], Step: [14/19], Loss: 4.5723\n",
      "Epoch: [89/200], Step: [15/19], Loss: 4.5916\n",
      "Epoch: [89/200], Step: [16/19], Loss: 4.5967\n",
      "Epoch: [89/200], Step: [17/19], Loss: 4.5985\n",
      "Epoch: [89/200], Step: [18/19], Loss: 4.5854\n",
      "Epoch: [90/200], Step: [1/19], Loss: 4.5911\n",
      "Epoch: [90/200], Step: [2/19], Loss: 4.5768\n",
      "Epoch: [90/200], Step: [3/19], Loss: 4.5901\n",
      "Epoch: [90/200], Step: [4/19], Loss: 4.5814\n",
      "Epoch: [90/200], Step: [5/19], Loss: 4.5852\n",
      "Epoch: [90/200], Step: [6/19], Loss: 4.5990\n",
      "Epoch: [90/200], Step: [7/19], Loss: 4.5952\n",
      "Epoch: [90/200], Step: [8/19], Loss: 4.6028\n",
      "Epoch: [90/200], Step: [9/19], Loss: 4.5964\n",
      "Epoch: [90/200], Step: [10/19], Loss: 4.5894\n",
      "Epoch: [90/200], Step: [11/19], Loss: 4.5972\n",
      "Epoch: [90/200], Step: [12/19], Loss: 4.6033\n",
      "Epoch: [90/200], Step: [13/19], Loss: 4.5906\n",
      "Epoch: [90/200], Step: [14/19], Loss: 4.5756\n",
      "Epoch: [90/200], Step: [15/19], Loss: 4.5827\n",
      "Epoch: [90/200], Step: [16/19], Loss: 4.5777\n",
      "Epoch: [90/200], Step: [17/19], Loss: 4.6007\n",
      "Epoch: [90/200], Step: [18/19], Loss: 4.5887\n",
      "Epoch: [91/200], Step: [1/19], Loss: 4.5639\n",
      "Epoch: [91/200], Step: [2/19], Loss: 4.5815\n",
      "Epoch: [91/200], Step: [3/19], Loss: 4.5961\n",
      "Epoch: [91/200], Step: [4/19], Loss: 4.5870\n",
      "Epoch: [91/200], Step: [5/19], Loss: 4.5943\n",
      "Epoch: [91/200], Step: [6/19], Loss: 4.5967\n",
      "Epoch: [91/200], Step: [7/19], Loss: 4.5742\n",
      "Epoch: [91/200], Step: [8/19], Loss: 4.5967\n",
      "Epoch: [91/200], Step: [9/19], Loss: 4.6028\n",
      "Epoch: [91/200], Step: [10/19], Loss: 4.5988\n",
      "Epoch: [91/200], Step: [11/19], Loss: 4.6076\n",
      "Epoch: [91/200], Step: [12/19], Loss: 4.6020\n",
      "Epoch: [91/200], Step: [13/19], Loss: 4.5977\n",
      "Epoch: [91/200], Step: [14/19], Loss: 4.5842\n",
      "Epoch: [91/200], Step: [15/19], Loss: 4.5688\n",
      "Epoch: [91/200], Step: [16/19], Loss: 4.5859\n",
      "Epoch: [91/200], Step: [17/19], Loss: 4.5822\n",
      "Epoch: [91/200], Step: [18/19], Loss: 4.5994\n",
      "Epoch: [92/200], Step: [1/19], Loss: 4.6026\n",
      "Epoch: [92/200], Step: [2/19], Loss: 4.5992\n",
      "Epoch: [92/200], Step: [3/19], Loss: 4.5963\n",
      "Epoch: [92/200], Step: [4/19], Loss: 4.5790\n",
      "Epoch: [92/200], Step: [5/19], Loss: 4.5846\n",
      "Epoch: [92/200], Step: [6/19], Loss: 4.5848\n",
      "Epoch: [92/200], Step: [7/19], Loss: 4.5976\n",
      "Epoch: [92/200], Step: [8/19], Loss: 4.5782\n",
      "Epoch: [92/200], Step: [9/19], Loss: 4.5698\n",
      "Epoch: [92/200], Step: [10/19], Loss: 4.5931\n",
      "Epoch: [92/200], Step: [11/19], Loss: 4.5959\n",
      "Epoch: [92/200], Step: [12/19], Loss: 4.6071\n",
      "Epoch: [92/200], Step: [13/19], Loss: 4.5748\n",
      "Epoch: [92/200], Step: [14/19], Loss: 4.5787\n",
      "Epoch: [92/200], Step: [15/19], Loss: 4.5937\n",
      "Epoch: [92/200], Step: [16/19], Loss: 4.5901\n",
      "Epoch: [92/200], Step: [17/19], Loss: 4.6042\n",
      "Epoch: [92/200], Step: [18/19], Loss: 4.5865\n",
      "Epoch: [93/200], Step: [1/19], Loss: 4.5842\n",
      "Epoch: [93/200], Step: [2/19], Loss: 4.5707\n",
      "Epoch: [93/200], Step: [3/19], Loss: 4.6052\n",
      "Epoch: [93/200], Step: [4/19], Loss: 4.5726\n",
      "Epoch: [93/200], Step: [5/19], Loss: 4.5803\n",
      "Epoch: [93/200], Step: [6/19], Loss: 4.5949\n",
      "Epoch: [93/200], Step: [7/19], Loss: 4.5941\n",
      "Epoch: [93/200], Step: [8/19], Loss: 4.5917\n",
      "Epoch: [93/200], Step: [9/19], Loss: 4.5984\n",
      "Epoch: [93/200], Step: [10/19], Loss: 4.5935\n",
      "Epoch: [93/200], Step: [11/19], Loss: 4.6109\n",
      "Epoch: [93/200], Step: [12/19], Loss: 4.6015\n",
      "Epoch: [93/200], Step: [13/19], Loss: 4.5810\n",
      "Epoch: [93/200], Step: [14/19], Loss: 4.5869\n",
      "Epoch: [93/200], Step: [15/19], Loss: 4.5761\n",
      "Epoch: [93/200], Step: [16/19], Loss: 4.5881\n",
      "Epoch: [93/200], Step: [17/19], Loss: 4.6046\n",
      "Epoch: [93/200], Step: [18/19], Loss: 4.5774\n",
      "Epoch: [94/200], Step: [1/19], Loss: 4.5676\n",
      "Epoch: [94/200], Step: [2/19], Loss: 4.6025\n",
      "Epoch: [94/200], Step: [3/19], Loss: 4.5971\n",
      "Epoch: [94/200], Step: [4/19], Loss: 4.5779\n",
      "Epoch: [94/200], Step: [5/19], Loss: 4.5739\n",
      "Epoch: [94/200], Step: [6/19], Loss: 4.5926\n",
      "Epoch: [94/200], Step: [7/19], Loss: 4.5955\n",
      "Epoch: [94/200], Step: [8/19], Loss: 4.5811\n",
      "Epoch: [94/200], Step: [9/19], Loss: 4.5754\n",
      "Epoch: [94/200], Step: [10/19], Loss: 4.5944\n",
      "Epoch: [94/200], Step: [11/19], Loss: 4.5899\n",
      "Epoch: [94/200], Step: [12/19], Loss: 4.5922\n",
      "Epoch: [94/200], Step: [13/19], Loss: 4.6012\n",
      "Epoch: [94/200], Step: [14/19], Loss: 4.5864\n",
      "Epoch: [94/200], Step: [15/19], Loss: 4.6004\n",
      "Epoch: [94/200], Step: [16/19], Loss: 4.6053\n",
      "Epoch: [94/200], Step: [17/19], Loss: 4.5958\n",
      "Epoch: [94/200], Step: [18/19], Loss: 4.5785\n",
      "Epoch: [95/200], Step: [1/19], Loss: 4.5783\n",
      "Epoch: [95/200], Step: [2/19], Loss: 4.5886\n",
      "Epoch: [95/200], Step: [3/19], Loss: 4.5929\n",
      "Epoch: [95/200], Step: [4/19], Loss: 4.5958\n",
      "Epoch: [95/200], Step: [5/19], Loss: 4.5921\n",
      "Epoch: [95/200], Step: [6/19], Loss: 4.6041\n",
      "Epoch: [95/200], Step: [7/19], Loss: 4.5870\n",
      "Epoch: [95/200], Step: [8/19], Loss: 4.5970\n",
      "Epoch: [95/200], Step: [9/19], Loss: 4.6042\n",
      "Epoch: [95/200], Step: [10/19], Loss: 4.6085\n",
      "Epoch: [95/200], Step: [11/19], Loss: 4.5811\n",
      "Epoch: [95/200], Step: [12/19], Loss: 4.6021\n",
      "Epoch: [95/200], Step: [13/19], Loss: 4.5836\n",
      "Epoch: [95/200], Step: [14/19], Loss: 4.5850\n",
      "Epoch: [95/200], Step: [15/19], Loss: 4.5739\n",
      "Epoch: [95/200], Step: [16/19], Loss: 4.5687\n",
      "Epoch: [95/200], Step: [17/19], Loss: 4.5729\n",
      "Epoch: [95/200], Step: [18/19], Loss: 4.5880\n",
      "Epoch: [96/200], Step: [1/19], Loss: 4.5812\n",
      "Epoch: [96/200], Step: [2/19], Loss: 4.6010\n",
      "Epoch: [96/200], Step: [3/19], Loss: 4.5975\n",
      "Epoch: [96/200], Step: [4/19], Loss: 4.5747\n",
      "Epoch: [96/200], Step: [5/19], Loss: 4.6092\n",
      "Epoch: [96/200], Step: [6/19], Loss: 4.5631\n",
      "Epoch: [96/200], Step: [7/19], Loss: 4.6088\n",
      "Epoch: [96/200], Step: [8/19], Loss: 4.5701\n",
      "Epoch: [96/200], Step: [9/19], Loss: 4.5797\n",
      "Epoch: [96/200], Step: [10/19], Loss: 4.5811\n",
      "Epoch: [96/200], Step: [11/19], Loss: 4.6071\n",
      "Epoch: [96/200], Step: [12/19], Loss: 4.5894\n",
      "Epoch: [96/200], Step: [13/19], Loss: 4.5912\n",
      "Epoch: [96/200], Step: [14/19], Loss: 4.5775\n",
      "Epoch: [96/200], Step: [15/19], Loss: 4.5684\n",
      "Epoch: [96/200], Step: [16/19], Loss: 4.6046\n",
      "Epoch: [96/200], Step: [17/19], Loss: 4.5968\n",
      "Epoch: [96/200], Step: [18/19], Loss: 4.5981\n",
      "Epoch: [97/200], Step: [1/19], Loss: 4.5883\n",
      "Epoch: [97/200], Step: [2/19], Loss: 4.5850\n",
      "Epoch: [97/200], Step: [3/19], Loss: 4.5790\n",
      "Epoch: [97/200], Step: [4/19], Loss: 4.5911\n",
      "Epoch: [97/200], Step: [5/19], Loss: 4.5967\n",
      "Epoch: [97/200], Step: [6/19], Loss: 4.6004\n",
      "Epoch: [97/200], Step: [7/19], Loss: 4.5837\n",
      "Epoch: [97/200], Step: [8/19], Loss: 4.6005\n",
      "Epoch: [97/200], Step: [9/19], Loss: 4.5919\n",
      "Epoch: [97/200], Step: [10/19], Loss: 4.5919\n",
      "Epoch: [97/200], Step: [11/19], Loss: 4.6090\n",
      "Epoch: [97/200], Step: [12/19], Loss: 4.5863\n",
      "Epoch: [97/200], Step: [13/19], Loss: 4.5707\n",
      "Epoch: [97/200], Step: [14/19], Loss: 4.5885\n",
      "Epoch: [97/200], Step: [15/19], Loss: 4.5646\n",
      "Epoch: [97/200], Step: [16/19], Loss: 4.6044\n",
      "Epoch: [97/200], Step: [17/19], Loss: 4.5709\n",
      "Epoch: [97/200], Step: [18/19], Loss: 4.5927\n",
      "Epoch: [98/200], Step: [1/19], Loss: 4.5918\n",
      "Epoch: [98/200], Step: [2/19], Loss: 4.5782\n",
      "Epoch: [98/200], Step: [3/19], Loss: 4.5851\n",
      "Epoch: [98/200], Step: [4/19], Loss: 4.6031\n",
      "Epoch: [98/200], Step: [5/19], Loss: 4.5852\n",
      "Epoch: [98/200], Step: [6/19], Loss: 4.5899\n",
      "Epoch: [98/200], Step: [7/19], Loss: 4.5812\n",
      "Epoch: [98/200], Step: [8/19], Loss: 4.5969\n",
      "Epoch: [98/200], Step: [9/19], Loss: 4.5888\n",
      "Epoch: [98/200], Step: [10/19], Loss: 4.5798\n",
      "Epoch: [98/200], Step: [11/19], Loss: 4.5765\n",
      "Epoch: [98/200], Step: [12/19], Loss: 4.5992\n",
      "Epoch: [98/200], Step: [13/19], Loss: 4.5967\n",
      "Epoch: [98/200], Step: [14/19], Loss: 4.5839\n",
      "Epoch: [98/200], Step: [15/19], Loss: 4.5746\n",
      "Epoch: [98/200], Step: [16/19], Loss: 4.5892\n",
      "Epoch: [98/200], Step: [17/19], Loss: 4.5868\n",
      "Epoch: [98/200], Step: [18/19], Loss: 4.6048\n",
      "Epoch: [99/200], Step: [1/19], Loss: 4.5831\n",
      "Epoch: [99/200], Step: [2/19], Loss: 4.5813\n",
      "Epoch: [99/200], Step: [3/19], Loss: 4.5873\n",
      "Epoch: [99/200], Step: [4/19], Loss: 4.5846\n",
      "Epoch: [99/200], Step: [5/19], Loss: 4.5960\n",
      "Epoch: [99/200], Step: [6/19], Loss: 4.5878\n",
      "Epoch: [99/200], Step: [7/19], Loss: 4.5797\n",
      "Epoch: [99/200], Step: [8/19], Loss: 4.6014\n",
      "Epoch: [99/200], Step: [9/19], Loss: 4.5979\n",
      "Epoch: [99/200], Step: [10/19], Loss: 4.5948\n",
      "Epoch: [99/200], Step: [11/19], Loss: 4.5575\n",
      "Epoch: [99/200], Step: [12/19], Loss: 4.5937\n",
      "Epoch: [99/200], Step: [13/19], Loss: 4.5857\n",
      "Epoch: [99/200], Step: [14/19], Loss: 4.5727\n",
      "Epoch: [99/200], Step: [15/19], Loss: 4.6049\n",
      "Epoch: [99/200], Step: [16/19], Loss: 4.5988\n",
      "Epoch: [99/200], Step: [17/19], Loss: 4.5829\n",
      "Epoch: [99/200], Step: [18/19], Loss: 4.5973\n",
      "Epoch: [100/200], Step: [1/19], Loss: 4.5909\n",
      "Epoch: [100/200], Step: [2/19], Loss: 4.5714\n",
      "Epoch: [100/200], Step: [3/19], Loss: 4.5867\n",
      "Epoch: [100/200], Step: [4/19], Loss: 4.5895\n",
      "Epoch: [100/200], Step: [5/19], Loss: 4.5915\n",
      "Epoch: [100/200], Step: [6/19], Loss: 4.5986\n",
      "Epoch: [100/200], Step: [7/19], Loss: 4.5799\n",
      "Epoch: [100/200], Step: [8/19], Loss: 4.5698\n",
      "Epoch: [100/200], Step: [9/19], Loss: 4.5971\n",
      "Epoch: [100/200], Step: [10/19], Loss: 4.5836\n",
      "Epoch: [100/200], Step: [11/19], Loss: 4.6002\n",
      "Epoch: [100/200], Step: [12/19], Loss: 4.6067\n",
      "Epoch: [100/200], Step: [13/19], Loss: 4.5740\n",
      "Epoch: [100/200], Step: [14/19], Loss: 4.6015\n",
      "Epoch: [100/200], Step: [15/19], Loss: 4.5957\n",
      "Epoch: [100/200], Step: [16/19], Loss: 4.5749\n",
      "Epoch: [100/200], Step: [17/19], Loss: 4.5807\n",
      "Epoch: [100/200], Step: [18/19], Loss: 4.5907\n",
      "Epoch: [101/200], Step: [1/19], Loss: 4.5871\n",
      "Epoch: [101/200], Step: [2/19], Loss: 4.5861\n",
      "Epoch: [101/200], Step: [3/19], Loss: 4.5897\n",
      "Epoch: [101/200], Step: [4/19], Loss: 4.5860\n",
      "Epoch: [101/200], Step: [5/19], Loss: 4.5950\n",
      "Epoch: [101/200], Step: [6/19], Loss: 4.5783\n",
      "Epoch: [101/200], Step: [7/19], Loss: 4.5963\n",
      "Epoch: [101/200], Step: [8/19], Loss: 4.5814\n",
      "Epoch: [101/200], Step: [9/19], Loss: 4.5941\n",
      "Epoch: [101/200], Step: [10/19], Loss: 4.5879\n",
      "Epoch: [101/200], Step: [11/19], Loss: 4.5816\n",
      "Epoch: [101/200], Step: [12/19], Loss: 4.5808\n",
      "Epoch: [101/200], Step: [13/19], Loss: 4.5812\n",
      "Epoch: [101/200], Step: [14/19], Loss: 4.6001\n",
      "Epoch: [101/200], Step: [15/19], Loss: 4.5896\n",
      "Epoch: [101/200], Step: [16/19], Loss: 4.5919\n",
      "Epoch: [101/200], Step: [17/19], Loss: 4.5732\n",
      "Epoch: [101/200], Step: [18/19], Loss: 4.5988\n",
      "Epoch: [102/200], Step: [1/19], Loss: 4.5895\n",
      "Epoch: [102/200], Step: [2/19], Loss: 4.5881\n",
      "Epoch: [102/200], Step: [3/19], Loss: 4.5939\n",
      "Epoch: [102/200], Step: [4/19], Loss: 4.5726\n",
      "Epoch: [102/200], Step: [5/19], Loss: 4.5985\n",
      "Epoch: [102/200], Step: [6/19], Loss: 4.6039\n",
      "Epoch: [102/200], Step: [7/19], Loss: 4.5765\n",
      "Epoch: [102/200], Step: [8/19], Loss: 4.5828\n",
      "Epoch: [102/200], Step: [9/19], Loss: 4.5967\n",
      "Epoch: [102/200], Step: [10/19], Loss: 4.5860\n",
      "Epoch: [102/200], Step: [11/19], Loss: 4.5834\n",
      "Epoch: [102/200], Step: [12/19], Loss: 4.5916\n",
      "Epoch: [102/200], Step: [13/19], Loss: 4.5924\n",
      "Epoch: [102/200], Step: [14/19], Loss: 4.5757\n",
      "Epoch: [102/200], Step: [15/19], Loss: 4.5890\n",
      "Epoch: [102/200], Step: [16/19], Loss: 4.5818\n",
      "Epoch: [102/200], Step: [17/19], Loss: 4.5903\n",
      "Epoch: [102/200], Step: [18/19], Loss: 4.5824\n",
      "Epoch: [103/200], Step: [1/19], Loss: 4.5856\n",
      "Epoch: [103/200], Step: [2/19], Loss: 4.5805\n",
      "Epoch: [103/200], Step: [3/19], Loss: 4.6048\n",
      "Epoch: [103/200], Step: [4/19], Loss: 4.6079\n",
      "Epoch: [103/200], Step: [5/19], Loss: 4.5852\n",
      "Epoch: [103/200], Step: [6/19], Loss: 4.5854\n",
      "Epoch: [103/200], Step: [7/19], Loss: 4.5710\n",
      "Epoch: [103/200], Step: [8/19], Loss: 4.5756\n",
      "Epoch: [103/200], Step: [9/19], Loss: 4.5915\n",
      "Epoch: [103/200], Step: [10/19], Loss: 4.5661\n",
      "Epoch: [103/200], Step: [11/19], Loss: 4.5802\n",
      "Epoch: [103/200], Step: [12/19], Loss: 4.5937\n",
      "Epoch: [103/200], Step: [13/19], Loss: 4.5851\n",
      "Epoch: [103/200], Step: [14/19], Loss: 4.5848\n",
      "Epoch: [103/200], Step: [15/19], Loss: 4.6005\n",
      "Epoch: [103/200], Step: [16/19], Loss: 4.5990\n",
      "Epoch: [103/200], Step: [17/19], Loss: 4.6004\n",
      "Epoch: [103/200], Step: [18/19], Loss: 4.5740\n",
      "Epoch: [104/200], Step: [1/19], Loss: 4.6004\n",
      "Epoch: [104/200], Step: [2/19], Loss: 4.5795\n",
      "Epoch: [104/200], Step: [3/19], Loss: 4.5724\n",
      "Epoch: [104/200], Step: [4/19], Loss: 4.5771\n",
      "Epoch: [104/200], Step: [5/19], Loss: 4.6050\n",
      "Epoch: [104/200], Step: [6/19], Loss: 4.5805\n",
      "Epoch: [104/200], Step: [7/19], Loss: 4.5915\n",
      "Epoch: [104/200], Step: [8/19], Loss: 4.5729\n",
      "Epoch: [104/200], Step: [9/19], Loss: 4.5870\n",
      "Epoch: [104/200], Step: [10/19], Loss: 4.5897\n",
      "Epoch: [104/200], Step: [11/19], Loss: 4.5866\n",
      "Epoch: [104/200], Step: [12/19], Loss: 4.5968\n",
      "Epoch: [104/200], Step: [13/19], Loss: 4.5962\n",
      "Epoch: [104/200], Step: [14/19], Loss: 4.5882\n",
      "Epoch: [104/200], Step: [15/19], Loss: 4.5919\n",
      "Epoch: [104/200], Step: [16/19], Loss: 4.6058\n",
      "Epoch: [104/200], Step: [17/19], Loss: 4.5649\n",
      "Epoch: [104/200], Step: [18/19], Loss: 4.5807\n",
      "Epoch: [105/200], Step: [1/19], Loss: 4.5897\n",
      "Epoch: [105/200], Step: [2/19], Loss: 4.5761\n",
      "Epoch: [105/200], Step: [3/19], Loss: 4.5842\n",
      "Epoch: [105/200], Step: [4/19], Loss: 4.6091\n",
      "Epoch: [105/200], Step: [5/19], Loss: 4.5940\n",
      "Epoch: [105/200], Step: [6/19], Loss: 4.5880\n",
      "Epoch: [105/200], Step: [7/19], Loss: 4.6081\n",
      "Epoch: [105/200], Step: [8/19], Loss: 4.5779\n",
      "Epoch: [105/200], Step: [9/19], Loss: 4.5871\n",
      "Epoch: [105/200], Step: [10/19], Loss: 4.5708\n",
      "Epoch: [105/200], Step: [11/19], Loss: 4.5917\n",
      "Epoch: [105/200], Step: [12/19], Loss: 4.6087\n",
      "Epoch: [105/200], Step: [13/19], Loss: 4.5822\n",
      "Epoch: [105/200], Step: [14/19], Loss: 4.5723\n",
      "Epoch: [105/200], Step: [15/19], Loss: 4.5840\n",
      "Epoch: [105/200], Step: [16/19], Loss: 4.5808\n",
      "Epoch: [105/200], Step: [17/19], Loss: 4.5761\n",
      "Epoch: [105/200], Step: [18/19], Loss: 4.5821\n",
      "Epoch: [106/200], Step: [1/19], Loss: 4.5948\n",
      "Epoch: [106/200], Step: [2/19], Loss: 4.5873\n",
      "Epoch: [106/200], Step: [3/19], Loss: 4.5795\n",
      "Epoch: [106/200], Step: [4/19], Loss: 4.5802\n",
      "Epoch: [106/200], Step: [5/19], Loss: 4.5926\n",
      "Epoch: [106/200], Step: [6/19], Loss: 4.5813\n",
      "Epoch: [106/200], Step: [7/19], Loss: 4.5811\n",
      "Epoch: [106/200], Step: [8/19], Loss: 4.5801\n",
      "Epoch: [106/200], Step: [9/19], Loss: 4.5955\n",
      "Epoch: [106/200], Step: [10/19], Loss: 4.5917\n",
      "Epoch: [106/200], Step: [11/19], Loss: 4.6075\n",
      "Epoch: [106/200], Step: [12/19], Loss: 4.5835\n",
      "Epoch: [106/200], Step: [13/19], Loss: 4.5899\n",
      "Epoch: [106/200], Step: [14/19], Loss: 4.5894\n",
      "Epoch: [106/200], Step: [15/19], Loss: 4.6020\n",
      "Epoch: [106/200], Step: [16/19], Loss: 4.5725\n",
      "Epoch: [106/200], Step: [17/19], Loss: 4.5586\n",
      "Epoch: [106/200], Step: [18/19], Loss: 4.5914\n",
      "Epoch: [107/200], Step: [1/19], Loss: 4.5900\n",
      "Epoch: [107/200], Step: [2/19], Loss: 4.5933\n",
      "Epoch: [107/200], Step: [3/19], Loss: 4.5934\n",
      "Epoch: [107/200], Step: [4/19], Loss: 4.5821\n",
      "Epoch: [107/200], Step: [5/19], Loss: 4.5789\n",
      "Epoch: [107/200], Step: [6/19], Loss: 4.5816\n",
      "Epoch: [107/200], Step: [7/19], Loss: 4.5821\n",
      "Epoch: [107/200], Step: [8/19], Loss: 4.5978\n",
      "Epoch: [107/200], Step: [9/19], Loss: 4.5962\n",
      "Epoch: [107/200], Step: [10/19], Loss: 4.5933\n",
      "Epoch: [107/200], Step: [11/19], Loss: 4.5719\n",
      "Epoch: [107/200], Step: [12/19], Loss: 4.5658\n",
      "Epoch: [107/200], Step: [13/19], Loss: 4.5767\n",
      "Epoch: [107/200], Step: [14/19], Loss: 4.5918\n",
      "Epoch: [107/200], Step: [15/19], Loss: 4.5901\n",
      "Epoch: [107/200], Step: [16/19], Loss: 4.5904\n",
      "Epoch: [107/200], Step: [17/19], Loss: 4.5880\n",
      "Epoch: [107/200], Step: [18/19], Loss: 4.5914\n",
      "Epoch: [108/200], Step: [1/19], Loss: 4.5926\n",
      "Epoch: [108/200], Step: [2/19], Loss: 4.5721\n",
      "Epoch: [108/200], Step: [3/19], Loss: 4.5954\n",
      "Epoch: [108/200], Step: [4/19], Loss: 4.5849\n",
      "Epoch: [108/200], Step: [5/19], Loss: 4.5918\n",
      "Epoch: [108/200], Step: [6/19], Loss: 4.5814\n",
      "Epoch: [108/200], Step: [7/19], Loss: 4.5885\n",
      "Epoch: [108/200], Step: [8/19], Loss: 4.5795\n",
      "Epoch: [108/200], Step: [9/19], Loss: 4.5794\n",
      "Epoch: [108/200], Step: [10/19], Loss: 4.5868\n",
      "Epoch: [108/200], Step: [11/19], Loss: 4.5871\n",
      "Epoch: [108/200], Step: [12/19], Loss: 4.6006\n",
      "Epoch: [108/200], Step: [13/19], Loss: 4.5920\n",
      "Epoch: [108/200], Step: [14/19], Loss: 4.5855\n",
      "Epoch: [108/200], Step: [15/19], Loss: 4.5802\n",
      "Epoch: [108/200], Step: [16/19], Loss: 4.5829\n",
      "Epoch: [108/200], Step: [17/19], Loss: 4.5788\n",
      "Epoch: [108/200], Step: [18/19], Loss: 4.5911\n",
      "Epoch: [109/200], Step: [1/19], Loss: 4.5919\n",
      "Epoch: [109/200], Step: [2/19], Loss: 4.6011\n",
      "Epoch: [109/200], Step: [3/19], Loss: 4.5891\n",
      "Epoch: [109/200], Step: [4/19], Loss: 4.5875\n",
      "Epoch: [109/200], Step: [5/19], Loss: 4.5739\n",
      "Epoch: [109/200], Step: [6/19], Loss: 4.5891\n",
      "Epoch: [109/200], Step: [7/19], Loss: 4.5916\n",
      "Epoch: [109/200], Step: [8/19], Loss: 4.5894\n",
      "Epoch: [109/200], Step: [9/19], Loss: 4.5616\n",
      "Epoch: [109/200], Step: [10/19], Loss: 4.5960\n",
      "Epoch: [109/200], Step: [11/19], Loss: 4.5998\n",
      "Epoch: [109/200], Step: [12/19], Loss: 4.5839\n",
      "Epoch: [109/200], Step: [13/19], Loss: 4.5857\n",
      "Epoch: [109/200], Step: [14/19], Loss: 4.5699\n",
      "Epoch: [109/200], Step: [15/19], Loss: 4.5735\n",
      "Epoch: [109/200], Step: [16/19], Loss: 4.6031\n",
      "Epoch: [109/200], Step: [17/19], Loss: 4.5772\n",
      "Epoch: [109/200], Step: [18/19], Loss: 4.5820\n",
      "Epoch: [110/200], Step: [1/19], Loss: 4.5826\n",
      "Epoch: [110/200], Step: [2/19], Loss: 4.5905\n",
      "Epoch: [110/200], Step: [3/19], Loss: 4.5849\n",
      "Epoch: [110/200], Step: [4/19], Loss: 4.5955\n",
      "Epoch: [110/200], Step: [5/19], Loss: 4.5972\n",
      "Epoch: [110/200], Step: [6/19], Loss: 4.5787\n",
      "Epoch: [110/200], Step: [7/19], Loss: 4.5941\n",
      "Epoch: [110/200], Step: [8/19], Loss: 4.5830\n",
      "Epoch: [110/200], Step: [9/19], Loss: 4.5993\n",
      "Epoch: [110/200], Step: [10/19], Loss: 4.5819\n",
      "Epoch: [110/200], Step: [11/19], Loss: 4.5766\n",
      "Epoch: [110/200], Step: [12/19], Loss: 4.5718\n",
      "Epoch: [110/200], Step: [13/19], Loss: 4.5812\n",
      "Epoch: [110/200], Step: [14/19], Loss: 4.5798\n",
      "Epoch: [110/200], Step: [15/19], Loss: 4.5979\n",
      "Epoch: [110/200], Step: [16/19], Loss: 4.5843\n",
      "Epoch: [110/200], Step: [17/19], Loss: 4.5886\n",
      "Epoch: [110/200], Step: [18/19], Loss: 4.5744\n",
      "Epoch: [111/200], Step: [1/19], Loss: 4.5795\n",
      "Epoch: [111/200], Step: [2/19], Loss: 4.5714\n",
      "Epoch: [111/200], Step: [3/19], Loss: 4.5773\n",
      "Epoch: [111/200], Step: [4/19], Loss: 4.5819\n",
      "Epoch: [111/200], Step: [5/19], Loss: 4.5876\n",
      "Epoch: [111/200], Step: [6/19], Loss: 4.5840\n",
      "Epoch: [111/200], Step: [7/19], Loss: 4.6026\n",
      "Epoch: [111/200], Step: [8/19], Loss: 4.5796\n",
      "Epoch: [111/200], Step: [9/19], Loss: 4.6025\n",
      "Epoch: [111/200], Step: [10/19], Loss: 4.5889\n",
      "Epoch: [111/200], Step: [11/19], Loss: 4.5782\n",
      "Epoch: [111/200], Step: [12/19], Loss: 4.5912\n",
      "Epoch: [111/200], Step: [13/19], Loss: 4.5890\n",
      "Epoch: [111/200], Step: [14/19], Loss: 4.5902\n",
      "Epoch: [111/200], Step: [15/19], Loss: 4.5844\n",
      "Epoch: [111/200], Step: [16/19], Loss: 4.5793\n",
      "Epoch: [111/200], Step: [17/19], Loss: 4.5868\n",
      "Epoch: [111/200], Step: [18/19], Loss: 4.5839\n",
      "Epoch: [112/200], Step: [1/19], Loss: 4.5978\n",
      "Epoch: [112/200], Step: [2/19], Loss: 4.6057\n",
      "Epoch: [112/200], Step: [3/19], Loss: 4.5885\n",
      "Epoch: [112/200], Step: [4/19], Loss: 4.5706\n",
      "Epoch: [112/200], Step: [5/19], Loss: 4.5834\n",
      "Epoch: [112/200], Step: [6/19], Loss: 4.5886\n",
      "Epoch: [112/200], Step: [7/19], Loss: 4.5709\n",
      "Epoch: [112/200], Step: [8/19], Loss: 4.5870\n",
      "Epoch: [112/200], Step: [9/19], Loss: 4.5916\n",
      "Epoch: [112/200], Step: [10/19], Loss: 4.5895\n",
      "Epoch: [112/200], Step: [11/19], Loss: 4.5931\n",
      "Epoch: [112/200], Step: [12/19], Loss: 4.5940\n",
      "Epoch: [112/200], Step: [13/19], Loss: 4.5636\n",
      "Epoch: [112/200], Step: [14/19], Loss: 4.5781\n",
      "Epoch: [112/200], Step: [15/19], Loss: 4.5782\n",
      "Epoch: [112/200], Step: [16/19], Loss: 4.5885\n",
      "Epoch: [112/200], Step: [17/19], Loss: 4.5924\n",
      "Epoch: [112/200], Step: [18/19], Loss: 4.5729\n",
      "Epoch: [113/200], Step: [1/19], Loss: 4.5763\n",
      "Epoch: [113/200], Step: [2/19], Loss: 4.5835\n",
      "Epoch: [113/200], Step: [3/19], Loss: 4.5781\n",
      "Epoch: [113/200], Step: [4/19], Loss: 4.5863\n",
      "Epoch: [113/200], Step: [5/19], Loss: 4.5852\n",
      "Epoch: [113/200], Step: [6/19], Loss: 4.5570\n",
      "Epoch: [113/200], Step: [7/19], Loss: 4.5929\n",
      "Epoch: [113/200], Step: [8/19], Loss: 4.5811\n",
      "Epoch: [113/200], Step: [9/19], Loss: 4.5995\n",
      "Epoch: [113/200], Step: [10/19], Loss: 4.5933\n",
      "Epoch: [113/200], Step: [11/19], Loss: 4.6055\n",
      "Epoch: [113/200], Step: [12/19], Loss: 4.5837\n",
      "Epoch: [113/200], Step: [13/19], Loss: 4.5982\n",
      "Epoch: [113/200], Step: [14/19], Loss: 4.5705\n",
      "Epoch: [113/200], Step: [15/19], Loss: 4.5891\n",
      "Epoch: [113/200], Step: [16/19], Loss: 4.5779\n",
      "Epoch: [113/200], Step: [17/19], Loss: 4.5679\n",
      "Epoch: [113/200], Step: [18/19], Loss: 4.6042\n",
      "Epoch: [114/200], Step: [1/19], Loss: 4.5907\n",
      "Epoch: [114/200], Step: [2/19], Loss: 4.5816\n",
      "Epoch: [114/200], Step: [3/19], Loss: 4.6018\n",
      "Epoch: [114/200], Step: [4/19], Loss: 4.5887\n",
      "Epoch: [114/200], Step: [5/19], Loss: 4.5687\n",
      "Epoch: [114/200], Step: [6/19], Loss: 4.5735\n",
      "Epoch: [114/200], Step: [7/19], Loss: 4.6021\n",
      "Epoch: [114/200], Step: [8/19], Loss: 4.5598\n",
      "Epoch: [114/200], Step: [9/19], Loss: 4.5815\n",
      "Epoch: [114/200], Step: [10/19], Loss: 4.5835\n",
      "Epoch: [114/200], Step: [11/19], Loss: 4.5687\n",
      "Epoch: [114/200], Step: [12/19], Loss: 4.5728\n",
      "Epoch: [114/200], Step: [13/19], Loss: 4.5965\n",
      "Epoch: [114/200], Step: [14/19], Loss: 4.5777\n",
      "Epoch: [114/200], Step: [15/19], Loss: 4.6093\n",
      "Epoch: [114/200], Step: [16/19], Loss: 4.5919\n",
      "Epoch: [114/200], Step: [17/19], Loss: 4.5940\n",
      "Epoch: [114/200], Step: [18/19], Loss: 4.5832\n",
      "Epoch: [115/200], Step: [1/19], Loss: 4.5904\n",
      "Epoch: [115/200], Step: [2/19], Loss: 4.5871\n",
      "Epoch: [115/200], Step: [3/19], Loss: 4.5736\n",
      "Epoch: [115/200], Step: [4/19], Loss: 4.5800\n",
      "Epoch: [115/200], Step: [5/19], Loss: 4.5753\n",
      "Epoch: [115/200], Step: [6/19], Loss: 4.5909\n",
      "Epoch: [115/200], Step: [7/19], Loss: 4.5917\n",
      "Epoch: [115/200], Step: [8/19], Loss: 4.5870\n",
      "Epoch: [115/200], Step: [9/19], Loss: 4.5919\n",
      "Epoch: [115/200], Step: [10/19], Loss: 4.5688\n",
      "Epoch: [115/200], Step: [11/19], Loss: 4.5958\n",
      "Epoch: [115/200], Step: [12/19], Loss: 4.5772\n",
      "Epoch: [115/200], Step: [13/19], Loss: 4.5611\n",
      "Epoch: [115/200], Step: [14/19], Loss: 4.5826\n",
      "Epoch: [115/200], Step: [15/19], Loss: 4.6001\n",
      "Epoch: [115/200], Step: [16/19], Loss: 4.6000\n",
      "Epoch: [115/200], Step: [17/19], Loss: 4.5874\n",
      "Epoch: [115/200], Step: [18/19], Loss: 4.5810\n",
      "Epoch: [116/200], Step: [1/19], Loss: 4.6023\n",
      "Epoch: [116/200], Step: [2/19], Loss: 4.5926\n",
      "Epoch: [116/200], Step: [3/19], Loss: 4.6109\n",
      "Epoch: [116/200], Step: [4/19], Loss: 4.5816\n",
      "Epoch: [116/200], Step: [5/19], Loss: 4.5952\n",
      "Epoch: [116/200], Step: [6/19], Loss: 4.5835\n",
      "Epoch: [116/200], Step: [7/19], Loss: 4.5719\n",
      "Epoch: [116/200], Step: [8/19], Loss: 4.5830\n",
      "Epoch: [116/200], Step: [9/19], Loss: 4.5740\n",
      "Epoch: [116/200], Step: [10/19], Loss: 4.5657\n",
      "Epoch: [116/200], Step: [11/19], Loss: 4.5899\n",
      "Epoch: [116/200], Step: [12/19], Loss: 4.5805\n",
      "Epoch: [116/200], Step: [13/19], Loss: 4.5789\n",
      "Epoch: [116/200], Step: [14/19], Loss: 4.5784\n",
      "Epoch: [116/200], Step: [15/19], Loss: 4.5749\n",
      "Epoch: [116/200], Step: [16/19], Loss: 4.5958\n",
      "Epoch: [116/200], Step: [17/19], Loss: 4.5794\n",
      "Epoch: [116/200], Step: [18/19], Loss: 4.5793\n",
      "Epoch: [117/200], Step: [1/19], Loss: 4.5857\n",
      "Epoch: [117/200], Step: [2/19], Loss: 4.5783\n",
      "Epoch: [117/200], Step: [3/19], Loss: 4.5655\n",
      "Epoch: [117/200], Step: [4/19], Loss: 4.5822\n",
      "Epoch: [117/200], Step: [5/19], Loss: 4.5691\n",
      "Epoch: [117/200], Step: [6/19], Loss: 4.5571\n",
      "Epoch: [117/200], Step: [7/19], Loss: 4.5854\n",
      "Epoch: [117/200], Step: [8/19], Loss: 4.5872\n",
      "Epoch: [117/200], Step: [9/19], Loss: 4.6036\n",
      "Epoch: [117/200], Step: [10/19], Loss: 4.5847\n",
      "Epoch: [117/200], Step: [11/19], Loss: 4.5815\n",
      "Epoch: [117/200], Step: [12/19], Loss: 4.5693\n",
      "Epoch: [117/200], Step: [13/19], Loss: 4.5947\n",
      "Epoch: [117/200], Step: [14/19], Loss: 4.5875\n",
      "Epoch: [117/200], Step: [15/19], Loss: 4.5968\n",
      "Epoch: [117/200], Step: [16/19], Loss: 4.5898\n",
      "Epoch: [117/200], Step: [17/19], Loss: 4.5927\n",
      "Epoch: [117/200], Step: [18/19], Loss: 4.6026\n",
      "Epoch: [118/200], Step: [1/19], Loss: 4.5864\n",
      "Epoch: [118/200], Step: [2/19], Loss: 4.5935\n",
      "Epoch: [118/200], Step: [3/19], Loss: 4.5882\n",
      "Epoch: [118/200], Step: [4/19], Loss: 4.5893\n",
      "Epoch: [118/200], Step: [5/19], Loss: 4.5797\n",
      "Epoch: [118/200], Step: [6/19], Loss: 4.5816\n",
      "Epoch: [118/200], Step: [7/19], Loss: 4.5801\n",
      "Epoch: [118/200], Step: [8/19], Loss: 4.5983\n",
      "Epoch: [118/200], Step: [9/19], Loss: 4.5818\n",
      "Epoch: [118/200], Step: [10/19], Loss: 4.5606\n",
      "Epoch: [118/200], Step: [11/19], Loss: 4.5780\n",
      "Epoch: [118/200], Step: [12/19], Loss: 4.5872\n",
      "Epoch: [118/200], Step: [13/19], Loss: 4.5774\n",
      "Epoch: [118/200], Step: [14/19], Loss: 4.5795\n",
      "Epoch: [118/200], Step: [15/19], Loss: 4.5865\n",
      "Epoch: [118/200], Step: [16/19], Loss: 4.5810\n",
      "Epoch: [118/200], Step: [17/19], Loss: 4.5870\n",
      "Epoch: [118/200], Step: [18/19], Loss: 4.5934\n",
      "Epoch: [119/200], Step: [1/19], Loss: 4.5877\n",
      "Epoch: [119/200], Step: [2/19], Loss: 4.5860\n",
      "Epoch: [119/200], Step: [3/19], Loss: 4.5951\n",
      "Epoch: [119/200], Step: [4/19], Loss: 4.5924\n",
      "Epoch: [119/200], Step: [5/19], Loss: 4.5719\n",
      "Epoch: [119/200], Step: [6/19], Loss: 4.5986\n",
      "Epoch: [119/200], Step: [7/19], Loss: 4.5736\n",
      "Epoch: [119/200], Step: [8/19], Loss: 4.5646\n",
      "Epoch: [119/200], Step: [9/19], Loss: 4.5713\n",
      "Epoch: [119/200], Step: [10/19], Loss: 4.5934\n",
      "Epoch: [119/200], Step: [11/19], Loss: 4.5656\n",
      "Epoch: [119/200], Step: [12/19], Loss: 4.5968\n",
      "Epoch: [119/200], Step: [13/19], Loss: 4.5841\n",
      "Epoch: [119/200], Step: [14/19], Loss: 4.5974\n",
      "Epoch: [119/200], Step: [15/19], Loss: 4.5905\n",
      "Epoch: [119/200], Step: [16/19], Loss: 4.5835\n",
      "Epoch: [119/200], Step: [17/19], Loss: 4.5767\n",
      "Epoch: [119/200], Step: [18/19], Loss: 4.5764\n",
      "Epoch: [120/200], Step: [1/19], Loss: 4.5774\n",
      "Epoch: [120/200], Step: [2/19], Loss: 4.5872\n",
      "Epoch: [120/200], Step: [3/19], Loss: 4.5717\n",
      "Epoch: [120/200], Step: [4/19], Loss: 4.5698\n",
      "Epoch: [120/200], Step: [5/19], Loss: 4.5764\n",
      "Epoch: [120/200], Step: [6/19], Loss: 4.5893\n",
      "Epoch: [120/200], Step: [7/19], Loss: 4.5883\n",
      "Epoch: [120/200], Step: [8/19], Loss: 4.5905\n",
      "Epoch: [120/200], Step: [9/19], Loss: 4.6004\n",
      "Epoch: [120/200], Step: [10/19], Loss: 4.5817\n",
      "Epoch: [120/200], Step: [11/19], Loss: 4.5839\n",
      "Epoch: [120/200], Step: [12/19], Loss: 4.5746\n",
      "Epoch: [120/200], Step: [13/19], Loss: 4.5879\n",
      "Epoch: [120/200], Step: [14/19], Loss: 4.5857\n",
      "Epoch: [120/200], Step: [15/19], Loss: 4.5836\n",
      "Epoch: [120/200], Step: [16/19], Loss: 4.5819\n",
      "Epoch: [120/200], Step: [17/19], Loss: 4.5966\n",
      "Epoch: [120/200], Step: [18/19], Loss: 4.5742\n",
      "Epoch: [121/200], Step: [1/19], Loss: 4.5661\n",
      "Epoch: [121/200], Step: [2/19], Loss: 4.5962\n",
      "Epoch: [121/200], Step: [3/19], Loss: 4.5823\n",
      "Epoch: [121/200], Step: [4/19], Loss: 4.5762\n",
      "Epoch: [121/200], Step: [5/19], Loss: 4.5700\n",
      "Epoch: [121/200], Step: [6/19], Loss: 4.5985\n",
      "Epoch: [121/200], Step: [7/19], Loss: 4.5901\n",
      "Epoch: [121/200], Step: [8/19], Loss: 4.5785\n",
      "Epoch: [121/200], Step: [9/19], Loss: 4.5789\n",
      "Epoch: [121/200], Step: [10/19], Loss: 4.5910\n",
      "Epoch: [121/200], Step: [11/19], Loss: 4.5716\n",
      "Epoch: [121/200], Step: [12/19], Loss: 4.5773\n",
      "Epoch: [121/200], Step: [13/19], Loss: 4.5808\n",
      "Epoch: [121/200], Step: [14/19], Loss: 4.5899\n",
      "Epoch: [121/200], Step: [15/19], Loss: 4.5858\n",
      "Epoch: [121/200], Step: [16/19], Loss: 4.5920\n",
      "Epoch: [121/200], Step: [17/19], Loss: 4.5864\n",
      "Epoch: [121/200], Step: [18/19], Loss: 4.5856\n",
      "Epoch: [122/200], Step: [1/19], Loss: 4.5828\n",
      "Epoch: [122/200], Step: [2/19], Loss: 4.5959\n",
      "Epoch: [122/200], Step: [3/19], Loss: 4.5847\n",
      "Epoch: [122/200], Step: [4/19], Loss: 4.5732\n",
      "Epoch: [122/200], Step: [5/19], Loss: 4.5670\n",
      "Epoch: [122/200], Step: [6/19], Loss: 4.5802\n",
      "Epoch: [122/200], Step: [7/19], Loss: 4.5715\n",
      "Epoch: [122/200], Step: [8/19], Loss: 4.5845\n",
      "Epoch: [122/200], Step: [9/19], Loss: 4.5836\n",
      "Epoch: [122/200], Step: [10/19], Loss: 4.5757\n",
      "Epoch: [122/200], Step: [11/19], Loss: 4.6041\n",
      "Epoch: [122/200], Step: [12/19], Loss: 4.5831\n",
      "Epoch: [122/200], Step: [13/19], Loss: 4.5832\n",
      "Epoch: [122/200], Step: [14/19], Loss: 4.5899\n",
      "Epoch: [122/200], Step: [15/19], Loss: 4.5908\n",
      "Epoch: [122/200], Step: [16/19], Loss: 4.5726\n",
      "Epoch: [122/200], Step: [17/19], Loss: 4.5871\n",
      "Epoch: [122/200], Step: [18/19], Loss: 4.5828\n",
      "Epoch: [123/200], Step: [1/19], Loss: 4.5867\n",
      "Epoch: [123/200], Step: [2/19], Loss: 4.5652\n",
      "Epoch: [123/200], Step: [3/19], Loss: 4.5971\n",
      "Epoch: [123/200], Step: [4/19], Loss: 4.5941\n",
      "Epoch: [123/200], Step: [5/19], Loss: 4.5787\n",
      "Epoch: [123/200], Step: [6/19], Loss: 4.5913\n",
      "Epoch: [123/200], Step: [7/19], Loss: 4.5848\n",
      "Epoch: [123/200], Step: [8/19], Loss: 4.5624\n",
      "Epoch: [123/200], Step: [9/19], Loss: 4.5940\n",
      "Epoch: [123/200], Step: [10/19], Loss: 4.5728\n",
      "Epoch: [123/200], Step: [11/19], Loss: 4.6049\n",
      "Epoch: [123/200], Step: [12/19], Loss: 4.5803\n",
      "Epoch: [123/200], Step: [13/19], Loss: 4.5843\n",
      "Epoch: [123/200], Step: [14/19], Loss: 4.5879\n",
      "Epoch: [123/200], Step: [15/19], Loss: 4.5699\n",
      "Epoch: [123/200], Step: [16/19], Loss: 4.5858\n",
      "Epoch: [123/200], Step: [17/19], Loss: 4.5707\n",
      "Epoch: [123/200], Step: [18/19], Loss: 4.5778\n",
      "Epoch: [124/200], Step: [1/19], Loss: 4.5923\n",
      "Epoch: [124/200], Step: [2/19], Loss: 4.5931\n",
      "Epoch: [124/200], Step: [3/19], Loss: 4.5810\n",
      "Epoch: [124/200], Step: [4/19], Loss: 4.5731\n",
      "Epoch: [124/200], Step: [5/19], Loss: 4.5741\n",
      "Epoch: [124/200], Step: [6/19], Loss: 4.5902\n",
      "Epoch: [124/200], Step: [7/19], Loss: 4.5771\n",
      "Epoch: [124/200], Step: [8/19], Loss: 4.5775\n",
      "Epoch: [124/200], Step: [9/19], Loss: 4.5849\n",
      "Epoch: [124/200], Step: [10/19], Loss: 4.5904\n",
      "Epoch: [124/200], Step: [11/19], Loss: 4.5980\n",
      "Epoch: [124/200], Step: [12/19], Loss: 4.5795\n",
      "Epoch: [124/200], Step: [13/19], Loss: 4.5733\n",
      "Epoch: [124/200], Step: [14/19], Loss: 4.5928\n",
      "Epoch: [124/200], Step: [15/19], Loss: 4.5761\n",
      "Epoch: [124/200], Step: [16/19], Loss: 4.5802\n",
      "Epoch: [124/200], Step: [17/19], Loss: 4.5721\n",
      "Epoch: [124/200], Step: [18/19], Loss: 4.5790\n",
      "Epoch: [125/200], Step: [1/19], Loss: 4.5667\n",
      "Epoch: [125/200], Step: [2/19], Loss: 4.5779\n",
      "Epoch: [125/200], Step: [3/19], Loss: 4.5884\n",
      "Epoch: [125/200], Step: [4/19], Loss: 4.5838\n",
      "Epoch: [125/200], Step: [5/19], Loss: 4.5784\n",
      "Epoch: [125/200], Step: [6/19], Loss: 4.6075\n",
      "Epoch: [125/200], Step: [7/19], Loss: 4.5782\n",
      "Epoch: [125/200], Step: [8/19], Loss: 4.5876\n",
      "Epoch: [125/200], Step: [9/19], Loss: 4.5945\n",
      "Epoch: [125/200], Step: [10/19], Loss: 4.5837\n",
      "Epoch: [125/200], Step: [11/19], Loss: 4.5799\n",
      "Epoch: [125/200], Step: [12/19], Loss: 4.5790\n",
      "Epoch: [125/200], Step: [13/19], Loss: 4.5649\n",
      "Epoch: [125/200], Step: [14/19], Loss: 4.5802\n",
      "Epoch: [125/200], Step: [15/19], Loss: 4.5768\n",
      "Epoch: [125/200], Step: [16/19], Loss: 4.5886\n",
      "Epoch: [125/200], Step: [17/19], Loss: 4.5905\n",
      "Epoch: [125/200], Step: [18/19], Loss: 4.5739\n",
      "Epoch: [126/200], Step: [1/19], Loss: 4.5674\n",
      "Epoch: [126/200], Step: [2/19], Loss: 4.5835\n",
      "Epoch: [126/200], Step: [3/19], Loss: 4.5765\n",
      "Epoch: [126/200], Step: [4/19], Loss: 4.5983\n",
      "Epoch: [126/200], Step: [5/19], Loss: 4.5677\n",
      "Epoch: [126/200], Step: [6/19], Loss: 4.5795\n",
      "Epoch: [126/200], Step: [7/19], Loss: 4.5747\n",
      "Epoch: [126/200], Step: [8/19], Loss: 4.5856\n",
      "Epoch: [126/200], Step: [9/19], Loss: 4.5659\n",
      "Epoch: [126/200], Step: [10/19], Loss: 4.5818\n",
      "Epoch: [126/200], Step: [11/19], Loss: 4.5926\n",
      "Epoch: [126/200], Step: [12/19], Loss: 4.5796\n",
      "Epoch: [126/200], Step: [13/19], Loss: 4.5894\n",
      "Epoch: [126/200], Step: [14/19], Loss: 4.5928\n",
      "Epoch: [126/200], Step: [15/19], Loss: 4.5826\n",
      "Epoch: [126/200], Step: [16/19], Loss: 4.5796\n",
      "Epoch: [126/200], Step: [17/19], Loss: 4.5903\n",
      "Epoch: [126/200], Step: [18/19], Loss: 4.5887\n",
      "Epoch: [127/200], Step: [1/19], Loss: 4.5810\n",
      "Epoch: [127/200], Step: [2/19], Loss: 4.5931\n",
      "Epoch: [127/200], Step: [3/19], Loss: 4.5712\n",
      "Epoch: [127/200], Step: [4/19], Loss: 4.5874\n",
      "Epoch: [127/200], Step: [5/19], Loss: 4.5808\n",
      "Epoch: [127/200], Step: [6/19], Loss: 4.5886\n",
      "Epoch: [127/200], Step: [7/19], Loss: 4.5931\n",
      "Epoch: [127/200], Step: [8/19], Loss: 4.5911\n",
      "Epoch: [127/200], Step: [9/19], Loss: 4.5808\n",
      "Epoch: [127/200], Step: [10/19], Loss: 4.5721\n",
      "Epoch: [127/200], Step: [11/19], Loss: 4.5731\n",
      "Epoch: [127/200], Step: [12/19], Loss: 4.5671\n",
      "Epoch: [127/200], Step: [13/19], Loss: 4.5877\n",
      "Epoch: [127/200], Step: [14/19], Loss: 4.5733\n",
      "Epoch: [127/200], Step: [15/19], Loss: 4.5773\n",
      "Epoch: [127/200], Step: [16/19], Loss: 4.5852\n",
      "Epoch: [127/200], Step: [17/19], Loss: 4.5910\n",
      "Epoch: [127/200], Step: [18/19], Loss: 4.5785\n",
      "Epoch: [128/200], Step: [1/19], Loss: 4.5639\n",
      "Epoch: [128/200], Step: [2/19], Loss: 4.5985\n",
      "Epoch: [128/200], Step: [3/19], Loss: 4.5938\n",
      "Epoch: [128/200], Step: [4/19], Loss: 4.5843\n",
      "Epoch: [128/200], Step: [5/19], Loss: 4.5781\n",
      "Epoch: [128/200], Step: [6/19], Loss: 4.5632\n",
      "Epoch: [128/200], Step: [7/19], Loss: 4.5836\n",
      "Epoch: [128/200], Step: [8/19], Loss: 4.5884\n",
      "Epoch: [128/200], Step: [9/19], Loss: 4.5928\n",
      "Epoch: [128/200], Step: [10/19], Loss: 4.5889\n",
      "Epoch: [128/200], Step: [11/19], Loss: 4.5903\n",
      "Epoch: [128/200], Step: [12/19], Loss: 4.5740\n",
      "Epoch: [128/200], Step: [13/19], Loss: 4.5793\n",
      "Epoch: [128/200], Step: [14/19], Loss: 4.5876\n",
      "Epoch: [128/200], Step: [15/19], Loss: 4.5677\n",
      "Epoch: [128/200], Step: [16/19], Loss: 4.5902\n",
      "Epoch: [128/200], Step: [17/19], Loss: 4.5754\n",
      "Epoch: [128/200], Step: [18/19], Loss: 4.5678\n",
      "Epoch: [129/200], Step: [1/19], Loss: 4.5957\n",
      "Epoch: [129/200], Step: [2/19], Loss: 4.5859\n",
      "Epoch: [129/200], Step: [3/19], Loss: 4.5789\n",
      "Epoch: [129/200], Step: [4/19], Loss: 4.5829\n",
      "Epoch: [129/200], Step: [5/19], Loss: 4.5799\n",
      "Epoch: [129/200], Step: [6/19], Loss: 4.5544\n",
      "Epoch: [129/200], Step: [7/19], Loss: 4.5943\n",
      "Epoch: [129/200], Step: [8/19], Loss: 4.5763\n",
      "Epoch: [129/200], Step: [9/19], Loss: 4.5879\n",
      "Epoch: [129/200], Step: [10/19], Loss: 4.5876\n",
      "Epoch: [129/200], Step: [11/19], Loss: 4.5764\n",
      "Epoch: [129/200], Step: [12/19], Loss: 4.5726\n",
      "Epoch: [129/200], Step: [13/19], Loss: 4.5831\n",
      "Epoch: [129/200], Step: [14/19], Loss: 4.5628\n",
      "Epoch: [129/200], Step: [15/19], Loss: 4.5795\n",
      "Epoch: [129/200], Step: [16/19], Loss: 4.5771\n",
      "Epoch: [129/200], Step: [17/19], Loss: 4.5957\n",
      "Epoch: [129/200], Step: [18/19], Loss: 4.5927\n",
      "Epoch: [130/200], Step: [1/19], Loss: 4.5857\n",
      "Epoch: [130/200], Step: [2/19], Loss: 4.5680\n",
      "Epoch: [130/200], Step: [3/19], Loss: 4.5524\n",
      "Epoch: [130/200], Step: [4/19], Loss: 4.5795\n",
      "Epoch: [130/200], Step: [5/19], Loss: 4.5705\n",
      "Epoch: [130/200], Step: [6/19], Loss: 4.5856\n",
      "Epoch: [130/200], Step: [7/19], Loss: 4.5962\n",
      "Epoch: [130/200], Step: [8/19], Loss: 4.5798\n",
      "Epoch: [130/200], Step: [9/19], Loss: 4.5806\n",
      "Epoch: [130/200], Step: [10/19], Loss: 4.5875\n",
      "Epoch: [130/200], Step: [11/19], Loss: 4.5684\n",
      "Epoch: [130/200], Step: [12/19], Loss: 4.5954\n",
      "Epoch: [130/200], Step: [13/19], Loss: 4.5913\n",
      "Epoch: [130/200], Step: [14/19], Loss: 4.5826\n",
      "Epoch: [130/200], Step: [15/19], Loss: 4.5786\n",
      "Epoch: [130/200], Step: [16/19], Loss: 4.5806\n",
      "Epoch: [130/200], Step: [17/19], Loss: 4.5990\n",
      "Epoch: [130/200], Step: [18/19], Loss: 4.5779\n",
      "Epoch: [131/200], Step: [1/19], Loss: 4.5734\n",
      "Epoch: [131/200], Step: [2/19], Loss: 4.5650\n",
      "Epoch: [131/200], Step: [3/19], Loss: 4.5731\n",
      "Epoch: [131/200], Step: [4/19], Loss: 4.6009\n",
      "Epoch: [131/200], Step: [5/19], Loss: 4.5837\n",
      "Epoch: [131/200], Step: [6/19], Loss: 4.5831\n",
      "Epoch: [131/200], Step: [7/19], Loss: 4.5862\n",
      "Epoch: [131/200], Step: [8/19], Loss: 4.5743\n",
      "Epoch: [131/200], Step: [9/19], Loss: 4.5874\n",
      "Epoch: [131/200], Step: [10/19], Loss: 4.5810\n",
      "Epoch: [131/200], Step: [11/19], Loss: 4.5866\n",
      "Epoch: [131/200], Step: [12/19], Loss: 4.5763\n",
      "Epoch: [131/200], Step: [13/19], Loss: 4.5896\n",
      "Epoch: [131/200], Step: [14/19], Loss: 4.5755\n",
      "Epoch: [131/200], Step: [15/19], Loss: 4.5777\n",
      "Epoch: [131/200], Step: [16/19], Loss: 4.5803\n",
      "Epoch: [131/200], Step: [17/19], Loss: 4.5891\n",
      "Epoch: [131/200], Step: [18/19], Loss: 4.5719\n",
      "Epoch: [132/200], Step: [1/19], Loss: 4.5720\n",
      "Epoch: [132/200], Step: [2/19], Loss: 4.5958\n",
      "Epoch: [132/200], Step: [3/19], Loss: 4.5772\n",
      "Epoch: [132/200], Step: [4/19], Loss: 4.5779\n",
      "Epoch: [132/200], Step: [5/19], Loss: 4.5952\n",
      "Epoch: [132/200], Step: [6/19], Loss: 4.5724\n",
      "Epoch: [132/200], Step: [7/19], Loss: 4.5915\n",
      "Epoch: [132/200], Step: [8/19], Loss: 4.5747\n",
      "Epoch: [132/200], Step: [9/19], Loss: 4.5672\n",
      "Epoch: [132/200], Step: [10/19], Loss: 4.5841\n",
      "Epoch: [132/200], Step: [11/19], Loss: 4.5858\n",
      "Epoch: [132/200], Step: [12/19], Loss: 4.5660\n",
      "Epoch: [132/200], Step: [13/19], Loss: 4.5813\n",
      "Epoch: [132/200], Step: [14/19], Loss: 4.5707\n",
      "Epoch: [132/200], Step: [15/19], Loss: 4.5913\n",
      "Epoch: [132/200], Step: [16/19], Loss: 4.5850\n",
      "Epoch: [132/200], Step: [17/19], Loss: 4.5810\n",
      "Epoch: [132/200], Step: [18/19], Loss: 4.5822\n",
      "Epoch: [133/200], Step: [1/19], Loss: 4.5741\n",
      "Epoch: [133/200], Step: [2/19], Loss: 4.5741\n",
      "Epoch: [133/200], Step: [3/19], Loss: 4.5918\n",
      "Epoch: [133/200], Step: [4/19], Loss: 4.5866\n",
      "Epoch: [133/200], Step: [5/19], Loss: 4.5668\n",
      "Epoch: [133/200], Step: [6/19], Loss: 4.5805\n",
      "Epoch: [133/200], Step: [7/19], Loss: 4.5807\n",
      "Epoch: [133/200], Step: [8/19], Loss: 4.5664\n",
      "Epoch: [133/200], Step: [9/19], Loss: 4.5774\n",
      "Epoch: [133/200], Step: [10/19], Loss: 4.5805\n",
      "Epoch: [133/200], Step: [11/19], Loss: 4.5748\n",
      "Epoch: [133/200], Step: [12/19], Loss: 4.5747\n",
      "Epoch: [133/200], Step: [13/19], Loss: 4.5882\n",
      "Epoch: [133/200], Step: [14/19], Loss: 4.5868\n",
      "Epoch: [133/200], Step: [15/19], Loss: 4.5738\n",
      "Epoch: [133/200], Step: [16/19], Loss: 4.5833\n",
      "Epoch: [133/200], Step: [17/19], Loss: 4.5851\n",
      "Epoch: [133/200], Step: [18/19], Loss: 4.6015\n",
      "Epoch: [134/200], Step: [1/19], Loss: 4.5649\n",
      "Epoch: [134/200], Step: [2/19], Loss: 4.5675\n",
      "Epoch: [134/200], Step: [3/19], Loss: 4.5809\n",
      "Epoch: [134/200], Step: [4/19], Loss: 4.5682\n",
      "Epoch: [134/200], Step: [5/19], Loss: 4.5725\n",
      "Epoch: [134/200], Step: [6/19], Loss: 4.5866\n",
      "Epoch: [134/200], Step: [7/19], Loss: 4.5931\n",
      "Epoch: [134/200], Step: [8/19], Loss: 4.5855\n",
      "Epoch: [134/200], Step: [9/19], Loss: 4.5941\n",
      "Epoch: [134/200], Step: [10/19], Loss: 4.5770\n",
      "Epoch: [134/200], Step: [11/19], Loss: 4.5657\n",
      "Epoch: [134/200], Step: [12/19], Loss: 4.5874\n",
      "Epoch: [134/200], Step: [13/19], Loss: 4.5791\n",
      "Epoch: [134/200], Step: [14/19], Loss: 4.5821\n",
      "Epoch: [134/200], Step: [15/19], Loss: 4.5945\n",
      "Epoch: [134/200], Step: [16/19], Loss: 4.5968\n",
      "Epoch: [134/200], Step: [17/19], Loss: 4.5858\n",
      "Epoch: [134/200], Step: [18/19], Loss: 4.5613\n",
      "Epoch: [135/200], Step: [1/19], Loss: 4.5808\n",
      "Epoch: [135/200], Step: [2/19], Loss: 4.5570\n",
      "Epoch: [135/200], Step: [3/19], Loss: 4.5918\n",
      "Epoch: [135/200], Step: [4/19], Loss: 4.5872\n",
      "Epoch: [135/200], Step: [5/19], Loss: 4.5786\n",
      "Epoch: [135/200], Step: [6/19], Loss: 4.5681\n",
      "Epoch: [135/200], Step: [7/19], Loss: 4.5716\n",
      "Epoch: [135/200], Step: [8/19], Loss: 4.5847\n",
      "Epoch: [135/200], Step: [9/19], Loss: 4.5924\n",
      "Epoch: [135/200], Step: [10/19], Loss: 4.5897\n",
      "Epoch: [135/200], Step: [11/19], Loss: 4.5789\n",
      "Epoch: [135/200], Step: [12/19], Loss: 4.5676\n",
      "Epoch: [135/200], Step: [13/19], Loss: 4.5827\n",
      "Epoch: [135/200], Step: [14/19], Loss: 4.5738\n",
      "Epoch: [135/200], Step: [15/19], Loss: 4.5874\n",
      "Epoch: [135/200], Step: [16/19], Loss: 4.5855\n",
      "Epoch: [135/200], Step: [17/19], Loss: 4.5806\n",
      "Epoch: [135/200], Step: [18/19], Loss: 4.5800\n",
      "Epoch: [136/200], Step: [1/19], Loss: 4.6018\n",
      "Epoch: [136/200], Step: [2/19], Loss: 4.5849\n",
      "Epoch: [136/200], Step: [3/19], Loss: 4.5744\n",
      "Epoch: [136/200], Step: [4/19], Loss: 4.5868\n",
      "Epoch: [136/200], Step: [5/19], Loss: 4.5936\n",
      "Epoch: [136/200], Step: [6/19], Loss: 4.5724\n",
      "Epoch: [136/200], Step: [7/19], Loss: 4.5671\n",
      "Epoch: [136/200], Step: [8/19], Loss: 4.5914\n",
      "Epoch: [136/200], Step: [9/19], Loss: 4.5676\n",
      "Epoch: [136/200], Step: [10/19], Loss: 4.5741\n",
      "Epoch: [136/200], Step: [11/19], Loss: 4.5697\n",
      "Epoch: [136/200], Step: [12/19], Loss: 4.5925\n",
      "Epoch: [136/200], Step: [13/19], Loss: 4.5785\n",
      "Epoch: [136/200], Step: [14/19], Loss: 4.5782\n",
      "Epoch: [136/200], Step: [15/19], Loss: 4.5811\n",
      "Epoch: [136/200], Step: [16/19], Loss: 4.5735\n",
      "Epoch: [136/200], Step: [17/19], Loss: 4.5741\n",
      "Epoch: [136/200], Step: [18/19], Loss: 4.5727\n",
      "Epoch: [137/200], Step: [1/19], Loss: 4.5781\n",
      "Epoch: [137/200], Step: [2/19], Loss: 4.5813\n",
      "Epoch: [137/200], Step: [3/19], Loss: 4.5788\n",
      "Epoch: [137/200], Step: [4/19], Loss: 4.5810\n",
      "Epoch: [137/200], Step: [5/19], Loss: 4.5805\n",
      "Epoch: [137/200], Step: [6/19], Loss: 4.5803\n",
      "Epoch: [137/200], Step: [7/19], Loss: 4.5910\n",
      "Epoch: [137/200], Step: [8/19], Loss: 4.5848\n",
      "Epoch: [137/200], Step: [9/19], Loss: 4.5755\n",
      "Epoch: [137/200], Step: [10/19], Loss: 4.5727\n",
      "Epoch: [137/200], Step: [11/19], Loss: 4.5769\n",
      "Epoch: [137/200], Step: [12/19], Loss: 4.5703\n",
      "Epoch: [137/200], Step: [13/19], Loss: 4.5717\n",
      "Epoch: [137/200], Step: [14/19], Loss: 4.5766\n",
      "Epoch: [137/200], Step: [15/19], Loss: 4.5878\n",
      "Epoch: [137/200], Step: [16/19], Loss: 4.5753\n",
      "Epoch: [137/200], Step: [17/19], Loss: 4.5806\n",
      "Epoch: [137/200], Step: [18/19], Loss: 4.5868\n",
      "Epoch: [138/200], Step: [1/19], Loss: 4.5658\n",
      "Epoch: [138/200], Step: [2/19], Loss: 4.5719\n",
      "Epoch: [138/200], Step: [3/19], Loss: 4.5683\n",
      "Epoch: [138/200], Step: [4/19], Loss: 4.5916\n",
      "Epoch: [138/200], Step: [5/19], Loss: 4.5931\n",
      "Epoch: [138/200], Step: [6/19], Loss: 4.5838\n",
      "Epoch: [138/200], Step: [7/19], Loss: 4.5746\n",
      "Epoch: [138/200], Step: [8/19], Loss: 4.5844\n",
      "Epoch: [138/200], Step: [9/19], Loss: 4.5782\n",
      "Epoch: [138/200], Step: [10/19], Loss: 4.5766\n",
      "Epoch: [138/200], Step: [11/19], Loss: 4.5637\n",
      "Epoch: [138/200], Step: [12/19], Loss: 4.5828\n",
      "Epoch: [138/200], Step: [13/19], Loss: 4.5794\n",
      "Epoch: [138/200], Step: [14/19], Loss: 4.5781\n",
      "Epoch: [138/200], Step: [15/19], Loss: 4.5905\n",
      "Epoch: [138/200], Step: [16/19], Loss: 4.5877\n",
      "Epoch: [138/200], Step: [17/19], Loss: 4.5810\n",
      "Epoch: [138/200], Step: [18/19], Loss: 4.5744\n",
      "Epoch: [139/200], Step: [1/19], Loss: 4.5987\n",
      "Epoch: [139/200], Step: [2/19], Loss: 4.5683\n",
      "Epoch: [139/200], Step: [3/19], Loss: 4.5927\n",
      "Epoch: [139/200], Step: [4/19], Loss: 4.5648\n",
      "Epoch: [139/200], Step: [5/19], Loss: 4.5731\n",
      "Epoch: [139/200], Step: [6/19], Loss: 4.5884\n",
      "Epoch: [139/200], Step: [7/19], Loss: 4.5824\n",
      "Epoch: [139/200], Step: [8/19], Loss: 4.5805\n",
      "Epoch: [139/200], Step: [9/19], Loss: 4.5772\n",
      "Epoch: [139/200], Step: [10/19], Loss: 4.5749\n",
      "Epoch: [139/200], Step: [11/19], Loss: 4.5644\n",
      "Epoch: [139/200], Step: [12/19], Loss: 4.5893\n",
      "Epoch: [139/200], Step: [13/19], Loss: 4.5940\n",
      "Epoch: [139/200], Step: [14/19], Loss: 4.5666\n",
      "Epoch: [139/200], Step: [15/19], Loss: 4.5680\n",
      "Epoch: [139/200], Step: [16/19], Loss: 4.5742\n",
      "Epoch: [139/200], Step: [17/19], Loss: 4.5875\n",
      "Epoch: [139/200], Step: [18/19], Loss: 4.5765\n",
      "Epoch: [140/200], Step: [1/19], Loss: 4.5836\n",
      "Epoch: [140/200], Step: [2/19], Loss: 4.5721\n",
      "Epoch: [140/200], Step: [3/19], Loss: 4.5857\n",
      "Epoch: [140/200], Step: [4/19], Loss: 4.5766\n",
      "Epoch: [140/200], Step: [5/19], Loss: 4.6030\n",
      "Epoch: [140/200], Step: [6/19], Loss: 4.5774\n",
      "Epoch: [140/200], Step: [7/19], Loss: 4.5902\n",
      "Epoch: [140/200], Step: [8/19], Loss: 4.5710\n",
      "Epoch: [140/200], Step: [9/19], Loss: 4.5823\n",
      "Epoch: [140/200], Step: [10/19], Loss: 4.5703\n",
      "Epoch: [140/200], Step: [11/19], Loss: 4.5787\n",
      "Epoch: [140/200], Step: [12/19], Loss: 4.5789\n",
      "Epoch: [140/200], Step: [13/19], Loss: 4.5802\n",
      "Epoch: [140/200], Step: [14/19], Loss: 4.5671\n",
      "Epoch: [140/200], Step: [15/19], Loss: 4.5758\n",
      "Epoch: [140/200], Step: [16/19], Loss: 4.5699\n",
      "Epoch: [140/200], Step: [17/19], Loss: 4.5749\n",
      "Epoch: [140/200], Step: [18/19], Loss: 4.5796\n",
      "Epoch: [141/200], Step: [1/19], Loss: 4.5965\n",
      "Epoch: [141/200], Step: [2/19], Loss: 4.5772\n",
      "Epoch: [141/200], Step: [3/19], Loss: 4.5837\n",
      "Epoch: [141/200], Step: [4/19], Loss: 4.5790\n",
      "Epoch: [141/200], Step: [5/19], Loss: 4.5581\n",
      "Epoch: [141/200], Step: [6/19], Loss: 4.5854\n",
      "Epoch: [141/200], Step: [7/19], Loss: 4.5793\n",
      "Epoch: [141/200], Step: [8/19], Loss: 4.5679\n",
      "Epoch: [141/200], Step: [9/19], Loss: 4.5970\n",
      "Epoch: [141/200], Step: [10/19], Loss: 4.5848\n",
      "Epoch: [141/200], Step: [11/19], Loss: 4.5770\n",
      "Epoch: [141/200], Step: [12/19], Loss: 4.5635\n",
      "Epoch: [141/200], Step: [13/19], Loss: 4.5714\n",
      "Epoch: [141/200], Step: [14/19], Loss: 4.5772\n",
      "Epoch: [141/200], Step: [15/19], Loss: 4.5769\n",
      "Epoch: [141/200], Step: [16/19], Loss: 4.5777\n",
      "Epoch: [141/200], Step: [17/19], Loss: 4.5749\n",
      "Epoch: [141/200], Step: [18/19], Loss: 4.5856\n",
      "Epoch: [142/200], Step: [1/19], Loss: 4.5970\n",
      "Epoch: [142/200], Step: [2/19], Loss: 4.5715\n",
      "Epoch: [142/200], Step: [3/19], Loss: 4.5660\n",
      "Epoch: [142/200], Step: [4/19], Loss: 4.5853\n",
      "Epoch: [142/200], Step: [5/19], Loss: 4.5662\n",
      "Epoch: [142/200], Step: [6/19], Loss: 4.5703\n",
      "Epoch: [142/200], Step: [7/19], Loss: 4.5864\n",
      "Epoch: [142/200], Step: [8/19], Loss: 4.5840\n",
      "Epoch: [142/200], Step: [9/19], Loss: 4.5895\n",
      "Epoch: [142/200], Step: [10/19], Loss: 4.5927\n",
      "Epoch: [142/200], Step: [11/19], Loss: 4.5681\n",
      "Epoch: [142/200], Step: [12/19], Loss: 4.5733\n",
      "Epoch: [142/200], Step: [13/19], Loss: 4.5859\n",
      "Epoch: [142/200], Step: [14/19], Loss: 4.5752\n",
      "Epoch: [142/200], Step: [15/19], Loss: 4.5755\n",
      "Epoch: [142/200], Step: [16/19], Loss: 4.5748\n",
      "Epoch: [142/200], Step: [17/19], Loss: 4.5770\n",
      "Epoch: [142/200], Step: [18/19], Loss: 4.5700\n",
      "Epoch: [143/200], Step: [1/19], Loss: 4.5933\n",
      "Epoch: [143/200], Step: [2/19], Loss: 4.5789\n",
      "Epoch: [143/200], Step: [3/19], Loss: 4.5534\n",
      "Epoch: [143/200], Step: [4/19], Loss: 4.5641\n",
      "Epoch: [143/200], Step: [5/19], Loss: 4.5910\n",
      "Epoch: [143/200], Step: [6/19], Loss: 4.5770\n",
      "Epoch: [143/200], Step: [7/19], Loss: 4.5723\n",
      "Epoch: [143/200], Step: [8/19], Loss: 4.5737\n",
      "Epoch: [143/200], Step: [9/19], Loss: 4.5793\n",
      "Epoch: [143/200], Step: [10/19], Loss: 4.5755\n",
      "Epoch: [143/200], Step: [11/19], Loss: 4.6015\n",
      "Epoch: [143/200], Step: [12/19], Loss: 4.5740\n",
      "Epoch: [143/200], Step: [13/19], Loss: 4.5787\n",
      "Epoch: [143/200], Step: [14/19], Loss: 4.5863\n",
      "Epoch: [143/200], Step: [15/19], Loss: 4.5768\n",
      "Epoch: [143/200], Step: [16/19], Loss: 4.5547\n",
      "Epoch: [143/200], Step: [17/19], Loss: 4.5778\n",
      "Epoch: [143/200], Step: [18/19], Loss: 4.5962\n",
      "Epoch: [144/200], Step: [1/19], Loss: 4.5726\n",
      "Epoch: [144/200], Step: [2/19], Loss: 4.5694\n",
      "Epoch: [144/200], Step: [3/19], Loss: 4.5840\n",
      "Epoch: [144/200], Step: [4/19], Loss: 4.5627\n",
      "Epoch: [144/200], Step: [5/19], Loss: 4.5723\n",
      "Epoch: [144/200], Step: [6/19], Loss: 4.5902\n",
      "Epoch: [144/200], Step: [7/19], Loss: 4.5702\n",
      "Epoch: [144/200], Step: [8/19], Loss: 4.5885\n",
      "Epoch: [144/200], Step: [9/19], Loss: 4.5891\n",
      "Epoch: [144/200], Step: [10/19], Loss: 4.5870\n",
      "Epoch: [144/200], Step: [11/19], Loss: 4.5770\n",
      "Epoch: [144/200], Step: [12/19], Loss: 4.5859\n",
      "Epoch: [144/200], Step: [13/19], Loss: 4.5680\n",
      "Epoch: [144/200], Step: [14/19], Loss: 4.5724\n",
      "Epoch: [144/200], Step: [15/19], Loss: 4.5825\n",
      "Epoch: [144/200], Step: [16/19], Loss: 4.5658\n",
      "Epoch: [144/200], Step: [17/19], Loss: 4.5868\n",
      "Epoch: [144/200], Step: [18/19], Loss: 4.5759\n",
      "Epoch: [145/200], Step: [1/19], Loss: 4.5881\n",
      "Epoch: [145/200], Step: [2/19], Loss: 4.5773\n",
      "Epoch: [145/200], Step: [3/19], Loss: 4.5854\n",
      "Epoch: [145/200], Step: [4/19], Loss: 4.5882\n",
      "Epoch: [145/200], Step: [5/19], Loss: 4.5734\n",
      "Epoch: [145/200], Step: [6/19], Loss: 4.5742\n",
      "Epoch: [145/200], Step: [7/19], Loss: 4.5920\n",
      "Epoch: [145/200], Step: [8/19], Loss: 4.5726\n",
      "Epoch: [145/200], Step: [9/19], Loss: 4.5824\n",
      "Epoch: [145/200], Step: [10/19], Loss: 4.5733\n",
      "Epoch: [145/200], Step: [11/19], Loss: 4.5737\n",
      "Epoch: [145/200], Step: [12/19], Loss: 4.5856\n",
      "Epoch: [145/200], Step: [13/19], Loss: 4.5629\n",
      "Epoch: [145/200], Step: [14/19], Loss: 4.5694\n",
      "Epoch: [145/200], Step: [15/19], Loss: 4.5661\n",
      "Epoch: [145/200], Step: [16/19], Loss: 4.5810\n",
      "Epoch: [145/200], Step: [17/19], Loss: 4.5816\n",
      "Epoch: [145/200], Step: [18/19], Loss: 4.5690\n",
      "Epoch: [146/200], Step: [1/19], Loss: 4.5731\n",
      "Epoch: [146/200], Step: [2/19], Loss: 4.5761\n",
      "Epoch: [146/200], Step: [3/19], Loss: 4.5965\n",
      "Epoch: [146/200], Step: [4/19], Loss: 4.5775\n",
      "Epoch: [146/200], Step: [5/19], Loss: 4.5653\n",
      "Epoch: [146/200], Step: [6/19], Loss: 4.5780\n",
      "Epoch: [146/200], Step: [7/19], Loss: 4.5725\n",
      "Epoch: [146/200], Step: [8/19], Loss: 4.5844\n",
      "Epoch: [146/200], Step: [9/19], Loss: 4.5623\n",
      "Epoch: [146/200], Step: [10/19], Loss: 4.5829\n",
      "Epoch: [146/200], Step: [11/19], Loss: 4.5703\n",
      "Epoch: [146/200], Step: [12/19], Loss: 4.5716\n",
      "Epoch: [146/200], Step: [13/19], Loss: 4.5923\n",
      "Epoch: [146/200], Step: [14/19], Loss: 4.5628\n",
      "Epoch: [146/200], Step: [15/19], Loss: 4.5879\n",
      "Epoch: [146/200], Step: [16/19], Loss: 4.5799\n",
      "Epoch: [146/200], Step: [17/19], Loss: 4.5785\n",
      "Epoch: [146/200], Step: [18/19], Loss: 4.5800\n",
      "Epoch: [147/200], Step: [1/19], Loss: 4.5872\n",
      "Epoch: [147/200], Step: [2/19], Loss: 4.5866\n",
      "Epoch: [147/200], Step: [3/19], Loss: 4.5736\n",
      "Epoch: [147/200], Step: [4/19], Loss: 4.5640\n",
      "Epoch: [147/200], Step: [5/19], Loss: 4.5835\n",
      "Epoch: [147/200], Step: [6/19], Loss: 4.5670\n",
      "Epoch: [147/200], Step: [7/19], Loss: 4.5875\n",
      "Epoch: [147/200], Step: [8/19], Loss: 4.5773\n",
      "Epoch: [147/200], Step: [9/19], Loss: 4.5679\n",
      "Epoch: [147/200], Step: [10/19], Loss: 4.5642\n",
      "Epoch: [147/200], Step: [11/19], Loss: 4.5767\n",
      "Epoch: [147/200], Step: [12/19], Loss: 4.5873\n",
      "Epoch: [147/200], Step: [13/19], Loss: 4.5842\n",
      "Epoch: [147/200], Step: [14/19], Loss: 4.5919\n",
      "Epoch: [147/200], Step: [15/19], Loss: 4.5723\n",
      "Epoch: [147/200], Step: [16/19], Loss: 4.5747\n",
      "Epoch: [147/200], Step: [17/19], Loss: 4.5728\n",
      "Epoch: [147/200], Step: [18/19], Loss: 4.5687\n",
      "Epoch: [148/200], Step: [1/19], Loss: 4.5824\n",
      "Epoch: [148/200], Step: [2/19], Loss: 4.5650\n",
      "Epoch: [148/200], Step: [3/19], Loss: 4.5777\n",
      "Epoch: [148/200], Step: [4/19], Loss: 4.5605\n",
      "Epoch: [148/200], Step: [5/19], Loss: 4.5633\n",
      "Epoch: [148/200], Step: [6/19], Loss: 4.5701\n",
      "Epoch: [148/200], Step: [7/19], Loss: 4.5741\n",
      "Epoch: [148/200], Step: [8/19], Loss: 4.5808\n",
      "Epoch: [148/200], Step: [9/19], Loss: 4.5838\n",
      "Epoch: [148/200], Step: [10/19], Loss: 4.5698\n",
      "Epoch: [148/200], Step: [11/19], Loss: 4.5684\n",
      "Epoch: [148/200], Step: [12/19], Loss: 4.5933\n",
      "Epoch: [148/200], Step: [13/19], Loss: 4.5869\n",
      "Epoch: [148/200], Step: [14/19], Loss: 4.5807\n",
      "Epoch: [148/200], Step: [15/19], Loss: 4.5618\n",
      "Epoch: [148/200], Step: [16/19], Loss: 4.5802\n",
      "Epoch: [148/200], Step: [17/19], Loss: 4.6053\n",
      "Epoch: [148/200], Step: [18/19], Loss: 4.5788\n",
      "Epoch: [149/200], Step: [1/19], Loss: 4.5846\n",
      "Epoch: [149/200], Step: [2/19], Loss: 4.5676\n",
      "Epoch: [149/200], Step: [3/19], Loss: 4.5693\n",
      "Epoch: [149/200], Step: [4/19], Loss: 4.5899\n",
      "Epoch: [149/200], Step: [5/19], Loss: 4.6016\n",
      "Epoch: [149/200], Step: [6/19], Loss: 4.5824\n",
      "Epoch: [149/200], Step: [7/19], Loss: 4.5820\n",
      "Epoch: [149/200], Step: [8/19], Loss: 4.5688\n",
      "Epoch: [149/200], Step: [9/19], Loss: 4.5614\n",
      "Epoch: [149/200], Step: [10/19], Loss: 4.5712\n",
      "Epoch: [149/200], Step: [11/19], Loss: 4.5832\n",
      "Epoch: [149/200], Step: [12/19], Loss: 4.5805\n",
      "Epoch: [149/200], Step: [13/19], Loss: 4.5808\n",
      "Epoch: [149/200], Step: [14/19], Loss: 4.5756\n",
      "Epoch: [149/200], Step: [15/19], Loss: 4.5713\n",
      "Epoch: [149/200], Step: [16/19], Loss: 4.5677\n",
      "Epoch: [149/200], Step: [17/19], Loss: 4.5756\n",
      "Epoch: [149/200], Step: [18/19], Loss: 4.5653\n",
      "Epoch: [150/200], Step: [1/19], Loss: 4.5667\n",
      "Epoch: [150/200], Step: [2/19], Loss: 4.5802\n",
      "Epoch: [150/200], Step: [3/19], Loss: 4.5771\n",
      "Epoch: [150/200], Step: [4/19], Loss: 4.5828\n",
      "Epoch: [150/200], Step: [5/19], Loss: 4.5666\n",
      "Epoch: [150/200], Step: [6/19], Loss: 4.5744\n",
      "Epoch: [150/200], Step: [7/19], Loss: 4.5697\n",
      "Epoch: [150/200], Step: [8/19], Loss: 4.5792\n",
      "Epoch: [150/200], Step: [9/19], Loss: 4.5607\n",
      "Epoch: [150/200], Step: [10/19], Loss: 4.5845\n",
      "Epoch: [150/200], Step: [11/19], Loss: 4.5849\n",
      "Epoch: [150/200], Step: [12/19], Loss: 4.5770\n",
      "Epoch: [150/200], Step: [13/19], Loss: 4.5872\n",
      "Epoch: [150/200], Step: [14/19], Loss: 4.5771\n",
      "Epoch: [150/200], Step: [15/19], Loss: 4.5684\n",
      "Epoch: [150/200], Step: [16/19], Loss: 4.5876\n",
      "Epoch: [150/200], Step: [17/19], Loss: 4.5816\n",
      "Epoch: [150/200], Step: [18/19], Loss: 4.5687\n",
      "Epoch: [151/200], Step: [1/19], Loss: 4.5742\n",
      "Epoch: [151/200], Step: [2/19], Loss: 4.5824\n",
      "Epoch: [151/200], Step: [3/19], Loss: 4.5837\n",
      "Epoch: [151/200], Step: [4/19], Loss: 4.5741\n",
      "Epoch: [151/200], Step: [5/19], Loss: 4.5586\n",
      "Epoch: [151/200], Step: [6/19], Loss: 4.5674\n",
      "Epoch: [151/200], Step: [7/19], Loss: 4.5879\n",
      "Epoch: [151/200], Step: [8/19], Loss: 4.5752\n",
      "Epoch: [151/200], Step: [9/19], Loss: 4.5657\n",
      "Epoch: [151/200], Step: [10/19], Loss: 4.5791\n",
      "Epoch: [151/200], Step: [11/19], Loss: 4.5680\n",
      "Epoch: [151/200], Step: [12/19], Loss: 4.5825\n",
      "Epoch: [151/200], Step: [13/19], Loss: 4.5789\n",
      "Epoch: [151/200], Step: [14/19], Loss: 4.5800\n",
      "Epoch: [151/200], Step: [15/19], Loss: 4.5834\n",
      "Epoch: [151/200], Step: [16/19], Loss: 4.5629\n",
      "Epoch: [151/200], Step: [17/19], Loss: 4.5843\n",
      "Epoch: [151/200], Step: [18/19], Loss: 4.5819\n",
      "Epoch: [152/200], Step: [1/19], Loss: 4.5807\n",
      "Epoch: [152/200], Step: [2/19], Loss: 4.5786\n",
      "Epoch: [152/200], Step: [3/19], Loss: 4.5751\n",
      "Epoch: [152/200], Step: [4/19], Loss: 4.5624\n",
      "Epoch: [152/200], Step: [5/19], Loss: 4.5626\n",
      "Epoch: [152/200], Step: [6/19], Loss: 4.5666\n",
      "Epoch: [152/200], Step: [7/19], Loss: 4.5993\n",
      "Epoch: [152/200], Step: [8/19], Loss: 4.5839\n",
      "Epoch: [152/200], Step: [9/19], Loss: 4.5797\n",
      "Epoch: [152/200], Step: [10/19], Loss: 4.5739\n",
      "Epoch: [152/200], Step: [11/19], Loss: 4.5712\n",
      "Epoch: [152/200], Step: [12/19], Loss: 4.5808\n",
      "Epoch: [152/200], Step: [13/19], Loss: 4.5797\n",
      "Epoch: [152/200], Step: [14/19], Loss: 4.5779\n",
      "Epoch: [152/200], Step: [15/19], Loss: 4.5617\n",
      "Epoch: [152/200], Step: [16/19], Loss: 4.5599\n",
      "Epoch: [152/200], Step: [17/19], Loss: 4.5744\n",
      "Epoch: [152/200], Step: [18/19], Loss: 4.5976\n",
      "Epoch: [153/200], Step: [1/19], Loss: 4.5793\n",
      "Epoch: [153/200], Step: [2/19], Loss: 4.5670\n",
      "Epoch: [153/200], Step: [3/19], Loss: 4.5925\n",
      "Epoch: [153/200], Step: [4/19], Loss: 4.5874\n",
      "Epoch: [153/200], Step: [5/19], Loss: 4.5768\n",
      "Epoch: [153/200], Step: [6/19], Loss: 4.5764\n",
      "Epoch: [153/200], Step: [7/19], Loss: 4.5731\n",
      "Epoch: [153/200], Step: [8/19], Loss: 4.5770\n",
      "Epoch: [153/200], Step: [9/19], Loss: 4.5661\n",
      "Epoch: [153/200], Step: [10/19], Loss: 4.5691\n",
      "Epoch: [153/200], Step: [11/19], Loss: 4.5900\n",
      "Epoch: [153/200], Step: [12/19], Loss: 4.5816\n",
      "Epoch: [153/200], Step: [13/19], Loss: 4.5652\n",
      "Epoch: [153/200], Step: [14/19], Loss: 4.5786\n",
      "Epoch: [153/200], Step: [15/19], Loss: 4.5771\n",
      "Epoch: [153/200], Step: [16/19], Loss: 4.5644\n",
      "Epoch: [153/200], Step: [17/19], Loss: 4.5758\n",
      "Epoch: [153/200], Step: [18/19], Loss: 4.5643\n",
      "Epoch: [154/200], Step: [1/19], Loss: 4.5760\n",
      "Epoch: [154/200], Step: [2/19], Loss: 4.5881\n",
      "Epoch: [154/200], Step: [3/19], Loss: 4.5738\n",
      "Epoch: [154/200], Step: [4/19], Loss: 4.5750\n",
      "Epoch: [154/200], Step: [5/19], Loss: 4.5576\n",
      "Epoch: [154/200], Step: [6/19], Loss: 4.5757\n",
      "Epoch: [154/200], Step: [7/19], Loss: 4.5742\n",
      "Epoch: [154/200], Step: [8/19], Loss: 4.5630\n",
      "Epoch: [154/200], Step: [9/19], Loss: 4.5749\n",
      "Epoch: [154/200], Step: [10/19], Loss: 4.5971\n",
      "Epoch: [154/200], Step: [11/19], Loss: 4.5802\n",
      "Epoch: [154/200], Step: [12/19], Loss: 4.5799\n",
      "Epoch: [154/200], Step: [13/19], Loss: 4.5787\n",
      "Epoch: [154/200], Step: [14/19], Loss: 4.5620\n",
      "Epoch: [154/200], Step: [15/19], Loss: 4.5722\n",
      "Epoch: [154/200], Step: [16/19], Loss: 4.5766\n",
      "Epoch: [154/200], Step: [17/19], Loss: 4.5680\n",
      "Epoch: [154/200], Step: [18/19], Loss: 4.5842\n",
      "Epoch: [155/200], Step: [1/19], Loss: 4.5759\n",
      "Epoch: [155/200], Step: [2/19], Loss: 4.5826\n",
      "Epoch: [155/200], Step: [3/19], Loss: 4.5857\n",
      "Epoch: [155/200], Step: [4/19], Loss: 4.5756\n",
      "Epoch: [155/200], Step: [5/19], Loss: 4.5670\n",
      "Epoch: [155/200], Step: [6/19], Loss: 4.5835\n",
      "Epoch: [155/200], Step: [7/19], Loss: 4.5695\n",
      "Epoch: [155/200], Step: [8/19], Loss: 4.5540\n",
      "Epoch: [155/200], Step: [9/19], Loss: 4.5874\n",
      "Epoch: [155/200], Step: [10/19], Loss: 4.5660\n",
      "Epoch: [155/200], Step: [11/19], Loss: 4.5815\n",
      "Epoch: [155/200], Step: [12/19], Loss: 4.5794\n",
      "Epoch: [155/200], Step: [13/19], Loss: 4.5741\n",
      "Epoch: [155/200], Step: [14/19], Loss: 4.5780\n",
      "Epoch: [155/200], Step: [15/19], Loss: 4.5675\n",
      "Epoch: [155/200], Step: [16/19], Loss: 4.5698\n",
      "Epoch: [155/200], Step: [17/19], Loss: 4.5837\n",
      "Epoch: [155/200], Step: [18/19], Loss: 4.5719\n",
      "Epoch: [156/200], Step: [1/19], Loss: 4.5813\n",
      "Epoch: [156/200], Step: [2/19], Loss: 4.5711\n",
      "Epoch: [156/200], Step: [3/19], Loss: 4.5896\n",
      "Epoch: [156/200], Step: [4/19], Loss: 4.5704\n",
      "Epoch: [156/200], Step: [5/19], Loss: 4.5765\n",
      "Epoch: [156/200], Step: [6/19], Loss: 4.5779\n",
      "Epoch: [156/200], Step: [7/19], Loss: 4.5656\n",
      "Epoch: [156/200], Step: [8/19], Loss: 4.5759\n",
      "Epoch: [156/200], Step: [9/19], Loss: 4.5669\n",
      "Epoch: [156/200], Step: [10/19], Loss: 4.5703\n",
      "Epoch: [156/200], Step: [11/19], Loss: 4.5600\n",
      "Epoch: [156/200], Step: [12/19], Loss: 4.5674\n",
      "Epoch: [156/200], Step: [13/19], Loss: 4.5761\n",
      "Epoch: [156/200], Step: [14/19], Loss: 4.5863\n",
      "Epoch: [156/200], Step: [15/19], Loss: 4.5890\n",
      "Epoch: [156/200], Step: [16/19], Loss: 4.5852\n",
      "Epoch: [156/200], Step: [17/19], Loss: 4.5756\n",
      "Epoch: [156/200], Step: [18/19], Loss: 4.5636\n",
      "Epoch: [157/200], Step: [1/19], Loss: 4.5721\n",
      "Epoch: [157/200], Step: [2/19], Loss: 4.5863\n",
      "Epoch: [157/200], Step: [3/19], Loss: 4.5603\n",
      "Epoch: [157/200], Step: [4/19], Loss: 4.5643\n",
      "Epoch: [157/200], Step: [5/19], Loss: 4.5797\n",
      "Epoch: [157/200], Step: [6/19], Loss: 4.5727\n",
      "Epoch: [157/200], Step: [7/19], Loss: 4.5736\n",
      "Epoch: [157/200], Step: [8/19], Loss: 4.5759\n",
      "Epoch: [157/200], Step: [9/19], Loss: 4.5727\n",
      "Epoch: [157/200], Step: [10/19], Loss: 4.5888\n",
      "Epoch: [157/200], Step: [11/19], Loss: 4.5920\n",
      "Epoch: [157/200], Step: [12/19], Loss: 4.5578\n",
      "Epoch: [157/200], Step: [13/19], Loss: 4.5769\n",
      "Epoch: [157/200], Step: [14/19], Loss: 4.5637\n",
      "Epoch: [157/200], Step: [15/19], Loss: 4.5622\n",
      "Epoch: [157/200], Step: [16/19], Loss: 4.5992\n",
      "Epoch: [157/200], Step: [17/19], Loss: 4.5607\n",
      "Epoch: [157/200], Step: [18/19], Loss: 4.5852\n",
      "Epoch: [158/200], Step: [1/19], Loss: 4.5813\n",
      "Epoch: [158/200], Step: [2/19], Loss: 4.5639\n",
      "Epoch: [158/200], Step: [3/19], Loss: 4.5798\n",
      "Epoch: [158/200], Step: [4/19], Loss: 4.5744\n",
      "Epoch: [158/200], Step: [5/19], Loss: 4.5850\n",
      "Epoch: [158/200], Step: [6/19], Loss: 4.5680\n",
      "Epoch: [158/200], Step: [7/19], Loss: 4.5767\n",
      "Epoch: [158/200], Step: [8/19], Loss: 4.5811\n",
      "Epoch: [158/200], Step: [9/19], Loss: 4.5708\n",
      "Epoch: [158/200], Step: [10/19], Loss: 4.5713\n",
      "Epoch: [158/200], Step: [11/19], Loss: 4.5812\n",
      "Epoch: [158/200], Step: [12/19], Loss: 4.5653\n",
      "Epoch: [158/200], Step: [13/19], Loss: 4.5813\n",
      "Epoch: [158/200], Step: [14/19], Loss: 4.5864\n",
      "Epoch: [158/200], Step: [15/19], Loss: 4.5707\n",
      "Epoch: [158/200], Step: [16/19], Loss: 4.5668\n",
      "Epoch: [158/200], Step: [17/19], Loss: 4.5750\n",
      "Epoch: [158/200], Step: [18/19], Loss: 4.5609\n",
      "Epoch: [159/200], Step: [1/19], Loss: 4.5684\n",
      "Epoch: [159/200], Step: [2/19], Loss: 4.5896\n",
      "Epoch: [159/200], Step: [3/19], Loss: 4.5771\n",
      "Epoch: [159/200], Step: [4/19], Loss: 4.5785\n",
      "Epoch: [159/200], Step: [5/19], Loss: 4.5835\n",
      "Epoch: [159/200], Step: [6/19], Loss: 4.5657\n",
      "Epoch: [159/200], Step: [7/19], Loss: 4.5654\n",
      "Epoch: [159/200], Step: [8/19], Loss: 4.5651\n",
      "Epoch: [159/200], Step: [9/19], Loss: 4.5798\n",
      "Epoch: [159/200], Step: [10/19], Loss: 4.5698\n",
      "Epoch: [159/200], Step: [11/19], Loss: 4.5966\n",
      "Epoch: [159/200], Step: [12/19], Loss: 4.5648\n",
      "Epoch: [159/200], Step: [13/19], Loss: 4.5809\n",
      "Epoch: [159/200], Step: [14/19], Loss: 4.5824\n",
      "Epoch: [159/200], Step: [15/19], Loss: 4.5620\n",
      "Epoch: [159/200], Step: [16/19], Loss: 4.5739\n",
      "Epoch: [159/200], Step: [17/19], Loss: 4.5725\n",
      "Epoch: [159/200], Step: [18/19], Loss: 4.5594\n",
      "Epoch: [160/200], Step: [1/19], Loss: 4.5793\n",
      "Epoch: [160/200], Step: [2/19], Loss: 4.5721\n",
      "Epoch: [160/200], Step: [3/19], Loss: 4.5765\n",
      "Epoch: [160/200], Step: [4/19], Loss: 4.5636\n",
      "Epoch: [160/200], Step: [5/19], Loss: 4.5851\n",
      "Epoch: [160/200], Step: [6/19], Loss: 4.5860\n",
      "Epoch: [160/200], Step: [7/19], Loss: 4.5811\n",
      "Epoch: [160/200], Step: [8/19], Loss: 4.5731\n",
      "Epoch: [160/200], Step: [9/19], Loss: 4.5686\n",
      "Epoch: [160/200], Step: [10/19], Loss: 4.5698\n",
      "Epoch: [160/200], Step: [11/19], Loss: 4.5774\n",
      "Epoch: [160/200], Step: [12/19], Loss: 4.5767\n",
      "Epoch: [160/200], Step: [13/19], Loss: 4.5844\n",
      "Epoch: [160/200], Step: [14/19], Loss: 4.5493\n",
      "Epoch: [160/200], Step: [15/19], Loss: 4.5702\n",
      "Epoch: [160/200], Step: [16/19], Loss: 4.5816\n",
      "Epoch: [160/200], Step: [17/19], Loss: 4.5825\n",
      "Epoch: [160/200], Step: [18/19], Loss: 4.5536\n",
      "Epoch: [161/200], Step: [1/19], Loss: 4.5629\n",
      "Epoch: [161/200], Step: [2/19], Loss: 4.5744\n",
      "Epoch: [161/200], Step: [3/19], Loss: 4.5768\n",
      "Epoch: [161/200], Step: [4/19], Loss: 4.5728\n",
      "Epoch: [161/200], Step: [5/19], Loss: 4.5828\n",
      "Epoch: [161/200], Step: [6/19], Loss: 4.5795\n",
      "Epoch: [161/200], Step: [7/19], Loss: 4.5622\n",
      "Epoch: [161/200], Step: [8/19], Loss: 4.5732\n",
      "Epoch: [161/200], Step: [9/19], Loss: 4.5752\n",
      "Epoch: [161/200], Step: [10/19], Loss: 4.5882\n",
      "Epoch: [161/200], Step: [11/19], Loss: 4.5724\n",
      "Epoch: [161/200], Step: [12/19], Loss: 4.5809\n",
      "Epoch: [161/200], Step: [13/19], Loss: 4.5677\n",
      "Epoch: [161/200], Step: [14/19], Loss: 4.5801\n",
      "Epoch: [161/200], Step: [15/19], Loss: 4.5638\n",
      "Epoch: [161/200], Step: [16/19], Loss: 4.5849\n",
      "Epoch: [161/200], Step: [17/19], Loss: 4.5528\n",
      "Epoch: [161/200], Step: [18/19], Loss: 4.5759\n",
      "Epoch: [162/200], Step: [1/19], Loss: 4.5669\n",
      "Epoch: [162/200], Step: [2/19], Loss: 4.5643\n",
      "Epoch: [162/200], Step: [3/19], Loss: 4.5782\n",
      "Epoch: [162/200], Step: [4/19], Loss: 4.5576\n",
      "Epoch: [162/200], Step: [5/19], Loss: 4.5695\n",
      "Epoch: [162/200], Step: [6/19], Loss: 4.5859\n",
      "Epoch: [162/200], Step: [7/19], Loss: 4.5694\n",
      "Epoch: [162/200], Step: [8/19], Loss: 4.5808\n",
      "Epoch: [162/200], Step: [9/19], Loss: 4.5852\n",
      "Epoch: [162/200], Step: [10/19], Loss: 4.5750\n",
      "Epoch: [162/200], Step: [11/19], Loss: 4.5785\n",
      "Epoch: [162/200], Step: [12/19], Loss: 4.5843\n",
      "Epoch: [162/200], Step: [13/19], Loss: 4.5574\n",
      "Epoch: [162/200], Step: [14/19], Loss: 4.5732\n",
      "Epoch: [162/200], Step: [15/19], Loss: 4.5840\n",
      "Epoch: [162/200], Step: [16/19], Loss: 4.5637\n",
      "Epoch: [162/200], Step: [17/19], Loss: 4.5779\n",
      "Epoch: [162/200], Step: [18/19], Loss: 4.5703\n",
      "Epoch: [163/200], Step: [1/19], Loss: 4.5693\n",
      "Epoch: [163/200], Step: [2/19], Loss: 4.5745\n",
      "Epoch: [163/200], Step: [3/19], Loss: 4.5955\n",
      "Epoch: [163/200], Step: [4/19], Loss: 4.5636\n",
      "Epoch: [163/200], Step: [5/19], Loss: 4.5630\n",
      "Epoch: [163/200], Step: [6/19], Loss: 4.5753\n",
      "Epoch: [163/200], Step: [7/19], Loss: 4.5790\n",
      "Epoch: [163/200], Step: [8/19], Loss: 4.5721\n",
      "Epoch: [163/200], Step: [9/19], Loss: 4.5823\n",
      "Epoch: [163/200], Step: [10/19], Loss: 4.5641\n",
      "Epoch: [163/200], Step: [11/19], Loss: 4.5739\n",
      "Epoch: [163/200], Step: [12/19], Loss: 4.5594\n",
      "Epoch: [163/200], Step: [13/19], Loss: 4.5834\n",
      "Epoch: [163/200], Step: [14/19], Loss: 4.5782\n",
      "Epoch: [163/200], Step: [15/19], Loss: 4.5765\n",
      "Epoch: [163/200], Step: [16/19], Loss: 4.5651\n",
      "Epoch: [163/200], Step: [17/19], Loss: 4.5765\n",
      "Epoch: [163/200], Step: [18/19], Loss: 4.5661\n",
      "Epoch: [164/200], Step: [1/19], Loss: 4.5776\n",
      "Epoch: [164/200], Step: [2/19], Loss: 4.5674\n",
      "Epoch: [164/200], Step: [3/19], Loss: 4.5755\n",
      "Epoch: [164/200], Step: [4/19], Loss: 4.5722\n",
      "Epoch: [164/200], Step: [5/19], Loss: 4.5757\n",
      "Epoch: [164/200], Step: [6/19], Loss: 4.5797\n",
      "Epoch: [164/200], Step: [7/19], Loss: 4.5779\n",
      "Epoch: [164/200], Step: [8/19], Loss: 4.5692\n",
      "Epoch: [164/200], Step: [9/19], Loss: 4.5705\n",
      "Epoch: [164/200], Step: [10/19], Loss: 4.5678\n",
      "Epoch: [164/200], Step: [11/19], Loss: 4.5681\n",
      "Epoch: [164/200], Step: [12/19], Loss: 4.5759\n",
      "Epoch: [164/200], Step: [13/19], Loss: 4.5751\n",
      "Epoch: [164/200], Step: [14/19], Loss: 4.5828\n",
      "Epoch: [164/200], Step: [15/19], Loss: 4.5596\n",
      "Epoch: [164/200], Step: [16/19], Loss: 4.5727\n",
      "Epoch: [164/200], Step: [17/19], Loss: 4.5716\n",
      "Epoch: [164/200], Step: [18/19], Loss: 4.5740\n",
      "Epoch: [165/200], Step: [1/19], Loss: 4.5726\n",
      "Epoch: [165/200], Step: [2/19], Loss: 4.5721\n",
      "Epoch: [165/200], Step: [3/19], Loss: 4.5726\n",
      "Epoch: [165/200], Step: [4/19], Loss: 4.5653\n",
      "Epoch: [165/200], Step: [5/19], Loss: 4.5802\n",
      "Epoch: [165/200], Step: [6/19], Loss: 4.5562\n",
      "Epoch: [165/200], Step: [7/19], Loss: 4.5708\n",
      "Epoch: [165/200], Step: [8/19], Loss: 4.5860\n",
      "Epoch: [165/200], Step: [9/19], Loss: 4.5740\n",
      "Epoch: [165/200], Step: [10/19], Loss: 4.5739\n",
      "Epoch: [165/200], Step: [11/19], Loss: 4.5794\n",
      "Epoch: [165/200], Step: [12/19], Loss: 4.5688\n",
      "Epoch: [165/200], Step: [13/19], Loss: 4.5741\n",
      "Epoch: [165/200], Step: [14/19], Loss: 4.5719\n",
      "Epoch: [165/200], Step: [15/19], Loss: 4.5734\n",
      "Epoch: [165/200], Step: [16/19], Loss: 4.5686\n",
      "Epoch: [165/200], Step: [17/19], Loss: 4.5761\n",
      "Epoch: [165/200], Step: [18/19], Loss: 4.5724\n",
      "Epoch: [166/200], Step: [1/19], Loss: 4.5618\n",
      "Epoch: [166/200], Step: [2/19], Loss: 4.5777\n",
      "Epoch: [166/200], Step: [3/19], Loss: 4.5788\n",
      "Epoch: [166/200], Step: [4/19], Loss: 4.5692\n",
      "Epoch: [166/200], Step: [5/19], Loss: 4.5638\n",
      "Epoch: [166/200], Step: [6/19], Loss: 4.5646\n",
      "Epoch: [166/200], Step: [7/19], Loss: 4.5749\n",
      "Epoch: [166/200], Step: [8/19], Loss: 4.5719\n",
      "Epoch: [166/200], Step: [9/19], Loss: 4.5973\n",
      "Epoch: [166/200], Step: [10/19], Loss: 4.5717\n",
      "Epoch: [166/200], Step: [11/19], Loss: 4.5728\n",
      "Epoch: [166/200], Step: [12/19], Loss: 4.5696\n",
      "Epoch: [166/200], Step: [13/19], Loss: 4.5668\n",
      "Epoch: [166/200], Step: [14/19], Loss: 4.5681\n",
      "Epoch: [166/200], Step: [15/19], Loss: 4.5587\n",
      "Epoch: [166/200], Step: [16/19], Loss: 4.5716\n",
      "Epoch: [166/200], Step: [17/19], Loss: 4.5763\n",
      "Epoch: [166/200], Step: [18/19], Loss: 4.5886\n",
      "Epoch: [167/200], Step: [1/19], Loss: 4.5822\n",
      "Epoch: [167/200], Step: [2/19], Loss: 4.5603\n",
      "Epoch: [167/200], Step: [3/19], Loss: 4.5754\n",
      "Epoch: [167/200], Step: [4/19], Loss: 4.5807\n",
      "Epoch: [167/200], Step: [5/19], Loss: 4.5772\n",
      "Epoch: [167/200], Step: [6/19], Loss: 4.5816\n",
      "Epoch: [167/200], Step: [7/19], Loss: 4.5776\n",
      "Epoch: [167/200], Step: [8/19], Loss: 4.5706\n",
      "Epoch: [167/200], Step: [9/19], Loss: 4.5727\n",
      "Epoch: [167/200], Step: [10/19], Loss: 4.5690\n",
      "Epoch: [167/200], Step: [11/19], Loss: 4.5646\n",
      "Epoch: [167/200], Step: [12/19], Loss: 4.5606\n",
      "Epoch: [167/200], Step: [13/19], Loss: 4.5574\n",
      "Epoch: [167/200], Step: [14/19], Loss: 4.5823\n",
      "Epoch: [167/200], Step: [15/19], Loss: 4.5736\n",
      "Epoch: [167/200], Step: [16/19], Loss: 4.5562\n",
      "Epoch: [167/200], Step: [17/19], Loss: 4.5761\n",
      "Epoch: [167/200], Step: [18/19], Loss: 4.5818\n",
      "Epoch: [168/200], Step: [1/19], Loss: 4.5555\n",
      "Epoch: [168/200], Step: [2/19], Loss: 4.5560\n",
      "Epoch: [168/200], Step: [3/19], Loss: 4.5805\n",
      "Epoch: [168/200], Step: [4/19], Loss: 4.5769\n",
      "Epoch: [168/200], Step: [5/19], Loss: 4.5619\n",
      "Epoch: [168/200], Step: [6/19], Loss: 4.5899\n",
      "Epoch: [168/200], Step: [7/19], Loss: 4.5605\n",
      "Epoch: [168/200], Step: [8/19], Loss: 4.5945\n",
      "Epoch: [168/200], Step: [9/19], Loss: 4.5829\n",
      "Epoch: [168/200], Step: [10/19], Loss: 4.5516\n",
      "Epoch: [168/200], Step: [11/19], Loss: 4.5769\n",
      "Epoch: [168/200], Step: [12/19], Loss: 4.5745\n",
      "Epoch: [168/200], Step: [13/19], Loss: 4.5740\n",
      "Epoch: [168/200], Step: [14/19], Loss: 4.5586\n",
      "Epoch: [168/200], Step: [15/19], Loss: 4.5808\n",
      "Epoch: [168/200], Step: [16/19], Loss: 4.5733\n",
      "Epoch: [168/200], Step: [17/19], Loss: 4.5660\n",
      "Epoch: [168/200], Step: [18/19], Loss: 4.5813\n",
      "Epoch: [169/200], Step: [1/19], Loss: 4.5769\n",
      "Epoch: [169/200], Step: [2/19], Loss: 4.5708\n",
      "Epoch: [169/200], Step: [3/19], Loss: 4.5733\n",
      "Epoch: [169/200], Step: [4/19], Loss: 4.5741\n",
      "Epoch: [169/200], Step: [5/19], Loss: 4.5792\n",
      "Epoch: [169/200], Step: [6/19], Loss: 4.5676\n",
      "Epoch: [169/200], Step: [7/19], Loss: 4.5663\n",
      "Epoch: [169/200], Step: [8/19], Loss: 4.5766\n",
      "Epoch: [169/200], Step: [9/19], Loss: 4.5703\n",
      "Epoch: [169/200], Step: [10/19], Loss: 4.5837\n",
      "Epoch: [169/200], Step: [11/19], Loss: 4.5828\n",
      "Epoch: [169/200], Step: [12/19], Loss: 4.5896\n",
      "Epoch: [169/200], Step: [13/19], Loss: 4.5558\n",
      "Epoch: [169/200], Step: [14/19], Loss: 4.5644\n",
      "Epoch: [169/200], Step: [15/19], Loss: 4.5828\n",
      "Epoch: [169/200], Step: [16/19], Loss: 4.5549\n",
      "Epoch: [169/200], Step: [17/19], Loss: 4.5567\n",
      "Epoch: [169/200], Step: [18/19], Loss: 4.5653\n",
      "Epoch: [170/200], Step: [1/19], Loss: 4.5726\n",
      "Epoch: [170/200], Step: [2/19], Loss: 4.5856\n",
      "Epoch: [170/200], Step: [3/19], Loss: 4.5651\n",
      "Epoch: [170/200], Step: [4/19], Loss: 4.5540\n",
      "Epoch: [170/200], Step: [5/19], Loss: 4.5576\n",
      "Epoch: [170/200], Step: [6/19], Loss: 4.5799\n",
      "Epoch: [170/200], Step: [7/19], Loss: 4.5811\n",
      "Epoch: [170/200], Step: [8/19], Loss: 4.5746\n",
      "Epoch: [170/200], Step: [9/19], Loss: 4.5674\n",
      "Epoch: [170/200], Step: [10/19], Loss: 4.5747\n",
      "Epoch: [170/200], Step: [11/19], Loss: 4.5662\n",
      "Epoch: [170/200], Step: [12/19], Loss: 4.5632\n",
      "Epoch: [170/200], Step: [13/19], Loss: 4.5676\n",
      "Epoch: [170/200], Step: [14/19], Loss: 4.5755\n",
      "Epoch: [170/200], Step: [15/19], Loss: 4.5809\n",
      "Epoch: [170/200], Step: [16/19], Loss: 4.5773\n",
      "Epoch: [170/200], Step: [17/19], Loss: 4.5736\n",
      "Epoch: [170/200], Step: [18/19], Loss: 4.5697\n",
      "Epoch: [171/200], Step: [1/19], Loss: 4.5642\n",
      "Epoch: [171/200], Step: [2/19], Loss: 4.5807\n",
      "Epoch: [171/200], Step: [3/19], Loss: 4.5792\n",
      "Epoch: [171/200], Step: [4/19], Loss: 4.5831\n",
      "Epoch: [171/200], Step: [5/19], Loss: 4.5628\n",
      "Epoch: [171/200], Step: [6/19], Loss: 4.5859\n",
      "Epoch: [171/200], Step: [7/19], Loss: 4.5728\n",
      "Epoch: [171/200], Step: [8/19], Loss: 4.5732\n",
      "Epoch: [171/200], Step: [9/19], Loss: 4.5604\n",
      "Epoch: [171/200], Step: [10/19], Loss: 4.5619\n",
      "Epoch: [171/200], Step: [11/19], Loss: 4.5697\n",
      "Epoch: [171/200], Step: [12/19], Loss: 4.5821\n",
      "Epoch: [171/200], Step: [13/19], Loss: 4.5650\n",
      "Epoch: [171/200], Step: [14/19], Loss: 4.5755\n",
      "Epoch: [171/200], Step: [15/19], Loss: 4.5615\n",
      "Epoch: [171/200], Step: [16/19], Loss: 4.5640\n",
      "Epoch: [171/200], Step: [17/19], Loss: 4.5815\n",
      "Epoch: [171/200], Step: [18/19], Loss: 4.5586\n",
      "Epoch: [172/200], Step: [1/19], Loss: 4.5726\n",
      "Epoch: [172/200], Step: [2/19], Loss: 4.5766\n",
      "Epoch: [172/200], Step: [3/19], Loss: 4.5689\n",
      "Epoch: [172/200], Step: [4/19], Loss: 4.5534\n",
      "Epoch: [172/200], Step: [5/19], Loss: 4.5659\n",
      "Epoch: [172/200], Step: [6/19], Loss: 4.5732\n",
      "Epoch: [172/200], Step: [7/19], Loss: 4.5463\n",
      "Epoch: [172/200], Step: [8/19], Loss: 4.5697\n",
      "Epoch: [172/200], Step: [9/19], Loss: 4.5814\n",
      "Epoch: [172/200], Step: [10/19], Loss: 4.5907\n",
      "Epoch: [172/200], Step: [11/19], Loss: 4.5633\n",
      "Epoch: [172/200], Step: [12/19], Loss: 4.5681\n",
      "Epoch: [172/200], Step: [13/19], Loss: 4.5702\n",
      "Epoch: [172/200], Step: [14/19], Loss: 4.5892\n",
      "Epoch: [172/200], Step: [15/19], Loss: 4.5763\n",
      "Epoch: [172/200], Step: [16/19], Loss: 4.5733\n",
      "Epoch: [172/200], Step: [17/19], Loss: 4.5686\n",
      "Epoch: [172/200], Step: [18/19], Loss: 4.5699\n",
      "Epoch: [173/200], Step: [1/19], Loss: 4.5601\n",
      "Epoch: [173/200], Step: [2/19], Loss: 4.5572\n",
      "Epoch: [173/200], Step: [3/19], Loss: 4.5813\n",
      "Epoch: [173/200], Step: [4/19], Loss: 4.5704\n",
      "Epoch: [173/200], Step: [5/19], Loss: 4.5597\n",
      "Epoch: [173/200], Step: [6/19], Loss: 4.5702\n",
      "Epoch: [173/200], Step: [7/19], Loss: 4.5860\n",
      "Epoch: [173/200], Step: [8/19], Loss: 4.5823\n",
      "Epoch: [173/200], Step: [9/19], Loss: 4.5688\n",
      "Epoch: [173/200], Step: [10/19], Loss: 4.5725\n",
      "Epoch: [173/200], Step: [11/19], Loss: 4.5815\n",
      "Epoch: [173/200], Step: [12/19], Loss: 4.5631\n",
      "Epoch: [173/200], Step: [13/19], Loss: 4.5749\n",
      "Epoch: [173/200], Step: [14/19], Loss: 4.5715\n",
      "Epoch: [173/200], Step: [15/19], Loss: 4.5694\n",
      "Epoch: [173/200], Step: [16/19], Loss: 4.5706\n",
      "Epoch: [173/200], Step: [17/19], Loss: 4.5629\n",
      "Epoch: [173/200], Step: [18/19], Loss: 4.5707\n",
      "Epoch: [174/200], Step: [1/19], Loss: 4.5668\n",
      "Epoch: [174/200], Step: [2/19], Loss: 4.5797\n",
      "Epoch: [174/200], Step: [3/19], Loss: 4.5676\n",
      "Epoch: [174/200], Step: [4/19], Loss: 4.5707\n",
      "Epoch: [174/200], Step: [5/19], Loss: 4.5751\n",
      "Epoch: [174/200], Step: [6/19], Loss: 4.5685\n",
      "Epoch: [174/200], Step: [7/19], Loss: 4.5686\n",
      "Epoch: [174/200], Step: [8/19], Loss: 4.5821\n",
      "Epoch: [174/200], Step: [9/19], Loss: 4.5633\n",
      "Epoch: [174/200], Step: [10/19], Loss: 4.5674\n",
      "Epoch: [174/200], Step: [11/19], Loss: 4.5675\n",
      "Epoch: [174/200], Step: [12/19], Loss: 4.5760\n",
      "Epoch: [174/200], Step: [13/19], Loss: 4.5449\n",
      "Epoch: [174/200], Step: [14/19], Loss: 4.5709\n",
      "Epoch: [174/200], Step: [15/19], Loss: 4.5861\n",
      "Epoch: [174/200], Step: [16/19], Loss: 4.5603\n",
      "Epoch: [174/200], Step: [17/19], Loss: 4.5846\n",
      "Epoch: [174/200], Step: [18/19], Loss: 4.5686\n",
      "Epoch: [175/200], Step: [1/19], Loss: 4.5613\n",
      "Epoch: [175/200], Step: [2/19], Loss: 4.5709\n",
      "Epoch: [175/200], Step: [3/19], Loss: 4.5711\n",
      "Epoch: [175/200], Step: [4/19], Loss: 4.5545\n",
      "Epoch: [175/200], Step: [5/19], Loss: 4.5936\n",
      "Epoch: [175/200], Step: [6/19], Loss: 4.5856\n",
      "Epoch: [175/200], Step: [7/19], Loss: 4.5648\n",
      "Epoch: [175/200], Step: [8/19], Loss: 4.5876\n",
      "Epoch: [175/200], Step: [9/19], Loss: 4.5597\n",
      "Epoch: [175/200], Step: [10/19], Loss: 4.5657\n",
      "Epoch: [175/200], Step: [11/19], Loss: 4.5712\n",
      "Epoch: [175/200], Step: [12/19], Loss: 4.5660\n",
      "Epoch: [175/200], Step: [13/19], Loss: 4.5634\n",
      "Epoch: [175/200], Step: [14/19], Loss: 4.5638\n",
      "Epoch: [175/200], Step: [15/19], Loss: 4.5692\n",
      "Epoch: [175/200], Step: [16/19], Loss: 4.5794\n",
      "Epoch: [175/200], Step: [17/19], Loss: 4.5727\n",
      "Epoch: [175/200], Step: [18/19], Loss: 4.5636\n",
      "Epoch: [176/200], Step: [1/19], Loss: 4.5806\n",
      "Epoch: [176/200], Step: [2/19], Loss: 4.5736\n",
      "Epoch: [176/200], Step: [3/19], Loss: 4.5646\n",
      "Epoch: [176/200], Step: [4/19], Loss: 4.5758\n",
      "Epoch: [176/200], Step: [5/19], Loss: 4.5620\n",
      "Epoch: [176/200], Step: [6/19], Loss: 4.5640\n",
      "Epoch: [176/200], Step: [7/19], Loss: 4.5652\n",
      "Epoch: [176/200], Step: [8/19], Loss: 4.5726\n",
      "Epoch: [176/200], Step: [9/19], Loss: 4.5639\n",
      "Epoch: [176/200], Step: [10/19], Loss: 4.5518\n",
      "Epoch: [176/200], Step: [11/19], Loss: 4.5748\n",
      "Epoch: [176/200], Step: [12/19], Loss: 4.5745\n",
      "Epoch: [176/200], Step: [13/19], Loss: 4.5799\n",
      "Epoch: [176/200], Step: [14/19], Loss: 4.5835\n",
      "Epoch: [176/200], Step: [15/19], Loss: 4.5794\n",
      "Epoch: [176/200], Step: [16/19], Loss: 4.5697\n",
      "Epoch: [176/200], Step: [17/19], Loss: 4.5768\n",
      "Epoch: [176/200], Step: [18/19], Loss: 4.5466\n",
      "Epoch: [177/200], Step: [1/19], Loss: 4.5708\n",
      "Epoch: [177/200], Step: [2/19], Loss: 4.5727\n",
      "Epoch: [177/200], Step: [3/19], Loss: 4.5615\n",
      "Epoch: [177/200], Step: [4/19], Loss: 4.5634\n",
      "Epoch: [177/200], Step: [5/19], Loss: 4.5893\n",
      "Epoch: [177/200], Step: [6/19], Loss: 4.5586\n",
      "Epoch: [177/200], Step: [7/19], Loss: 4.5655\n",
      "Epoch: [177/200], Step: [8/19], Loss: 4.5564\n",
      "Epoch: [177/200], Step: [9/19], Loss: 4.5602\n",
      "Epoch: [177/200], Step: [10/19], Loss: 4.5686\n",
      "Epoch: [177/200], Step: [11/19], Loss: 4.5587\n",
      "Epoch: [177/200], Step: [12/19], Loss: 4.5792\n",
      "Epoch: [177/200], Step: [13/19], Loss: 4.5698\n",
      "Epoch: [177/200], Step: [14/19], Loss: 4.5772\n",
      "Epoch: [177/200], Step: [15/19], Loss: 4.5791\n",
      "Epoch: [177/200], Step: [16/19], Loss: 4.5737\n",
      "Epoch: [177/200], Step: [17/19], Loss: 4.5791\n",
      "Epoch: [177/200], Step: [18/19], Loss: 4.5712\n",
      "Epoch: [178/200], Step: [1/19], Loss: 4.5566\n",
      "Epoch: [178/200], Step: [2/19], Loss: 4.5770\n",
      "Epoch: [178/200], Step: [3/19], Loss: 4.5722\n",
      "Epoch: [178/200], Step: [4/19], Loss: 4.5550\n",
      "Epoch: [178/200], Step: [5/19], Loss: 4.5744\n",
      "Epoch: [178/200], Step: [6/19], Loss: 4.6004\n",
      "Epoch: [178/200], Step: [7/19], Loss: 4.5562\n",
      "Epoch: [178/200], Step: [8/19], Loss: 4.5656\n",
      "Epoch: [178/200], Step: [9/19], Loss: 4.5704\n",
      "Epoch: [178/200], Step: [10/19], Loss: 4.5744\n",
      "Epoch: [178/200], Step: [11/19], Loss: 4.5548\n",
      "Epoch: [178/200], Step: [12/19], Loss: 4.5805\n",
      "Epoch: [178/200], Step: [13/19], Loss: 4.5796\n",
      "Epoch: [178/200], Step: [14/19], Loss: 4.5827\n",
      "Epoch: [178/200], Step: [15/19], Loss: 4.5629\n",
      "Epoch: [178/200], Step: [16/19], Loss: 4.5559\n",
      "Epoch: [178/200], Step: [17/19], Loss: 4.5566\n",
      "Epoch: [178/200], Step: [18/19], Loss: 4.5753\n",
      "Epoch: [179/200], Step: [1/19], Loss: 4.5796\n",
      "Epoch: [179/200], Step: [2/19], Loss: 4.5692\n",
      "Epoch: [179/200], Step: [3/19], Loss: 4.5844\n",
      "Epoch: [179/200], Step: [4/19], Loss: 4.5742\n",
      "Epoch: [179/200], Step: [5/19], Loss: 4.5616\n",
      "Epoch: [179/200], Step: [6/19], Loss: 4.5668\n",
      "Epoch: [179/200], Step: [7/19], Loss: 4.5612\n",
      "Epoch: [179/200], Step: [8/19], Loss: 4.5757\n",
      "Epoch: [179/200], Step: [9/19], Loss: 4.5773\n",
      "Epoch: [179/200], Step: [10/19], Loss: 4.5756\n",
      "Epoch: [179/200], Step: [11/19], Loss: 4.5637\n",
      "Epoch: [179/200], Step: [12/19], Loss: 4.5641\n",
      "Epoch: [179/200], Step: [13/19], Loss: 4.5619\n",
      "Epoch: [179/200], Step: [14/19], Loss: 4.5650\n",
      "Epoch: [179/200], Step: [15/19], Loss: 4.5787\n",
      "Epoch: [179/200], Step: [16/19], Loss: 4.5654\n",
      "Epoch: [179/200], Step: [17/19], Loss: 4.5571\n",
      "Epoch: [179/200], Step: [18/19], Loss: 4.5642\n",
      "Epoch: [180/200], Step: [1/19], Loss: 4.5638\n",
      "Epoch: [180/200], Step: [2/19], Loss: 4.5650\n",
      "Epoch: [180/200], Step: [3/19], Loss: 4.5742\n",
      "Epoch: [180/200], Step: [4/19], Loss: 4.5663\n",
      "Epoch: [180/200], Step: [5/19], Loss: 4.5729\n",
      "Epoch: [180/200], Step: [6/19], Loss: 4.5587\n",
      "Epoch: [180/200], Step: [7/19], Loss: 4.5704\n",
      "Epoch: [180/200], Step: [8/19], Loss: 4.5696\n",
      "Epoch: [180/200], Step: [9/19], Loss: 4.5658\n",
      "Epoch: [180/200], Step: [10/19], Loss: 4.5768\n",
      "Epoch: [180/200], Step: [11/19], Loss: 4.5774\n",
      "Epoch: [180/200], Step: [12/19], Loss: 4.5747\n",
      "Epoch: [180/200], Step: [13/19], Loss: 4.5618\n",
      "Epoch: [180/200], Step: [14/19], Loss: 4.5638\n",
      "Epoch: [180/200], Step: [15/19], Loss: 4.5688\n",
      "Epoch: [180/200], Step: [16/19], Loss: 4.5607\n",
      "Epoch: [180/200], Step: [17/19], Loss: 4.5776\n",
      "Epoch: [180/200], Step: [18/19], Loss: 4.5728\n",
      "Epoch: [181/200], Step: [1/19], Loss: 4.5701\n",
      "Epoch: [181/200], Step: [2/19], Loss: 4.5598\n",
      "Epoch: [181/200], Step: [3/19], Loss: 4.5632\n",
      "Epoch: [181/200], Step: [4/19], Loss: 4.5689\n",
      "Epoch: [181/200], Step: [5/19], Loss: 4.5696\n",
      "Epoch: [181/200], Step: [6/19], Loss: 4.5642\n",
      "Epoch: [181/200], Step: [7/19], Loss: 4.5593\n",
      "Epoch: [181/200], Step: [8/19], Loss: 4.5626\n",
      "Epoch: [181/200], Step: [9/19], Loss: 4.5647\n",
      "Epoch: [181/200], Step: [10/19], Loss: 4.5810\n",
      "Epoch: [181/200], Step: [11/19], Loss: 4.5730\n",
      "Epoch: [181/200], Step: [12/19], Loss: 4.5567\n",
      "Epoch: [181/200], Step: [13/19], Loss: 4.5777\n",
      "Epoch: [181/200], Step: [14/19], Loss: 4.5654\n",
      "Epoch: [181/200], Step: [15/19], Loss: 4.5836\n",
      "Epoch: [181/200], Step: [16/19], Loss: 4.5592\n",
      "Epoch: [181/200], Step: [17/19], Loss: 4.5739\n",
      "Epoch: [181/200], Step: [18/19], Loss: 4.5837\n",
      "Epoch: [182/200], Step: [1/19], Loss: 4.5634\n",
      "Epoch: [182/200], Step: [2/19], Loss: 4.5506\n",
      "Epoch: [182/200], Step: [3/19], Loss: 4.5755\n",
      "Epoch: [182/200], Step: [4/19], Loss: 4.5942\n",
      "Epoch: [182/200], Step: [5/19], Loss: 4.5644\n",
      "Epoch: [182/200], Step: [6/19], Loss: 4.5759\n",
      "Epoch: [182/200], Step: [7/19], Loss: 4.5822\n",
      "Epoch: [182/200], Step: [8/19], Loss: 4.5538\n",
      "Epoch: [182/200], Step: [9/19], Loss: 4.5624\n",
      "Epoch: [182/200], Step: [10/19], Loss: 4.5574\n",
      "Epoch: [182/200], Step: [11/19], Loss: 4.5725\n",
      "Epoch: [182/200], Step: [12/19], Loss: 4.5635\n",
      "Epoch: [182/200], Step: [13/19], Loss: 4.5633\n",
      "Epoch: [182/200], Step: [14/19], Loss: 4.5695\n",
      "Epoch: [182/200], Step: [15/19], Loss: 4.5748\n",
      "Epoch: [182/200], Step: [16/19], Loss: 4.5710\n",
      "Epoch: [182/200], Step: [17/19], Loss: 4.5610\n",
      "Epoch: [182/200], Step: [18/19], Loss: 4.5765\n",
      "Epoch: [183/200], Step: [1/19], Loss: 4.5702\n",
      "Epoch: [183/200], Step: [2/19], Loss: 4.5735\n",
      "Epoch: [183/200], Step: [3/19], Loss: 4.5680\n",
      "Epoch: [183/200], Step: [4/19], Loss: 4.5603\n",
      "Epoch: [183/200], Step: [5/19], Loss: 4.5778\n",
      "Epoch: [183/200], Step: [6/19], Loss: 4.5734\n",
      "Epoch: [183/200], Step: [7/19], Loss: 4.5820\n",
      "Epoch: [183/200], Step: [8/19], Loss: 4.5964\n",
      "Epoch: [183/200], Step: [9/19], Loss: 4.5543\n",
      "Epoch: [183/200], Step: [10/19], Loss: 4.5610\n",
      "Epoch: [183/200], Step: [11/19], Loss: 4.5637\n",
      "Epoch: [183/200], Step: [12/19], Loss: 4.5611\n",
      "Epoch: [183/200], Step: [13/19], Loss: 4.5639\n",
      "Epoch: [183/200], Step: [14/19], Loss: 4.5737\n",
      "Epoch: [183/200], Step: [15/19], Loss: 4.5666\n",
      "Epoch: [183/200], Step: [16/19], Loss: 4.5681\n",
      "Epoch: [183/200], Step: [17/19], Loss: 4.5618\n",
      "Epoch: [183/200], Step: [18/19], Loss: 4.5514\n",
      "Epoch: [184/200], Step: [1/19], Loss: 4.5766\n",
      "Epoch: [184/200], Step: [2/19], Loss: 4.5613\n",
      "Epoch: [184/200], Step: [3/19], Loss: 4.5791\n",
      "Epoch: [184/200], Step: [4/19], Loss: 4.5500\n",
      "Epoch: [184/200], Step: [5/19], Loss: 4.5634\n",
      "Epoch: [184/200], Step: [6/19], Loss: 4.5793\n",
      "Epoch: [184/200], Step: [7/19], Loss: 4.5704\n",
      "Epoch: [184/200], Step: [8/19], Loss: 4.5738\n",
      "Epoch: [184/200], Step: [9/19], Loss: 4.5560\n",
      "Epoch: [184/200], Step: [10/19], Loss: 4.5659\n",
      "Epoch: [184/200], Step: [11/19], Loss: 4.5733\n",
      "Epoch: [184/200], Step: [12/19], Loss: 4.5549\n",
      "Epoch: [184/200], Step: [13/19], Loss: 4.5815\n",
      "Epoch: [184/200], Step: [14/19], Loss: 4.5640\n",
      "Epoch: [184/200], Step: [15/19], Loss: 4.5636\n",
      "Epoch: [184/200], Step: [16/19], Loss: 4.5622\n",
      "Epoch: [184/200], Step: [17/19], Loss: 4.5761\n",
      "Epoch: [184/200], Step: [18/19], Loss: 4.5714\n",
      "Epoch: [185/200], Step: [1/19], Loss: 4.5651\n",
      "Epoch: [185/200], Step: [2/19], Loss: 4.5732\n",
      "Epoch: [185/200], Step: [3/19], Loss: 4.5768\n",
      "Epoch: [185/200], Step: [4/19], Loss: 4.5805\n",
      "Epoch: [185/200], Step: [5/19], Loss: 4.5749\n",
      "Epoch: [185/200], Step: [6/19], Loss: 4.5633\n",
      "Epoch: [185/200], Step: [7/19], Loss: 4.5715\n",
      "Epoch: [185/200], Step: [8/19], Loss: 4.5757\n",
      "Epoch: [185/200], Step: [9/19], Loss: 4.5638\n",
      "Epoch: [185/200], Step: [10/19], Loss: 4.5686\n",
      "Epoch: [185/200], Step: [11/19], Loss: 4.5538\n",
      "Epoch: [185/200], Step: [12/19], Loss: 4.5728\n",
      "Epoch: [185/200], Step: [13/19], Loss: 4.5623\n",
      "Epoch: [185/200], Step: [14/19], Loss: 4.5530\n",
      "Epoch: [185/200], Step: [15/19], Loss: 4.5629\n",
      "Epoch: [185/200], Step: [16/19], Loss: 4.5603\n",
      "Epoch: [185/200], Step: [17/19], Loss: 4.5609\n",
      "Epoch: [185/200], Step: [18/19], Loss: 4.5786\n",
      "Epoch: [186/200], Step: [1/19], Loss: 4.5572\n",
      "Epoch: [186/200], Step: [2/19], Loss: 4.5766\n",
      "Epoch: [186/200], Step: [3/19], Loss: 4.5790\n",
      "Epoch: [186/200], Step: [4/19], Loss: 4.5740\n",
      "Epoch: [186/200], Step: [5/19], Loss: 4.5737\n",
      "Epoch: [186/200], Step: [6/19], Loss: 4.5774\n",
      "Epoch: [186/200], Step: [7/19], Loss: 4.5760\n",
      "Epoch: [186/200], Step: [8/19], Loss: 4.5758\n",
      "Epoch: [186/200], Step: [9/19], Loss: 4.5585\n",
      "Epoch: [186/200], Step: [10/19], Loss: 4.5604\n",
      "Epoch: [186/200], Step: [11/19], Loss: 4.5444\n",
      "Epoch: [186/200], Step: [12/19], Loss: 4.5805\n",
      "Epoch: [186/200], Step: [13/19], Loss: 4.5575\n",
      "Epoch: [186/200], Step: [14/19], Loss: 4.5834\n",
      "Epoch: [186/200], Step: [15/19], Loss: 4.5849\n",
      "Epoch: [186/200], Step: [16/19], Loss: 4.5447\n",
      "Epoch: [186/200], Step: [17/19], Loss: 4.5602\n",
      "Epoch: [186/200], Step: [18/19], Loss: 4.5493\n",
      "Epoch: [187/200], Step: [1/19], Loss: 4.5534\n",
      "Epoch: [187/200], Step: [2/19], Loss: 4.5559\n",
      "Epoch: [187/200], Step: [3/19], Loss: 4.5761\n",
      "Epoch: [187/200], Step: [4/19], Loss: 4.5555\n",
      "Epoch: [187/200], Step: [5/19], Loss: 4.5618\n",
      "Epoch: [187/200], Step: [6/19], Loss: 4.5706\n",
      "Epoch: [187/200], Step: [7/19], Loss: 4.5652\n",
      "Epoch: [187/200], Step: [8/19], Loss: 4.5623\n",
      "Epoch: [187/200], Step: [9/19], Loss: 4.5761\n",
      "Epoch: [187/200], Step: [10/19], Loss: 4.5620\n",
      "Epoch: [187/200], Step: [11/19], Loss: 4.5628\n",
      "Epoch: [187/200], Step: [12/19], Loss: 4.5802\n",
      "Epoch: [187/200], Step: [13/19], Loss: 4.5766\n",
      "Epoch: [187/200], Step: [14/19], Loss: 4.5702\n",
      "Epoch: [187/200], Step: [15/19], Loss: 4.5577\n",
      "Epoch: [187/200], Step: [16/19], Loss: 4.5800\n",
      "Epoch: [187/200], Step: [17/19], Loss: 4.5710\n",
      "Epoch: [187/200], Step: [18/19], Loss: 4.5711\n",
      "Epoch: [188/200], Step: [1/19], Loss: 4.5652\n",
      "Epoch: [188/200], Step: [2/19], Loss: 4.5687\n",
      "Epoch: [188/200], Step: [3/19], Loss: 4.5795\n",
      "Epoch: [188/200], Step: [4/19], Loss: 4.5710\n",
      "Epoch: [188/200], Step: [5/19], Loss: 4.5726\n",
      "Epoch: [188/200], Step: [6/19], Loss: 4.5644\n",
      "Epoch: [188/200], Step: [7/19], Loss: 4.5835\n",
      "Epoch: [188/200], Step: [8/19], Loss: 4.5616\n",
      "Epoch: [188/200], Step: [9/19], Loss: 4.5686\n",
      "Epoch: [188/200], Step: [10/19], Loss: 4.5562\n",
      "Epoch: [188/200], Step: [11/19], Loss: 4.5639\n",
      "Epoch: [188/200], Step: [12/19], Loss: 4.5716\n",
      "Epoch: [188/200], Step: [13/19], Loss: 4.5737\n",
      "Epoch: [188/200], Step: [14/19], Loss: 4.5591\n",
      "Epoch: [188/200], Step: [15/19], Loss: 4.5615\n",
      "Epoch: [188/200], Step: [16/19], Loss: 4.5586\n",
      "Epoch: [188/200], Step: [17/19], Loss: 4.5698\n",
      "Epoch: [188/200], Step: [18/19], Loss: 4.5545\n",
      "Epoch: [189/200], Step: [1/19], Loss: 4.5753\n",
      "Epoch: [189/200], Step: [2/19], Loss: 4.5705\n",
      "Epoch: [189/200], Step: [3/19], Loss: 4.5561\n",
      "Epoch: [189/200], Step: [4/19], Loss: 4.5686\n",
      "Epoch: [189/200], Step: [5/19], Loss: 4.5787\n",
      "Epoch: [189/200], Step: [6/19], Loss: 4.5894\n",
      "Epoch: [189/200], Step: [7/19], Loss: 4.5763\n",
      "Epoch: [189/200], Step: [8/19], Loss: 4.5697\n",
      "Epoch: [189/200], Step: [9/19], Loss: 4.5775\n",
      "Epoch: [189/200], Step: [10/19], Loss: 4.5663\n",
      "Epoch: [189/200], Step: [11/19], Loss: 4.5639\n",
      "Epoch: [189/200], Step: [12/19], Loss: 4.5505\n",
      "Epoch: [189/200], Step: [13/19], Loss: 4.5684\n",
      "Epoch: [189/200], Step: [14/19], Loss: 4.5535\n",
      "Epoch: [189/200], Step: [15/19], Loss: 4.5622\n",
      "Epoch: [189/200], Step: [16/19], Loss: 4.5514\n",
      "Epoch: [189/200], Step: [17/19], Loss: 4.5487\n",
      "Epoch: [189/200], Step: [18/19], Loss: 4.5723\n",
      "Epoch: [190/200], Step: [1/19], Loss: 4.5772\n",
      "Epoch: [190/200], Step: [2/19], Loss: 4.5657\n",
      "Epoch: [190/200], Step: [3/19], Loss: 4.5523\n",
      "Epoch: [190/200], Step: [4/19], Loss: 4.5523\n",
      "Epoch: [190/200], Step: [5/19], Loss: 4.5571\n",
      "Epoch: [190/200], Step: [6/19], Loss: 4.5667\n",
      "Epoch: [190/200], Step: [7/19], Loss: 4.5568\n",
      "Epoch: [190/200], Step: [8/19], Loss: 4.5575\n",
      "Epoch: [190/200], Step: [9/19], Loss: 4.5660\n",
      "Epoch: [190/200], Step: [10/19], Loss: 4.5771\n",
      "Epoch: [190/200], Step: [11/19], Loss: 4.5737\n",
      "Epoch: [190/200], Step: [12/19], Loss: 4.5688\n",
      "Epoch: [190/200], Step: [13/19], Loss: 4.5819\n",
      "Epoch: [190/200], Step: [14/19], Loss: 4.5638\n",
      "Epoch: [190/200], Step: [15/19], Loss: 4.5780\n",
      "Epoch: [190/200], Step: [16/19], Loss: 4.5706\n",
      "Epoch: [190/200], Step: [17/19], Loss: 4.5590\n",
      "Epoch: [190/200], Step: [18/19], Loss: 4.5703\n",
      "Epoch: [191/200], Step: [1/19], Loss: 4.5562\n",
      "Epoch: [191/200], Step: [2/19], Loss: 4.5667\n",
      "Epoch: [191/200], Step: [3/19], Loss: 4.5598\n",
      "Epoch: [191/200], Step: [4/19], Loss: 4.5844\n",
      "Epoch: [191/200], Step: [5/19], Loss: 4.5534\n",
      "Epoch: [191/200], Step: [6/19], Loss: 4.5679\n",
      "Epoch: [191/200], Step: [7/19], Loss: 4.5772\n",
      "Epoch: [191/200], Step: [8/19], Loss: 4.5470\n",
      "Epoch: [191/200], Step: [9/19], Loss: 4.5661\n",
      "Epoch: [191/200], Step: [10/19], Loss: 4.5818\n",
      "Epoch: [191/200], Step: [11/19], Loss: 4.5601\n",
      "Epoch: [191/200], Step: [12/19], Loss: 4.5747\n",
      "Epoch: [191/200], Step: [13/19], Loss: 4.5669\n",
      "Epoch: [191/200], Step: [14/19], Loss: 4.5621\n",
      "Epoch: [191/200], Step: [15/19], Loss: 4.5793\n",
      "Epoch: [191/200], Step: [16/19], Loss: 4.5713\n",
      "Epoch: [191/200], Step: [17/19], Loss: 4.5593\n",
      "Epoch: [191/200], Step: [18/19], Loss: 4.5558\n",
      "Epoch: [192/200], Step: [1/19], Loss: 4.5760\n",
      "Epoch: [192/200], Step: [2/19], Loss: 4.5827\n",
      "Epoch: [192/200], Step: [3/19], Loss: 4.5759\n",
      "Epoch: [192/200], Step: [4/19], Loss: 4.5783\n",
      "Epoch: [192/200], Step: [5/19], Loss: 4.5796\n",
      "Epoch: [192/200], Step: [6/19], Loss: 4.5629\n",
      "Epoch: [192/200], Step: [7/19], Loss: 4.5523\n",
      "Epoch: [192/200], Step: [8/19], Loss: 4.5562\n",
      "Epoch: [192/200], Step: [9/19], Loss: 4.5564\n",
      "Epoch: [192/200], Step: [10/19], Loss: 4.5724\n",
      "Epoch: [192/200], Step: [11/19], Loss: 4.5725\n",
      "Epoch: [192/200], Step: [12/19], Loss: 4.5549\n",
      "Epoch: [192/200], Step: [13/19], Loss: 4.5607\n",
      "Epoch: [192/200], Step: [14/19], Loss: 4.5777\n",
      "Epoch: [192/200], Step: [15/19], Loss: 4.5654\n",
      "Epoch: [192/200], Step: [16/19], Loss: 4.5518\n",
      "Epoch: [192/200], Step: [17/19], Loss: 4.5455\n",
      "Epoch: [192/200], Step: [18/19], Loss: 4.5640\n",
      "Epoch: [193/200], Step: [1/19], Loss: 4.5692\n",
      "Epoch: [193/200], Step: [2/19], Loss: 4.5685\n",
      "Epoch: [193/200], Step: [3/19], Loss: 4.5739\n",
      "Epoch: [193/200], Step: [4/19], Loss: 4.5604\n",
      "Epoch: [193/200], Step: [5/19], Loss: 4.5694\n",
      "Epoch: [193/200], Step: [6/19], Loss: 4.5632\n",
      "Epoch: [193/200], Step: [7/19], Loss: 4.5471\n",
      "Epoch: [193/200], Step: [8/19], Loss: 4.5550\n",
      "Epoch: [193/200], Step: [9/19], Loss: 4.5682\n",
      "Epoch: [193/200], Step: [10/19], Loss: 4.5695\n",
      "Epoch: [193/200], Step: [11/19], Loss: 4.5653\n",
      "Epoch: [193/200], Step: [12/19], Loss: 4.5695\n",
      "Epoch: [193/200], Step: [13/19], Loss: 4.5800\n",
      "Epoch: [193/200], Step: [14/19], Loss: 4.5546\n",
      "Epoch: [193/200], Step: [15/19], Loss: 4.5668\n",
      "Epoch: [193/200], Step: [16/19], Loss: 4.5794\n",
      "Epoch: [193/200], Step: [17/19], Loss: 4.5687\n",
      "Epoch: [193/200], Step: [18/19], Loss: 4.5515\n",
      "Epoch: [194/200], Step: [1/19], Loss: 4.5614\n",
      "Epoch: [194/200], Step: [2/19], Loss: 4.5765\n",
      "Epoch: [194/200], Step: [3/19], Loss: 4.5518\n",
      "Epoch: [194/200], Step: [4/19], Loss: 4.5714\n",
      "Epoch: [194/200], Step: [5/19], Loss: 4.5562\n",
      "Epoch: [194/200], Step: [6/19], Loss: 4.5765\n",
      "Epoch: [194/200], Step: [7/19], Loss: 4.5643\n",
      "Epoch: [194/200], Step: [8/19], Loss: 4.5720\n",
      "Epoch: [194/200], Step: [9/19], Loss: 4.5705\n",
      "Epoch: [194/200], Step: [10/19], Loss: 4.5592\n",
      "Epoch: [194/200], Step: [11/19], Loss: 4.5618\n",
      "Epoch: [194/200], Step: [12/19], Loss: 4.5656\n",
      "Epoch: [194/200], Step: [13/19], Loss: 4.5639\n",
      "Epoch: [194/200], Step: [14/19], Loss: 4.5436\n",
      "Epoch: [194/200], Step: [15/19], Loss: 4.5681\n",
      "Epoch: [194/200], Step: [16/19], Loss: 4.5637\n",
      "Epoch: [194/200], Step: [17/19], Loss: 4.5820\n",
      "Epoch: [194/200], Step: [18/19], Loss: 4.5671\n",
      "Epoch: [195/200], Step: [1/19], Loss: 4.5710\n",
      "Epoch: [195/200], Step: [2/19], Loss: 4.5727\n",
      "Epoch: [195/200], Step: [3/19], Loss: 4.5536\n",
      "Epoch: [195/200], Step: [4/19], Loss: 4.5679\n",
      "Epoch: [195/200], Step: [5/19], Loss: 4.5807\n",
      "Epoch: [195/200], Step: [6/19], Loss: 4.5742\n",
      "Epoch: [195/200], Step: [7/19], Loss: 4.5746\n",
      "Epoch: [195/200], Step: [8/19], Loss: 4.5775\n",
      "Epoch: [195/200], Step: [9/19], Loss: 4.5569\n",
      "Epoch: [195/200], Step: [10/19], Loss: 4.5677\n",
      "Epoch: [195/200], Step: [11/19], Loss: 4.5534\n",
      "Epoch: [195/200], Step: [12/19], Loss: 4.5554\n",
      "Epoch: [195/200], Step: [13/19], Loss: 4.5640\n",
      "Epoch: [195/200], Step: [14/19], Loss: 4.5542\n",
      "Epoch: [195/200], Step: [15/19], Loss: 4.5571\n",
      "Epoch: [195/200], Step: [16/19], Loss: 4.5516\n",
      "Epoch: [195/200], Step: [17/19], Loss: 4.5796\n",
      "Epoch: [195/200], Step: [18/19], Loss: 4.5587\n",
      "Epoch: [196/200], Step: [1/19], Loss: 4.5590\n",
      "Epoch: [196/200], Step: [2/19], Loss: 4.5613\n",
      "Epoch: [196/200], Step: [3/19], Loss: 4.5753\n",
      "Epoch: [196/200], Step: [4/19], Loss: 4.5774\n",
      "Epoch: [196/200], Step: [5/19], Loss: 4.5650\n",
      "Epoch: [196/200], Step: [6/19], Loss: 4.5493\n",
      "Epoch: [196/200], Step: [7/19], Loss: 4.5592\n",
      "Epoch: [196/200], Step: [8/19], Loss: 4.5657\n",
      "Epoch: [196/200], Step: [9/19], Loss: 4.5766\n",
      "Epoch: [196/200], Step: [10/19], Loss: 4.5637\n",
      "Epoch: [196/200], Step: [11/19], Loss: 4.5714\n",
      "Epoch: [196/200], Step: [12/19], Loss: 4.5625\n",
      "Epoch: [196/200], Step: [13/19], Loss: 4.5658\n",
      "Epoch: [196/200], Step: [14/19], Loss: 4.5626\n",
      "Epoch: [196/200], Step: [15/19], Loss: 4.5761\n",
      "Epoch: [196/200], Step: [16/19], Loss: 4.5574\n",
      "Epoch: [196/200], Step: [17/19], Loss: 4.5498\n",
      "Epoch: [196/200], Step: [18/19], Loss: 4.5678\n",
      "Epoch: [197/200], Step: [1/19], Loss: 4.5520\n",
      "Epoch: [197/200], Step: [2/19], Loss: 4.5642\n",
      "Epoch: [197/200], Step: [3/19], Loss: 4.5578\n",
      "Epoch: [197/200], Step: [4/19], Loss: 4.5679\n",
      "Epoch: [197/200], Step: [5/19], Loss: 4.5513\n",
      "Epoch: [197/200], Step: [6/19], Loss: 4.5571\n",
      "Epoch: [197/200], Step: [7/19], Loss: 4.5750\n",
      "Epoch: [197/200], Step: [8/19], Loss: 4.5582\n",
      "Epoch: [197/200], Step: [9/19], Loss: 4.5486\n",
      "Epoch: [197/200], Step: [10/19], Loss: 4.5734\n",
      "Epoch: [197/200], Step: [11/19], Loss: 4.5657\n",
      "Epoch: [197/200], Step: [12/19], Loss: 4.5693\n",
      "Epoch: [197/200], Step: [13/19], Loss: 4.5508\n",
      "Epoch: [197/200], Step: [14/19], Loss: 4.5750\n",
      "Epoch: [197/200], Step: [15/19], Loss: 4.5589\n",
      "Epoch: [197/200], Step: [16/19], Loss: 4.5744\n",
      "Epoch: [197/200], Step: [17/19], Loss: 4.5837\n",
      "Epoch: [197/200], Step: [18/19], Loss: 4.5780\n",
      "Epoch: [198/200], Step: [1/19], Loss: 4.5589\n",
      "Epoch: [198/200], Step: [2/19], Loss: 4.5560\n",
      "Epoch: [198/200], Step: [3/19], Loss: 4.5619\n",
      "Epoch: [198/200], Step: [4/19], Loss: 4.5697\n",
      "Epoch: [198/200], Step: [5/19], Loss: 4.5790\n",
      "Epoch: [198/200], Step: [6/19], Loss: 4.5729\n",
      "Epoch: [198/200], Step: [7/19], Loss: 4.5575\n",
      "Epoch: [198/200], Step: [8/19], Loss: 4.5545\n",
      "Epoch: [198/200], Step: [9/19], Loss: 4.5628\n",
      "Epoch: [198/200], Step: [10/19], Loss: 4.5679\n",
      "Epoch: [198/200], Step: [11/19], Loss: 4.5713\n",
      "Epoch: [198/200], Step: [12/19], Loss: 4.5578\n",
      "Epoch: [198/200], Step: [13/19], Loss: 4.5555\n",
      "Epoch: [198/200], Step: [14/19], Loss: 4.5548\n",
      "Epoch: [198/200], Step: [15/19], Loss: 4.5477\n",
      "Epoch: [198/200], Step: [16/19], Loss: 4.5852\n",
      "Epoch: [198/200], Step: [17/19], Loss: 4.5715\n",
      "Epoch: [198/200], Step: [18/19], Loss: 4.5718\n",
      "Epoch: [199/200], Step: [1/19], Loss: 4.5596\n",
      "Epoch: [199/200], Step: [2/19], Loss: 4.5719\n",
      "Epoch: [199/200], Step: [3/19], Loss: 4.5648\n",
      "Epoch: [199/200], Step: [4/19], Loss: 4.5624\n",
      "Epoch: [199/200], Step: [5/19], Loss: 4.5764\n",
      "Epoch: [199/200], Step: [6/19], Loss: 4.5683\n",
      "Epoch: [199/200], Step: [7/19], Loss: 4.5719\n",
      "Epoch: [199/200], Step: [8/19], Loss: 4.5710\n",
      "Epoch: [199/200], Step: [9/19], Loss: 4.5762\n",
      "Epoch: [199/200], Step: [10/19], Loss: 4.5587\n",
      "Epoch: [199/200], Step: [11/19], Loss: 4.5431\n",
      "Epoch: [199/200], Step: [12/19], Loss: 4.5563\n",
      "Epoch: [199/200], Step: [13/19], Loss: 4.5502\n",
      "Epoch: [199/200], Step: [14/19], Loss: 4.5689\n",
      "Epoch: [199/200], Step: [15/19], Loss: 4.5684\n",
      "Epoch: [199/200], Step: [16/19], Loss: 4.5734\n",
      "Epoch: [199/200], Step: [17/19], Loss: 4.5552\n",
      "Epoch: [199/200], Step: [18/19], Loss: 4.5551\n",
      "Epoch: [200/200], Step: [1/19], Loss: 4.5763\n",
      "Epoch: [200/200], Step: [2/19], Loss: 4.5684\n",
      "Epoch: [200/200], Step: [3/19], Loss: 4.5666\n",
      "Epoch: [200/200], Step: [4/19], Loss: 4.5666\n",
      "Epoch: [200/200], Step: [5/19], Loss: 4.5597\n",
      "Epoch: [200/200], Step: [6/19], Loss: 4.5623\n",
      "Epoch: [200/200], Step: [7/19], Loss: 4.5632\n",
      "Epoch: [200/200], Step: [8/19], Loss: 4.5566\n",
      "Epoch: [200/200], Step: [9/19], Loss: 4.5560\n",
      "Epoch: [200/200], Step: [10/19], Loss: 4.5721\n",
      "Epoch: [200/200], Step: [11/19], Loss: 4.5696\n",
      "Epoch: [200/200], Step: [12/19], Loss: 4.5627\n",
      "Epoch: [200/200], Step: [13/19], Loss: 4.5686\n",
      "Epoch: [200/200], Step: [14/19], Loss: 4.5815\n",
      "Epoch: [200/200], Step: [15/19], Loss: 4.5469\n",
      "Epoch: [200/200], Step: [16/19], Loss: 4.5632\n",
      "Epoch: [200/200], Step: [17/19], Loss: 4.5710\n",
      "Epoch: [200/200], Step: [18/19], Loss: 4.5357\n"
     ]
    }
   ],
   "source": [
    "trian_num = 18\n",
    "test_num = 2\n",
    "num_classes = 100\n",
    "# 加载数据\n",
    "train_image, train_label, test_image, test_label = LoadData(num_classes, trian_num, test_num, args.seed)\n",
    "# 训练用图像应该为二维，即28*28\n",
    "train_images_list = list()\n",
    "for i in range(0, len(train_image)):\n",
    "    temp_image = np.reshape(train_image[i], (1, 28, 28))\n",
    "    train_images_list.append(temp_image)\n",
    "train_images = np.array(train_images_list)\n",
    "print(train_images.shape)\n",
    "\n",
    "# 测试用图像应该为二维，即28*28\n",
    "test_images_list = list()\n",
    "for i in range(0, len(test_image)):\n",
    "    temp_image = np.reshape(test_image[i], (1, 28, 28))\n",
    "    test_images_list.append(temp_image)\n",
    "test_images = np.array(test_images_list)\n",
    "print(test_images.shape)\n",
    "\n",
    "# 转换为pytorch可处理数据集\n",
    "train_dataset = da.TensorDataset(torch.from_numpy(train_images), torch.from_numpy(train_label))\n",
    "test_dataset = da.TensorDataset(torch.from_numpy(test_images), torch.from_numpy(test_label))\n",
    "# 数据分批\n",
    "size_batch = 100\n",
    "train_loader = da.DataLoader(dataset=train_dataset, batch_size=size_batch, shuffle=True)\n",
    "test_loader = da.DataLoader(dataset=test_dataset, batch_size=size_batch, shuffle=False)\n",
    "# 定义超参数, 模型, 损失函数 和 优化器\n",
    "# 定义内部超参数\n",
    "learning_rate = 1e-3\n",
    "\n",
    "# 定义模型\n",
    "class CNN(nn.Module):\n",
    "    def __init__(self, size_out):\n",
    "        super(CNN, self).__init__()\n",
    "        self.layer1 = torch.nn.Sequential(\n",
    "            torch.nn.Conv2d(1, 20, kernel_size=10),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.MaxPool2d(kernel_size=4)\n",
    "        )\n",
    "        self.layer2 = torch.nn.Sequential(\n",
    "            torch.nn.Linear(320, 150),\n",
    "            torch.nn.Linear(150, 100)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.to(torch.float32)\n",
    "        layer1_out = self.layer1(x)\n",
    "        layer1_out_solve = layer1_out.view(-1, 320)\n",
    "        layer2_out = self.layer2(layer1_out_solve)\n",
    "        return layer2_out\n",
    "\n",
    "\n",
    "model = CNN(size_out)  # .to(device)\n",
    "# 定义损失函数\n",
    "loss = nn.CrossEntropyLoss()\n",
    "# 定义优化器\n",
    "# optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)\n",
    "# 训练模型\n",
    "for epoch in range(args.epochs):\n",
    "    for i, (images, labels) in enumerate(train_loader, 0):\n",
    "        # 前向传播\n",
    "        output = model(images)\n",
    "\n",
    "        Loss = loss(output, labels.long())\n",
    "        # 如果采用第二种数据加载思路，请用下面这行代码\n",
    "        # Loss = loss(output, labels)\n",
    "        # 反向传播\n",
    "        optimizer.zero_grad()\n",
    "        Loss.backward()\n",
    "        optimizer.step()\n",
    "        # 输出\n",
    "        print('Epoch: [{}/{}], Step: [{}/{}], Loss: {:.4f}'\n",
    "              .format(epoch + 1, args.epochs, i + 1, int(len(train_image) / size_batch) + 1, Loss.item()))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 373,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy:：2.5%\n"
     ]
    }
   ],
   "source": [
    "# 模型测试\n",
    "with torch.no_grad():\n",
    "    total_cor = 0\n",
    "    total_num = 0\n",
    "    for images, labels in test_loader:\n",
    "        images = images\n",
    "        labels = labels\n",
    "        output = model(images)\n",
    "        _, predict_result = torch.max(output.data, 1)\n",
    "        total_num += labels.size(0)\n",
    "        total_cor += (predict_result == labels).sum().item()\n",
    "    print(\"Test Accuracy:：{}%\".format(100 * total_cor / total_num))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 4.5不同迭代次数"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "epochs = 100\n",
    "trian_num = 18\n",
    "test_num = 2\n",
    "num_classes = 100\n",
    "# 加载数据\n",
    "train_image, train_label, test_image, test_label = LoadData(num_classes, trian_num, test_num, args.seed)\n",
    "# 训练用图像应该为二维，即28*28\n",
    "train_images_list = list()\n",
    "for i in range(0, len(train_image)):\n",
    "    temp_image = np.reshape(train_image[i], (1, 28, 28))\n",
    "    train_images_list.append(temp_image)\n",
    "train_images = np.array(train_images_list)\n",
    "print(train_images.shape)\n",
    "\n",
    "# 测试用图像应该为二维，即28*28\n",
    "test_images_list = list()\n",
    "for i in range(0, len(test_image)):\n",
    "    temp_image = np.reshape(test_image[i], (1, 28, 28))\n",
    "    test_images_list.append(temp_image)\n",
    "test_images = np.array(test_images_list)\n",
    "print(test_images.shape)\n",
    "\n",
    "# 转换为pytorch可处理数据集\n",
    "train_dataset = da.TensorDataset(torch.from_numpy(train_images), torch.from_numpy(train_label))\n",
    "test_dataset = da.TensorDataset(torch.from_numpy(test_images), torch.from_numpy(test_label))\n",
    "# 数据分批\n",
    "size_batch = 100\n",
    "train_loader = da.DataLoader(dataset=train_dataset, batch_size=size_batch, shuffle=True)\n",
    "test_loader = da.DataLoader(dataset=test_dataset, batch_size=size_batch, shuffle=False)\n",
    "# 定义超参数, 模型, 损失函数 和 优化器\n",
    "# 定义内部超参数\n",
    "learning_rate = 1e-3\n",
    "\n",
    "# 定义模型\n",
    "class CNN(nn.Module):\n",
    "    def __init__(self, size_out):\n",
    "        super(CNN, self).__init__()\n",
    "        self.layer1 = torch.nn.Sequential(\n",
    "            torch.nn.Conv2d(1, 20, kernel_size=10),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.MaxPool2d(kernel_size=4)\n",
    "        )\n",
    "        self.layer2 = torch.nn.Sequential(\n",
    "            torch.nn.Linear(320, 150),\n",
    "            torch.nn.Linear(150, 100)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.to(torch.float32)\n",
    "        layer1_out = self.layer1(x)\n",
    "        layer1_out_solve = layer1_out.view(-1, 320)\n",
    "        layer2_out = self.layer2(layer1_out_solve)\n",
    "        return layer2_out\n",
    "\n",
    "\n",
    "model = CNN(size_out)  # .to(device)\n",
    "# 定义损失函数\n",
    "loss = nn.CrossEntropyLoss()\n",
    "# 定义优化器\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "# optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)\n",
    "# 训练模型\n",
    "for epoch in range(epochs):\n",
    "    for i, (images, labels) in enumerate(train_loader, 0):\n",
    "        # 前向传播\n",
    "        output = model(images)\n",
    "\n",
    "        Loss = loss(output, labels.long())\n",
    "        # 如果采用第二种数据加载思路，请用下面这行代码\n",
    "        # Loss = loss(output, labels)\n",
    "        # 反向传播\n",
    "        optimizer.zero_grad()\n",
    "        Loss.backward()\n",
    "        optimizer.step()\n",
    "        # 输出\n",
    "        print('Epoch: [{}/{}], Step: [{}/{}], Loss: {:.4f}'\n",
    "              .format(epoch + 1, epochs, i + 1, int(len(train_image) / size_batch) + 1, Loss.item()))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 375,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy:：76.0%\n"
     ]
    }
   ],
   "source": [
    "# 模型测试\n",
    "with torch.no_grad():\n",
    "    total_cor = 0\n",
    "    total_num = 0\n",
    "    for images, labels in test_loader:\n",
    "        images = images\n",
    "        labels = labels\n",
    "        output = model(images)\n",
    "        _, predict_result = torch.max(output.data, 1)\n",
    "        total_num += labels.size(0)\n",
    "        total_cor += (predict_result == labels).sum().item()\n",
    "    print(\"Test Accuracy:：{}%\".format(100 * total_cor / total_num))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "epochs = 400\n",
    "trian_num = 18\n",
    "test_num = 2\n",
    "num_classes = 100\n",
    "# 加载数据\n",
    "train_image, train_label, test_image, test_label = LoadData(num_classes, trian_num, test_num, args.seed)\n",
    "# 训练用图像应该为二维，即28*28\n",
    "train_images_list = list()\n",
    "for i in range(0, len(train_image)):\n",
    "    temp_image = np.reshape(train_image[i], (1, 28, 28))\n",
    "    train_images_list.append(temp_image)\n",
    "train_images = np.array(train_images_list)\n",
    "print(train_images.shape)\n",
    "\n",
    "# 测试用图像应该为二维，即28*28\n",
    "test_images_list = list()\n",
    "for i in range(0, len(test_image)):\n",
    "    temp_image = np.reshape(test_image[i], (1, 28, 28))\n",
    "    test_images_list.append(temp_image)\n",
    "test_images = np.array(test_images_list)\n",
    "print(test_images.shape)\n",
    "\n",
    "# 转换为pytorch可处理数据集\n",
    "train_dataset = da.TensorDataset(torch.from_numpy(train_images), torch.from_numpy(train_label))\n",
    "test_dataset = da.TensorDataset(torch.from_numpy(test_images), torch.from_numpy(test_label))\n",
    "# 数据分批\n",
    "size_batch = 100\n",
    "train_loader = da.DataLoader(dataset=train_dataset, batch_size=size_batch, shuffle=True)\n",
    "test_loader = da.DataLoader(dataset=test_dataset, batch_size=size_batch, shuffle=False)\n",
    "# 定义超参数, 模型, 损失函数 和 优化器\n",
    "# 定义内部超参数\n",
    "learning_rate = 1e-3\n",
    "\n",
    "# 定义模型\n",
    "class CNN(nn.Module):\n",
    "    def __init__(self, size_out):\n",
    "        super(CNN, self).__init__()\n",
    "        self.layer1 = torch.nn.Sequential(\n",
    "            torch.nn.Conv2d(1, 20, kernel_size=10),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.MaxPool2d(kernel_size=4)\n",
    "        )\n",
    "        self.layer2 = torch.nn.Sequential(\n",
    "            torch.nn.Linear(320, 150),\n",
    "            torch.nn.Linear(150, 100)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.to(torch.float32)\n",
    "        layer1_out = self.layer1(x)\n",
    "        layer1_out_solve = layer1_out.view(-1, 320)\n",
    "        layer2_out = self.layer2(layer1_out_solve)\n",
    "        return layer2_out\n",
    "\n",
    "\n",
    "model = CNN(size_out)  # .to(device)\n",
    "# 定义损失函数\n",
    "loss = nn.CrossEntropyLoss()\n",
    "# 定义优化器\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "# optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)\n",
    "# 训练模型\n",
    "for epoch in range(epochs):\n",
    "    for i, (images, labels) in enumerate(train_loader, 0):\n",
    "        # 前向传播\n",
    "        output = model(images)\n",
    "\n",
    "        Loss = loss(output, labels.long())\n",
    "        # 如果采用第二种数据加载思路，请用下面这行代码\n",
    "        # Loss = loss(output, labels)\n",
    "        # 反向传播\n",
    "        optimizer.zero_grad()\n",
    "        Loss.backward()\n",
    "        optimizer.step()\n",
    "        # 输出\n",
    "        print('Epoch: [{}/{}], Step: [{}/{}], Loss: {:.4f}'\n",
    "              .format(epoch + 1, epochs, i + 1, int(len(train_image) / size_batch) + 1, Loss.item()))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 377,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy:：76.5%\n"
     ]
    }
   ],
   "source": [
    "# 模型测试\n",
    "with torch.no_grad():\n",
    "    total_cor = 0\n",
    "    total_num = 0\n",
    "    for images, labels in test_loader:\n",
    "        images = images\n",
    "        labels = labels\n",
    "        output = model(images)\n",
    "        _, predict_result = torch.max(output.data, 1)\n",
    "        total_num += labels.size(0)\n",
    "        total_cor += (predict_result == labels).sum().item()\n",
    "    print(\"Test Accuracy:：{}%\".format(100 * total_cor / total_num))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}